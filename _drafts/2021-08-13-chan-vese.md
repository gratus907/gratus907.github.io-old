---
layout: single
title: "논문읽기 : Chan-Vese Algorithm"
categories: cs-adventure
sidebar:
  nav: "sidepost"
comment: true
comments : true
toc : true
---
<div id="toc">
Contents
</div>
* TOC
{:toc}
----------

# Introduction 
이번에 읽은 논문은 **Active Contours Without Edges** 라는, 2001년의 논문입니다. 2001년 IEEE Transactions on Image Processing, Vol.10, No.2 에 발표된 논문으로, 이 분야 - image processing - 에서는 엄청나게 중요한 논문으로, 현재까지 1만 3천 회 가량 인용되었습니다. 

목표는 어떤 이미지가 주어졌을 때, 이 이미지의 외곽선 **"Contour"** 를 따는 것입니다. 특히, 여기서는 segmentation이라고 해서 그림의 픽셀을 몇개의 클래스로 구분하는 문제를 해결하는 것으로 보고 있습니다. 예를 들어, 배경 앞에 사람이 서 있다면, 사람과 배경을 구분하는 문제를 classification이라고 할 수 있겠습니다. 꽤 오래 전 (저널 발표일 기준 2001) 논문이므로, 본격적인 Deep Learning의 시대가 오기 전의 방법론을 볼 수 있었습니다.

먼저, 용어를 간단히 정의합니다. Segmentation과 Contour detection은 원래 약간 다른 문제지만, 여기서는 Segmentation의 방법으로 Level set (등고선) 의 Contour를 따는 방법을 생각하기로 합니다. 이를 위해, 예를 들어 어떤 grayscale 이미지가 주어지면, 이 선을 대충 색이 진한 쪽과 흐린 쪽으로 나누기 위해 진한 점들이 이루는 Contour를 찾고자 한다고 이해하면 되겠습니다. 

------

# Key Ideas
## Energy functional
이 문제에서는, Segmentation 문제를 Functional Optimization의 문제로 환원합니다. 여기서 Functional이란, 정의역이 함수의 집합인 함수를 말합니다. 

특히, 우리는 결과물의 외곽선이 Smooth하기를 원하므로, $X$에서 Lipschitz Continuous 한 함수의 집합 $\mathcal{L}$ 에서 $\R$로 가는 함수열을 생각할 것입니다. 여기서 Lipschitz 연속이란 연속성보다 더 강한 개념으로, 점 $x, y$ 와 어떤 상수 $K$에 대해 $\norm{f(x) - f(y)} \leq K \norm{x - y}$ 를 만족하는 함수들을 의미합니다. 

함수의 Level set에 대해 논의하기 위해, 우리는 함수 $\phi \in \mathcal{L}$ 에 대해, $\phi = 0$ 인 점들을 이은 곡선을 $C$라고 정의합니다. 또한, $\phi(x) > 0$ 인 공간을 $A$, $\phi(x) < 0$ 인 공간을 $B$라고 쓰겠습니다. 마지막으로, 원래의 이미지 픽셀값을 $u_0(x, y)$ 함수로 나타냅니다.

이제, 다음과 같은 Functional들을 정의합니다. 
- $Len(C)$ : 곡선의 길이. 곡선의 길이가 길면 $\phi$ 가 덜 smooth하기 때문에 (해석학적인 term이라기보다는, 기하적인 smooth함), 매끄러운 곡선을 그리도록 페널티를 통해 incentivise 합니다. 
- $Area(A)$ : $\phi(x) > 0$ 인 부분의 넓이. 길이와 기본적인 의미는 같습니다.
- $\int_{A} \abs{u_0(x, y) - c_1}^2 \dd{x}\dd{y}$ : 어떤 실수값 $c_1$ 을 잡아서, $\phi$ 안쪽에서 $u_0$ 의 평균을 나타내고 싶습니다. 이때 이 평균값이 가급적 정확하기를 바란다는 의미입니다.
- $\int_{B} \abs{u_0(x, y) - c_2}^2 \dd{x}\dd{y}$ : $\phi$ 바깥쪽에서도 똑같은 작업을 합니다. 

직관적으로, 저 네 값 모두 작았으면 좋겠다는것을 알 수 있습니다. 앞 두개가 작으려면 함수가 대충 곡선으로 쭉 매끄럽게 이어져야 하고, 뒤 두개가 작으려면 그 안쪽과 바깥쪽에 어떤 intensity 값을 잡아서 그 값에 가깝게 잘려야 합니다.  
우리는 저 네 Functional의 선형결합을 "Energy Functional" 이라고 부르기로 하고, 저 값을 minimize하는 $c_1, c_2, \phi$ 를 찾는 것을 목표로 합니다. 

## Integral formulation
그러나, 저 식은 저대로는 상당히 계산하기가 어렵습니다. 좀더 계산을 잘 하기 위해, 식을 살짝 조절해 봅시다. 이를 위해, 헤비사이드 함수 $H$를 도입합니다. $H$는 $x \geq 0$ 일 때 1, $x < 0$ 일 때 0인 함수입니다. 이를 도입하면 $H(\phi(x, y)) = 1$ iff $\phi(x, y) \geq 0$ 가 성립합니다.

1. Length : 길이는 스토크스 정리와 헤비사이드 함수의 정의를 이용하면, 아래와 같이 쓸 수 있습니다. 
  $$L(\phi) = Len(C) = \int_{\R^2} \abs{\nabla H(\phi(x, y))} \dd{x}\dd{y}$$ 
  당연히 일반적으로 $H(\phi(x, y))$ 는 미분이 불가능하지만, 우리는 Heaviside의 도함수를 Dirac-delta로 쓰고 있으므로 (in distribution function sense) 적분은 잘 됩니다. [^1] 
2. Area : 간단한 다변수 적분입니다. 
  $$S(\phi) = Area(A) = \int_{\R^2} H(\phi(x, y)) \dd{x}\dd{y}$$
3. 역시 간단한 두 개의 다변수 적분식을 쓸 수 있습니다. 
  $$F_i(C) = \int_{\R^2} \abs{u_0(x, y) - c_1}^2 H(\phi(x, y))\dd{x}\dd{y}$$ 
  $$F_o(C) = \int_{\R^2} \abs{u_0(x, y) - c_2}^2 (1 - H(\phi(x, y)))\dd{x}\dd{y}$$ 

하나 관찰할 수 있는 것은, $c_1$과 $c_2$는 $\phi$를 고정하고 최적화하면 최적화할 수 있다는 점입니다. 구체적으로, 첫번째 식을 다시 앞서의 형태인 
$$\int_{A} \abs{u_0(x, y) - c_1}^2 \dd{x}\dd{y}$$ 이렇게 돌려놓고 보면, $c_1$은 자명하게 $u_0$의 $A$에서의 '평균' 이 되어야 합니다. 따라서, 앞으로 $c_1, c_2$는 $\phi$로부터 간단한 적분을 통해 계산 가능하므로, 위 식을 $\phi$로만 최소화한다고 문제를 단순화하겠습니다.  
또한, 실제 알고리즘은 1, 2, 3에 각각 적당한 상수를 붙여서 계산합니다. 특히, (3) 의 $F_i$ 와 $F_o$에 다른 상수를 붙여서 계산하는데, 실제로는 원본 논문의 저자들도 상수를 대충 잡았고, 이 상수를 어떻게 잡아야 하는지에 대해서는 많은 논증이 없었으므로 저는 여기서 (1) * $\mu$ + (2) * $\nu$ + (3) * $\lambda$ 로 놓고 계산하겠습니다. 

## Relaxation & Euler-Lagrange
이제, 여기까지 오면서 우리가 최종적으로 무엇을 최소화하는지 보겠습니다. 
$$\int_{\R^2} \mu L(\phi) + \nu S(\phi) + \lambda(F_i(\phi) + F_o(\phi)) \dd{x}\dd{y}$$

## Partial Differential Equation
우리는 이런 편미분방정식은 풀 방법이 없기 때문에, 마지막으로 수치해석을 적용합니다. 구체적으로, Finite Differnce method를 이용해야 합니다. 

Finite Difference method는 별로 어렵지 않은데, $\Delta_{-}^{x}$ 같은 식으로 $x, y$ 방향 $+, -$ 로 네개의 time differnece를 정의하고 여러 공식들을 적용하기만 하면 됩니다. 편미방이 복잡하게 생겼지만 각 term은 그렇게 어렵지 않습니다.


------


# Conclusion
이 알고리즘은 Noisy image에서도 생각보다 훌륭한 성능을 보여주고, vector-valued 같은 확장도 그렇게 어렵지 않습니다. 특히, 이런 식의 Energy Functional을 잘 정의하기에 따라서 범용성이 굉장히 높고 원하는 Feature가 있다면 추가로 embed 할 수도 있어서 확장성도 높습니다. 

------

# Thoughts
1. Parameter 가 상당히 중요해 보입니다. $\mu, \nu, \lambda$ 의 어떤 조합이 좋은 결과를 내는지에 대해서는 원본 논문에서는 별로 Discuss하지 않았는데, 실험적으로 확인해야 하는 걸까요? $\mu, \nu$ 는 어떻게 실험적으로 검증할 수 있어 보이는데, $\lambda$는 좀 오바인것 같습니다. 검색을 좀 해보니, 다른 논문 몇편에서 이미지의 어떤 computable한 성질들로부터 parameter를 자동으로 튜닝하는 논문들이 있었습니다. 
2. Functional Optimization은 미적분학 II에서 변분법을 배운 이후로 처음인데, 일반적인 optimization의 방법론들과는 좀 다르다보니 어렵습니다. 특히 $f$ 에서 $x$를 움직여서 최적화하는 경우는 $f'$ 같은 정보들이 있는데, Functional은 어떻게 최적화하는지 잘 모르겠습니다. 
듣기로는 Banach space위에서의 Lagrange Multiplier같은 해괴한게 있다고 합니다. Functional도 결국 Banach space위에서 어떤 함수를 최적화하는 거니까, 비슷한 개념이 있을까요? 구체적으로는 Banach space $X$에서 정의된 Functional $F$의 도함수? 

------

[^1]: 해석개론을 배우고 나서부터 디랙-델타를 적분에 활용하는게 오히려 정말 이해가 안 갔었는데, 이 개념은 Measure, Distribution function 등 실해석학 및 그 이상의 해석학을 배우면 다시 make sense 합니다. 잠시 공학수학의 관점으로 돌아가서 이 식을 받아들이기로 합니다. 어차피, 이 적분을 실제로 계산할 것은 아니니까요.