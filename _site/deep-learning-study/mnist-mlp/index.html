<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.21.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="ko" class="no-js">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-TNVQ3G5D5B"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-TNVQ3G5D5B');
</script>

    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Softmax Regression / MLP로 MNIST 풀어보기 - Gratus907’s Study Note</title>
<meta name="description" content="Contents     Problem / Dataset   Softmax Regression   Multi-Layer Perceptron    심층 신경망의 수학적 기초 5강, 6강 (9월 16일, 23일) 에 기반합니다. 이번 내용은 대부분이 코드에 대한 내용이라서, $\LaTeX$ 노트를 변환하지 않고 여기에 바로 작성했습니다.  아직 읽지 않았다면, 최소한 Softmax(링크)와 MLP(링크)에 대한 포스팅 을, 되도록 링크 에 있는 포스팅 중 shallow-nn과 SVM, LR에 대한 내용을 읽으면 이론적 배경이 충분할 것으로 생각합니다 (제가 이 내용을 공부해서 이해한대로 정리했으니까요..?)  Problem / Dataset 이번에 해결하고자 하는 문제는, MNIST라는 매우 유명한 데이터셋을 이용합니다. MNIST는 Hand-written 숫자로 구성된 데이터셋으로, 널리 알려진 CNN 모델을 처음 제시한 LeCun의 연구에 사용되었던 데이터셋이기도 합니다. 실제로 작동하는 딥 러닝을 만들기에는 너무 작은 데이터셋이지만 공부하는 목적으로 주로 사용됩니다.  각 이미지는 28 by 28 grayscale image로, 편의상 $\R^{28 \times 28}$ 으로 생각하면 됩니다.  가장 먼저 해야할 일은 pytorch module을 import하고, 데이터를 받아오고 정리하는 것입니다. Pytorch에서는 DataLoader라는 모듈을 이용하여, 편하게 데이터를 Batch로 먹인다거나 하는 작업을 할 수 있습니다.          import torch import torch.nn as nn from torch.optim import Optimizer from torch.utils.data import DataLoader from torchvision import datasets from torchvision.transforms import transforms  train_set = datasets.MNIST(root=&#39;./mnist_data/&#39;, train=True, transform=transforms.ToTensor(), download=True) test_set = datasets.MNIST(root=&#39;./mnist_data/&#39;, train=False, transform=transforms.ToTensor(), download=True) train_loader = DataLoader(dataset=train_set, batch_size=64, shuffle=True) test_loader = DataLoader(dataset=test_set, batch_size=1, shuffle=False) device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)       device = 부분은 가능하다면 cuda GPU를 사용하도록 하는 부분인데, 사실 이번 태스크는 너무 작기 때문에 GPU를 쓰면 이득이 없거나 오히려 더 느려질 수도 있습니다.  Softmax Regression 먼저, 우리는 Softmax regression을 시도해 보겠습니다. Pytorch에서는 다음과 같은 과정으로 머신러닝 모델을 학습합니다.    Model 정의 : 입력 $x$를 어떤 과정을 거쳐 출력값으로 만들지를 정의합니다. 이 모델에는 훈련가능한 parameter가 있습니다.   Loss function 정의 : 모델이 어떤 방법으로 현재 정확도를 측정할지를 정의합니다.   학습 : Training data를 이용하여 모델의 파라미터를 조정합니다.   이제, Model을 정의해야 합니다. Model은 기본적으로 softmax regression에서 공부했던 모델로, $Ax + b$ 를 10개 만들어야 합니다.  여기서, __init__ 을 이용하여 이 모델에서 쓸 Layer들을 정의하고, 그 Layer들을 forward 메서드를 통해 사용해서 넘겨주면 됩니다.         class SoftMax(nn.Module):     def __init__(self):         super().__init__()         self.layer = nn.Linear(28*28, 10, bias=True)     def forward(self, x):         return self.layer(x.float().view(-1, 28 * 28))  model = SoftMax()      Softmax Regression은 이렇게 정의된 model에 다음과 같은 최적화 문제를 푸는 방법입니다.  (\underset{a \in \R^{k \times n}, b \in \R^k}{\minimize}\ \frac{1}{N}\sum_{i = 1}^{N}  \left(-(a_{Y_i}^T X_i + b_{Y_i} + \log\left(\sum_{j = 1}^{k} e^{a_j^TX_i + b}\right)\right))  pytorch에는 $Ax + b$의 결과만 받아내면 이를 계산해주는 함수가 있으므로, 모델은 $Ax + b$만 해주면 됩니다. 대신 Loss function을 다음과 같이 사용합니다. 여기서 lr 은 learning rate, $\alpha$ 값을 의미합니다. 최적화 자체에는 Stochastic Gradient Descent를 사용하겠습니다.         loss_function = torch.nn.CrossEntropyLoss()     optimizer = torch.optim.SGD(model.parameters(), lr=0.03)          이제, 우리에게 필요한 모델의 훈련 과정은 이렇게 진행됩니다. Epoch를 많이 돌릴수록 정확해지긴 하지만, 투입하는 시간 대비 얼만큼의 효율이 있는지는 상황마다 다르므로 적당히 판단할 필요가 있습니다.  워낙 단순한 모델이라 학습할게 많지 않으므로 여기서는 epoch=2 (즉, 전체 데이터를 두바퀴 돌립니다) 만 돌리겠습니다.          NUM_EPOCH = 2 for epoch in range(NUM_EPOCH) :     for images, labels in train_loader :         optimizer.zero_grad()         train_loss = loss_function(model(images), labels)         train_loss.backward()         optimizer.step()       이를 뜯어보면,    optimizer.zero_grad() 로 기존 MLP 모델에 남아있던 gradient 값들을 다 날리고   train_loss 는 현재 시점에 모델이 이미지를 받아서 추측을 해보고 그 loss function 값을 확인하고,   .backward() 로 현재 시점의 gradient를 계산하고   optimizer.step() 으로 실제 optimization (여기선 SGD)를 수행합니다.   그렇다면, 이 모델은 얼마나 정확할까요?         test_loss, correct = 0, 0 for ind, (image, label) in enumerate(test_loader) :     image, label = image.to(device), label.to(device)     output = model(image)     test_loss += loss_function(output, label).item()     pred = output.max(1, keepdim=True)[1]     correct += pred.eq(label.view_as(pred)).sum().item()  print(f&#39;&#39;&#39;[Test set]\nAverage loss: {test_loss /len(test_loader):.4f},  Accuracy: {correct}/{len(test_loader)} ({100. * correct / len(test_loader):.2f}%)&#39;&#39;&#39;)      이 코드는 테스트셋을 모두 한바퀴 돌리면서, test loss의 값과 결과의 정확도를 확인합니다.  초기화 상황 등에 따라 조금 달라질수는 있을텐데, 저는 2번의 epoch로 92.11%의 정확도를 얻을 수 있었습니다.  Multi-Layer Perceptron Pytorch는 굉장히 쓰기 편한 모듈인데, 위 코드에서 정말 model만 바꾸면 바로 MLP를 사용해볼수 있습니다. MLP를 수행하면서 점점 개수가 줄어들어야 하는데, 여기서는 크게 중요하지 않으므로 그냥 편의상 점점 줄어드는 이쁜 값을 몇개 적어넣겠습니다.         class MLP(nn.Module) :     def __init__(self,) :         super().__init__()         self.linear = nn.Linear(784, 256, bias=True)         self.linear2 = nn.Linear(256, 128, bias=True)         self.linear3 = nn.Linear(128, 64, bias=True)         self.linear4 = nn.Linear(64, 10, bias=True)              def forward(self, x) :         x = x.float().view(-1, 784)         x = nn.functional.relu(self.linear(x))         x = nn.functional.relu(self.linear2(x))         x = nn.functional.relu(self.linear3(x))         x = self.linear4(x)         return x      model = MLP().to(device)      이 모델은 activation function으로 ReLU를 쓰는 depth 4짜리 MLP인데, 784 -&gt; 256 -&gt; 128 -&gt; 64 -&gt; 10으로 단계적으로 개수를 줄여나갑니다. 순서대로 Linear-&gt;ReLU-&gt;…-&gt;Linear로 끝납니다.  위 코드에서 loss_function 정의부터는 그대로 실행하면 됩니다. 저는 로컬에서 정확도가 96% 정도 나오고, 참을성을 갖고 Epoch를 10으로 바꿨을 때는 98%의 정확도를 얻을 수 있었습니다.  Q. 왜 4단계 layer를 (3단계, 5단계가 아니라…) 쓰나요? -&gt; 일반적으로 Layer가 깊어질수록 파라미터가 많아져서 training 시간이 오래 걸리고 capacity가 커집니다. 반대로, layer가 얕으면 non-linearity를 충분히 주지 못해서 underfitting할 우려가 있습니다.  그렇다고 해서 문제로부터 바로 몇 Layer짜리 MLP를 쓸지 결정할 수 있는 것은 아닙니다. 해보니까 저는 4 Layer 정도가 가장 적당해 보였습니다.">


  <meta name="author" content="Wonseok Shin">
  
  <meta property="article:author" content="Wonseok Shin">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="ko_KR">
<meta property="og:site_name" content="Gratus907's Study Note">
<meta property="og:title" content="Softmax Regression / MLP로 MNIST 풀어보기">
<meta property="og:url" content="http://localhost:4000/deep-learning-study/mnist-mlp/">


  <meta property="og:description" content="Contents     Problem / Dataset   Softmax Regression   Multi-Layer Perceptron    심층 신경망의 수학적 기초 5강, 6강 (9월 16일, 23일) 에 기반합니다. 이번 내용은 대부분이 코드에 대한 내용이라서, $\LaTeX$ 노트를 변환하지 않고 여기에 바로 작성했습니다.  아직 읽지 않았다면, 최소한 Softmax(링크)와 MLP(링크)에 대한 포스팅 을, 되도록 링크 에 있는 포스팅 중 shallow-nn과 SVM, LR에 대한 내용을 읽으면 이론적 배경이 충분할 것으로 생각합니다 (제가 이 내용을 공부해서 이해한대로 정리했으니까요..?)  Problem / Dataset 이번에 해결하고자 하는 문제는, MNIST라는 매우 유명한 데이터셋을 이용합니다. MNIST는 Hand-written 숫자로 구성된 데이터셋으로, 널리 알려진 CNN 모델을 처음 제시한 LeCun의 연구에 사용되었던 데이터셋이기도 합니다. 실제로 작동하는 딥 러닝을 만들기에는 너무 작은 데이터셋이지만 공부하는 목적으로 주로 사용됩니다.  각 이미지는 28 by 28 grayscale image로, 편의상 $\R^{28 \times 28}$ 으로 생각하면 됩니다.  가장 먼저 해야할 일은 pytorch module을 import하고, 데이터를 받아오고 정리하는 것입니다. Pytorch에서는 DataLoader라는 모듈을 이용하여, 편하게 데이터를 Batch로 먹인다거나 하는 작업을 할 수 있습니다.          import torch import torch.nn as nn from torch.optim import Optimizer from torch.utils.data import DataLoader from torchvision import datasets from torchvision.transforms import transforms  train_set = datasets.MNIST(root=&#39;./mnist_data/&#39;, train=True, transform=transforms.ToTensor(), download=True) test_set = datasets.MNIST(root=&#39;./mnist_data/&#39;, train=False, transform=transforms.ToTensor(), download=True) train_loader = DataLoader(dataset=train_set, batch_size=64, shuffle=True) test_loader = DataLoader(dataset=test_set, batch_size=1, shuffle=False) device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)       device = 부분은 가능하다면 cuda GPU를 사용하도록 하는 부분인데, 사실 이번 태스크는 너무 작기 때문에 GPU를 쓰면 이득이 없거나 오히려 더 느려질 수도 있습니다.  Softmax Regression 먼저, 우리는 Softmax regression을 시도해 보겠습니다. Pytorch에서는 다음과 같은 과정으로 머신러닝 모델을 학습합니다.    Model 정의 : 입력 $x$를 어떤 과정을 거쳐 출력값으로 만들지를 정의합니다. 이 모델에는 훈련가능한 parameter가 있습니다.   Loss function 정의 : 모델이 어떤 방법으로 현재 정확도를 측정할지를 정의합니다.   학습 : Training data를 이용하여 모델의 파라미터를 조정합니다.   이제, Model을 정의해야 합니다. Model은 기본적으로 softmax regression에서 공부했던 모델로, $Ax + b$ 를 10개 만들어야 합니다.  여기서, __init__ 을 이용하여 이 모델에서 쓸 Layer들을 정의하고, 그 Layer들을 forward 메서드를 통해 사용해서 넘겨주면 됩니다.         class SoftMax(nn.Module):     def __init__(self):         super().__init__()         self.layer = nn.Linear(28*28, 10, bias=True)     def forward(self, x):         return self.layer(x.float().view(-1, 28 * 28))  model = SoftMax()      Softmax Regression은 이렇게 정의된 model에 다음과 같은 최적화 문제를 푸는 방법입니다.  (\underset{a \in \R^{k \times n}, b \in \R^k}{\minimize}\ \frac{1}{N}\sum_{i = 1}^{N}  \left(-(a_{Y_i}^T X_i + b_{Y_i} + \log\left(\sum_{j = 1}^{k} e^{a_j^TX_i + b}\right)\right))  pytorch에는 $Ax + b$의 결과만 받아내면 이를 계산해주는 함수가 있으므로, 모델은 $Ax + b$만 해주면 됩니다. 대신 Loss function을 다음과 같이 사용합니다. 여기서 lr 은 learning rate, $\alpha$ 값을 의미합니다. 최적화 자체에는 Stochastic Gradient Descent를 사용하겠습니다.         loss_function = torch.nn.CrossEntropyLoss()     optimizer = torch.optim.SGD(model.parameters(), lr=0.03)          이제, 우리에게 필요한 모델의 훈련 과정은 이렇게 진행됩니다. Epoch를 많이 돌릴수록 정확해지긴 하지만, 투입하는 시간 대비 얼만큼의 효율이 있는지는 상황마다 다르므로 적당히 판단할 필요가 있습니다.  워낙 단순한 모델이라 학습할게 많지 않으므로 여기서는 epoch=2 (즉, 전체 데이터를 두바퀴 돌립니다) 만 돌리겠습니다.          NUM_EPOCH = 2 for epoch in range(NUM_EPOCH) :     for images, labels in train_loader :         optimizer.zero_grad()         train_loss = loss_function(model(images), labels)         train_loss.backward()         optimizer.step()       이를 뜯어보면,    optimizer.zero_grad() 로 기존 MLP 모델에 남아있던 gradient 값들을 다 날리고   train_loss 는 현재 시점에 모델이 이미지를 받아서 추측을 해보고 그 loss function 값을 확인하고,   .backward() 로 현재 시점의 gradient를 계산하고   optimizer.step() 으로 실제 optimization (여기선 SGD)를 수행합니다.   그렇다면, 이 모델은 얼마나 정확할까요?         test_loss, correct = 0, 0 for ind, (image, label) in enumerate(test_loader) :     image, label = image.to(device), label.to(device)     output = model(image)     test_loss += loss_function(output, label).item()     pred = output.max(1, keepdim=True)[1]     correct += pred.eq(label.view_as(pred)).sum().item()  print(f&#39;&#39;&#39;[Test set]\nAverage loss: {test_loss /len(test_loader):.4f},  Accuracy: {correct}/{len(test_loader)} ({100. * correct / len(test_loader):.2f}%)&#39;&#39;&#39;)      이 코드는 테스트셋을 모두 한바퀴 돌리면서, test loss의 값과 결과의 정확도를 확인합니다.  초기화 상황 등에 따라 조금 달라질수는 있을텐데, 저는 2번의 epoch로 92.11%의 정확도를 얻을 수 있었습니다.  Multi-Layer Perceptron Pytorch는 굉장히 쓰기 편한 모듈인데, 위 코드에서 정말 model만 바꾸면 바로 MLP를 사용해볼수 있습니다. MLP를 수행하면서 점점 개수가 줄어들어야 하는데, 여기서는 크게 중요하지 않으므로 그냥 편의상 점점 줄어드는 이쁜 값을 몇개 적어넣겠습니다.         class MLP(nn.Module) :     def __init__(self,) :         super().__init__()         self.linear = nn.Linear(784, 256, bias=True)         self.linear2 = nn.Linear(256, 128, bias=True)         self.linear3 = nn.Linear(128, 64, bias=True)         self.linear4 = nn.Linear(64, 10, bias=True)              def forward(self, x) :         x = x.float().view(-1, 784)         x = nn.functional.relu(self.linear(x))         x = nn.functional.relu(self.linear2(x))         x = nn.functional.relu(self.linear3(x))         x = self.linear4(x)         return x      model = MLP().to(device)      이 모델은 activation function으로 ReLU를 쓰는 depth 4짜리 MLP인데, 784 -&gt; 256 -&gt; 128 -&gt; 64 -&gt; 10으로 단계적으로 개수를 줄여나갑니다. 순서대로 Linear-&gt;ReLU-&gt;…-&gt;Linear로 끝납니다.  위 코드에서 loss_function 정의부터는 그대로 실행하면 됩니다. 저는 로컬에서 정확도가 96% 정도 나오고, 참을성을 갖고 Epoch를 10으로 바꿨을 때는 98%의 정확도를 얻을 수 있었습니다.  Q. 왜 4단계 layer를 (3단계, 5단계가 아니라…) 쓰나요? -&gt; 일반적으로 Layer가 깊어질수록 파라미터가 많아져서 training 시간이 오래 걸리고 capacity가 커집니다. 반대로, layer가 얕으면 non-linearity를 충분히 주지 못해서 underfitting할 우려가 있습니다.  그렇다고 해서 문제로부터 바로 몇 Layer짜리 MLP를 쓸지 결정할 수 있는 것은 아닙니다. 해보니까 저는 4 Layer 정도가 가장 적당해 보였습니다.">







  <meta property="article:published_time" content="2021-10-06T00:00:00+09:00">





  

  


<link rel="canonical" href="http://localhost:4000/deep-learning-study/mnist-mlp/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Wonseok Shin",
      "url": "http://localhost:4000/"
    
  }
</script>


  <meta name="google-site-verification" content="google47004f86a38ab187.html" />





  <meta name="naver-site-verification" content="navere133bbae47e70cd9311ce1c7d5cf18d7.html">

<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Gratus907's Study Note Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

    <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [ ['$$', '$$'], ['\\[', '\\]'], ['\\(', '\\)']],
      packages: {'[+]': ['physics']}
    },
    loader: {
      load: ["input/tex", "output/chtml", '[tex]/physics']
    },
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>
<p style="display: none;">$$
    \newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
    \newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
    \newcommand{\N}{\mathbb{N}}
    \newcommand{\R}{\mathbb{R}}
    \newcommand{\Z}{\mathbb{Z}}
    \newcommand{\Q}{\mathbb{Q}}
    \newcommand{\C}{\mathbb{C}}
    \renewcommand{\L}{\mathcal{L}}
    \newcommand{\x}{\times}
    \newcommand{\contra}{\scalebox{1.5}{$\lightning$}}
    \newcommand{\inner}[2]{\left\langle #1 , #2 \right\rangle}
    \newcommand{\st}{\text{ such that }}
    \newcommand{\for}{\text{ for }}
    \newcommand{\Setcond}[2]{ \left\{\, #1 \mid #2 \, \right\}}
    \newcommand{\setcond}[2]{\Setcond{#1}{#2}}
    \newcommand{\seq}[1]{ \left\langle #1 \right\rangle}
    \newcommand{\Set}[1]{ \left\{ #1 \right\}}
    \newcommand{\set}[1]{ \set{#1} }
    \newcommand{\sgn}{\text{sign}}
    \newcommand{\halfline}{\vspace{0.5em}}
    \newcommand{\diag}{\text{diag}}

    \newcommand{\legn}[2]{\left(\frac{#1}{#2}\right)} 
    \newcommand{\ord}{\text{ord}}
    \newcommand{\di}{\mathrel{|}} 
    \newcommand{\gen}[1] 
    \newcommand{\irr}{\mathrm{irr }}
    \renewcommand{\deg}{\mathrm{deg }}
    \newcommand{\nsgeq}{\trianglelefteq}
    \newcommand{\nsg}{\triangleleft}
    
    \newcommand{\argmin}{\mathrm{argmin}}
    \newcommand{\argmax}{\mathrm{argmax}}
    \newcommand{\minimize}{\mathrm{minimize}}
    \newcommand{\maximize}{\mathrm{maximize}}
    \newcommand{\subto}{\mathrm{subject\ to}}
    \newcommand{\DKL}[2]{D_{\mathrm{KL}}\left(#1 \di\di #2\right)}
    \newcommand{\ReLU}{\mathrm{ReLU}}
    
    \newcommand{\E}{\mathbb{E}}
    \newcommand{\expect}[1]{\E\left[#1\right]}
    \newcommand{\expectwith}[2]{\E_{#1}\left[#2\right]}
    \renewcommand{\P}{\mathbb{P}}
    \newcommand{\uniform}[2]{\mathrm{Uniform}\left(#1 \dots #2\right)}
    \newcommand{\gdist}[2]{\mathcal{N}\left(#1, #2\right)}
    $$</p>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-9M2LK7DWFS"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-9M2LK7DWFS');
    </script>
  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Gratus907's Study Note
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/postings/">Posts</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/">Categories</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tags</a>
            </li><li class="masthead__menu-item">
              <a href="/about-me/">About me</a>
            </li><li class="masthead__menu-item">
              <a href="/about-blog/">About Blog</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">토글 메뉴</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




<div id="main" role="main">
  
  <aside class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Wonseok Shin</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>2018- SNU CSE, interested in algorithms, mathematics</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">팔로우</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Seoul, Korea</span>
        </li>
      

      
        
          
        
          
        
          
            <li><a href="https://github.com/gratus907" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
      

      

      
        <li>
          <a href="mailto:gratus907@snu.ac.kr">
            <meta itemprop="email" content="gratus907@snu.ac.kr" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">이메일</span>
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
    
      
      
      
      
    
    
      

<nav class="nav__list">
  
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">토글 메뉴</label>
  <ul class="nav__items">
    
      <li>
        
          <a href="/cs-adventure/"><span class="nav__sub-title">CS 논문읽기</span></a>
        

        
      </li>
    
      <li>
        
          <span class="nav__sub-title">Problem Solving</span>
        

        
        <ul>
          
            <li><a href="/ps-weekly/">PS Weekly</a></li>
          
            <li><a href="/find-contest/">문제 출처별로 보기</a></li>
          
            <li><a href="/ps-teatime/">PS Teatime</a></li>
          
            <li><a href="/competitive-programming-rounds/">Competitive Programming</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          <span class="nav__sub-title">주제별 노트정리</span>
        

        
        <ul>
          
            <li><a href="/ds-alg-note/">자료구조/알고리즘</a></li>
          
            <li><a href="/ds-alg-advanced/">고급 자료구조/알고리즘</a></li>
          
            <li><a href="/deep-learning-study/">Deep Learning Notes</a></li>
          
        </ul>
        
      </li>
    
  </ul>
</nav>

    
    
  
  </aside>


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">

    <meta itemprop="headline" content="Softmax Regression / MLP로 MNIST 풀어보기">
    <meta itemprop="description" content="  Contents  Problem / Dataset  Softmax Regression  Multi-Layer Perceptron심층 신경망의 수학적 기초 5강, 6강 (9월 16일, 23일) 에 기반합니다. 이번 내용은 대부분이 코드에 대한 내용이라서, $\LaTeX$ 노트를 변환하지 않고 여기에 바로 작성했습니다.아직 읽지 않았다면, 최소한 Softmax(링크)와 MLP(링크)에 대한 포스팅 을, 되도록 링크 에 있는 포스팅 중 shallow-nn과 SVM, LR에 대한 내용을 읽으면 이론적 배경이 충분할 것으로 생각합니다 (제가 이 내용을 공부해서 이해한대로 정리했으니까요..?)Problem / Dataset이번에 해결하고자 하는 문제는, MNIST라는 매우 유명한 데이터셋을 이용합니다. MNIST는 Hand-written 숫자로 구성된 데이터셋으로, 널리 알려진 CNN 모델을 처음 제시한 LeCun의 연구에 사용되었던 데이터셋이기도 합니다. 실제로 작동하는 딥 러닝을 만들기에는 너무 작은 데이터셋이지만 공부하는 목적으로 주로 사용됩니다.각 이미지는 28 by 28 grayscale image로, 편의상 $\R^{28 \times 28}$ 으로 생각하면 됩니다.가장 먼저 해야할 일은 pytorch module을 import하고, 데이터를 받아오고 정리하는 것입니다.Pytorch에서는 DataLoader라는 모듈을 이용하여, 편하게 데이터를 Batch로 먹인다거나 하는 작업을 할 수 있습니다.      import torchimport torch.nn as nnfrom torch.optim import Optimizerfrom torch.utils.data import DataLoaderfrom torchvision import datasetsfrom torchvision.transforms import transformstrain_set = datasets.MNIST(root=&#39;./mnist_data/&#39;, train=True, transform=transforms.ToTensor(), download=True)test_set = datasets.MNIST(root=&#39;./mnist_data/&#39;, train=False, transform=transforms.ToTensor(), download=True)train_loader = DataLoader(dataset=train_set, batch_size=64, shuffle=True)test_loader = DataLoader(dataset=test_set, batch_size=1, shuffle=False)device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)  device = 부분은 가능하다면 cuda GPU를 사용하도록 하는 부분인데, 사실 이번 태스크는 너무 작기 때문에 GPU를 쓰면 이득이 없거나 오히려 더 느려질 수도 있습니다.Softmax Regression먼저, 우리는 Softmax regression을 시도해 보겠습니다. Pytorch에서는 다음과 같은 과정으로 머신러닝 모델을 학습합니다.  Model 정의 : 입력 $x$를 어떤 과정을 거쳐 출력값으로 만들지를 정의합니다. 이 모델에는 훈련가능한 parameter가 있습니다.  Loss function 정의 : 모델이 어떤 방법으로 현재 정확도를 측정할지를 정의합니다.  학습 : Training data를 이용하여 모델의 파라미터를 조정합니다.이제, Model을 정의해야 합니다. Model은 기본적으로 softmax regression에서 공부했던 모델로, $Ax + b$ 를 10개 만들어야 합니다.여기서, __init__ 을 이용하여 이 모델에서 쓸 Layer들을 정의하고, 그 Layer들을 forward 메서드를 통해 사용해서 넘겨주면 됩니다.      class SoftMax(nn.Module):    def __init__(self):        super().__init__()        self.layer = nn.Linear(28*28, 10, bias=True)    def forward(self, x):        return self.layer(x.float().view(-1, 28 * 28))model = SoftMax()  Softmax Regression은 이렇게 정의된 model에 다음과 같은 최적화 문제를 푸는 방법입니다. (\underset{a \in \R^{k \times n}, b \in \R^k}{\minimize}\ \frac{1}{N}\sum_{i = 1}^{N}  \left(-(a_{Y_i}^T X_i + b_{Y_i} + \log\left(\sum_{j = 1}^{k} e^{a_j^TX_i + b}\right)\right))pytorch에는 $Ax + b$의 결과만 받아내면 이를 계산해주는 함수가 있으므로, 모델은 $Ax + b$만 해주면 됩니다.대신 Loss function을 다음과 같이 사용합니다. 여기서 lr 은 learning rate, $\alpha$ 값을 의미합니다. 최적화 자체에는 Stochastic Gradient Descent를 사용하겠습니다.      loss_function = torch.nn.CrossEntropyLoss()    optimizer = torch.optim.SGD(model.parameters(), lr=0.03)     이제, 우리에게 필요한 모델의 훈련 과정은 이렇게 진행됩니다. Epoch를 많이 돌릴수록 정확해지긴 하지만, 투입하는 시간 대비 얼만큼의 효율이 있는지는 상황마다 다르므로 적당히 판단할 필요가 있습니다.워낙 단순한 모델이라 학습할게 많지 않으므로 여기서는 epoch=2 (즉, 전체 데이터를 두바퀴 돌립니다) 만 돌리겠습니다.      NUM_EPOCH = 2for epoch in range(NUM_EPOCH) :    for images, labels in train_loader :        optimizer.zero_grad()        train_loss = loss_function(model(images), labels)        train_loss.backward()        optimizer.step()  이를 뜯어보면,  optimizer.zero_grad() 로 기존 MLP 모델에 남아있던 gradient 값들을 다 날리고  train_loss 는 현재 시점에 모델이 이미지를 받아서 추측을 해보고 그 loss function 값을 확인하고,  .backward() 로 현재 시점의 gradient를 계산하고  optimizer.step() 으로 실제 optimization (여기선 SGD)를 수행합니다.그렇다면, 이 모델은 얼마나 정확할까요?      test_loss, correct = 0, 0for ind, (image, label) in enumerate(test_loader) :    image, label = image.to(device), label.to(device)    output = model(image)    test_loss += loss_function(output, label).item()    pred = output.max(1, keepdim=True)[1]    correct += pred.eq(label.view_as(pred)).sum().item()print(f&#39;&#39;&#39;[Test set]\nAverage loss: {test_loss /len(test_loader):.4f}, Accuracy: {correct}/{len(test_loader)} ({100. * correct / len(test_loader):.2f}%)&#39;&#39;&#39;)  이 코드는 테스트셋을 모두 한바퀴 돌리면서, test loss의 값과 결과의 정확도를 확인합니다.초기화 상황 등에 따라 조금 달라질수는 있을텐데, 저는 2번의 epoch로 92.11%의 정확도를 얻을 수 있었습니다.Multi-Layer PerceptronPytorch는 굉장히 쓰기 편한 모듈인데, 위 코드에서 정말 model만 바꾸면 바로 MLP를 사용해볼수 있습니다. MLP를 수행하면서 점점 개수가 줄어들어야 하는데, 여기서는 크게 중요하지 않으므로 그냥 편의상 점점 줄어드는 이쁜 값을 몇개 적어넣겠습니다.      class MLP(nn.Module) :    def __init__(self,) :        super().__init__()        self.linear = nn.Linear(784, 256, bias=True)        self.linear2 = nn.Linear(256, 128, bias=True)        self.linear3 = nn.Linear(128, 64, bias=True)        self.linear4 = nn.Linear(64, 10, bias=True)            def forward(self, x) :        x = x.float().view(-1, 784)        x = nn.functional.relu(self.linear(x))        x = nn.functional.relu(self.linear2(x))        x = nn.functional.relu(self.linear3(x))        x = self.linear4(x)        return x    model = MLP().to(device)  이 모델은 activation function으로 ReLU를 쓰는 depth 4짜리 MLP인데, 784 -&gt; 256 -&gt; 128 -&gt; 64 -&gt; 10으로 단계적으로 개수를 줄여나갑니다. 순서대로 Linear-&gt;ReLU-&gt;…-&gt;Linear로 끝납니다.위 코드에서 loss_function 정의부터는 그대로 실행하면 됩니다. 저는 로컬에서 정확도가 96% 정도 나오고, 참을성을 갖고 Epoch를 10으로 바꿨을 때는 98%의 정확도를 얻을 수 있었습니다.Q. 왜 4단계 layer를 (3단계, 5단계가 아니라…) 쓰나요?-&gt; 일반적으로 Layer가 깊어질수록 파라미터가 많아져서 training 시간이 오래 걸리고 capacity가 커집니다. 반대로, layer가 얕으면 non-linearity를 충분히 주지 못해서 underfitting할 우려가 있습니다. 그렇다고 해서 문제로부터 바로 몇 Layer짜리 MLP를 쓸지 결정할 수 있는 것은 아닙니다. 해보니까 저는 4 Layer 정도가 가장 적당해 보였습니다.">
    <meta itemprop="datePublished" content="2021-10-06T00:00:00+09:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Softmax Regression / MLP로 MNIST 풀어보기
</h1>
          

  <p class="page__meta">
    

    
    

    
      
      
      
      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
          758 words
      </span>
    
  </p>


        </header>
      
      <img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgratus907.github.io/deep-learning-study/mnist-mlp/&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false"/></a> </div> <br/>
      <div style="display:none;"> <span id="busuanzi_container_site_pv" style="display:none;"><span id="busuanzi_value_site_pv" style="display:none;"></span></span></div>
      <section class="page__content" itemprop="text"> 
        
<aside class="sidebar__right sticky">
    <nav class="toc">
      <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On This Page</h4></header>
      <ul class="toc__menu">
  <li><a href="#problem--dataset">Problem / Dataset</a></li>
  <li><a href="#softmax-regression">Softmax Regression</a></li>
  <li><a href="#multi-layer-perceptron">Multi-Layer Perceptron</a></li>
</ul>

    </nav>
</aside> 

    
        <div id="toc">
  <p>Contents</p>
</div>
<ul id="markdown-toc">
  <li><a href="#problem--dataset" id="markdown-toc-problem--dataset">Problem / Dataset</a></li>
  <li><a href="#softmax-regression" id="markdown-toc-softmax-regression">Softmax Regression</a></li>
  <li><a href="#multi-layer-perceptron" id="markdown-toc-multi-layer-perceptron">Multi-Layer Perceptron</a></li>
</ul>
<hr />

<p><strong>심층 신경망의 수학적 기초</strong> 5강, 6강 (9월 16일, 23일) 에 기반합니다. 이번 내용은 대부분이 코드에 대한 내용이라서, $\LaTeX$ 노트를 변환하지 않고 여기에 바로 작성했습니다.</p>

<p>아직 읽지 않았다면, 최소한 <a href="/deep-learning-study/softmax-regression">Softmax(링크)</a>와 <a href="/deep-learning-study/multilayer-perceptron">MLP(링크)</a>에 대한 포스팅 을, 되도록 <a href="/deep-learning-study/">링크</a> 에 있는 포스팅 중 shallow-nn과 SVM, LR에 대한 내용을 읽으면 이론적 배경이 충분할 것으로 생각합니다 (제가 이 내용을 공부해서 이해한대로 정리했으니까요..?)</p>

<h2 id="problem--dataset">Problem / Dataset</h2>
<p>이번에 해결하고자 하는 문제는, MNIST라는 매우 유명한 데이터셋을 이용합니다. MNIST는 Hand-written 숫자로 구성된 데이터셋으로, 널리 알려진 CNN 모델을 처음 제시한 LeCun의 연구에 사용되었던 데이터셋이기도 합니다. 실제로 작동하는 딥 러닝을 만들기에는 너무 작은 데이터셋이지만 공부하는 목적으로 주로 사용됩니다.</p>

<p>각 이미지는 28 by 28 grayscale image로, 편의상 $\R^{28 \times 28}$ 으로 생각하면 됩니다.</p>

<p>가장 먼저 해야할 일은 pytorch module을 import하고, 데이터를 받아오고 정리하는 것입니다.<br />
Pytorch에서는 <code class="language-plaintext highlighter-rouge">DataLoader</code>라는 모듈을 이용하여, 편하게 데이터를 Batch로 먹인다거나 하는 작업을 할 수 있습니다.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Optimizer</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">torchvision.transforms</span> <span class="kn">import</span> <span class="n">transforms</span>

<span class="n">train_set</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">'./mnist_data/'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">(),</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">test_set</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">'./mnist_data/'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">(),</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">test_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">"cpu"</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">device =</code> 부분은 가능하다면 cuda GPU를 사용하도록 하는 부분인데, 사실 이번 태스크는 너무 작기 때문에 GPU를 쓰면 이득이 없거나 오히려 더 느려질 수도 있습니다.</p>

<h2 id="softmax-regression">Softmax Regression</h2>
<p>먼저, 우리는 Softmax regression을 시도해 보겠습니다. Pytorch에서는 다음과 같은 과정으로 머신러닝 모델을 학습합니다.</p>
<ul>
  <li>Model 정의 : 입력 $x$를 어떤 과정을 거쳐 출력값으로 만들지를 정의합니다. 이 모델에는 훈련가능한 parameter가 있습니다.</li>
  <li>Loss function 정의 : 모델이 어떤 방법으로 현재 정확도를 측정할지를 정의합니다.</li>
  <li>학습 : Training data를 이용하여 모델의 파라미터를 조정합니다.</li>
</ul>

<p>이제, Model을 정의해야 합니다. Model은 기본적으로 softmax regression에서 공부했던 모델로, $Ax + b$ 를 10개 만들어야 합니다.</p>

<p>여기서, <code class="language-plaintext highlighter-rouge">__init__</code> 을 이용하여 이 모델에서 쓸 Layer들을 정의하고, 그 Layer들을 <code class="language-plaintext highlighter-rouge">forward</code> 메서드를 통해 사용해서 넘겨주면 됩니다.</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SoftMax</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nb">float</span><span class="p">().</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">))</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SoftMax</span><span class="p">()</span>
</code></pre></div></div>
<p>Softmax Regression은 이렇게 정의된 model에 다음과 같은 최적화 문제를 푸는 방법입니다. 
\(\underset{a \in \R^{k \times n}, b \in \R^k}{\minimize}\ \frac{1}{N}\sum_{i = 1}^{N}  \left(-(a_{Y_i}^T X_i + b_{Y_i} + \log\left(\sum_{j = 1}^{k} e^{a_j^TX_i + b}\right)\right)\)</p>

<p>pytorch에는 $Ax + b$의 결과만 받아내면 이를 계산해주는 함수가 있으므로, 모델은 $Ax + b$만 해주면 됩니다.<br />
대신 Loss function을 다음과 같이 사용합니다. 여기서 <code class="language-plaintext highlighter-rouge">lr</code> 은 learning rate, $\alpha$ 값을 의미합니다. 최적화 자체에는 Stochastic Gradient Descent를 사용하겠습니다.</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loss_function</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>    
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.03</span><span class="p">)</span>   
</code></pre></div></div>

<p>이제, 우리에게 필요한 모델의 훈련 과정은 이렇게 진행됩니다. Epoch를 많이 돌릴수록 정확해지긴 하지만, 투입하는 시간 대비 얼만큼의 효율이 있는지는 상황마다 다르므로 적당히 판단할 필요가 있습니다.</p>

<p>워낙 단순한 모델이라 학습할게 많지 않으므로 여기서는 epoch=2 (즉, 전체 데이터를 두바퀴 돌립니다) 만 돌리겠습니다.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">NUM_EPOCH</span> <span class="o">=</span> <span class="mi">2</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">NUM_EPOCH</span><span class="p">)</span> <span class="p">:</span>
    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_loader</span> <span class="p">:</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">train_loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">),</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">train_loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<p>이를 뜯어보면,</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">optimizer.zero_grad()</code> 로 기존 MLP 모델에 남아있던 gradient 값들을 다 날리고</li>
  <li><code class="language-plaintext highlighter-rouge">train_loss</code> 는 현재 시점에 모델이 이미지를 받아서 추측을 해보고 그 loss function 값을 확인하고,</li>
  <li><code class="language-plaintext highlighter-rouge">.backward()</code> 로 현재 시점의 gradient를 계산하고</li>
  <li><code class="language-plaintext highlighter-rouge">optimizer.step()</code> 으로 실제 optimization (여기선 SGD)를 수행합니다.</li>
</ul>

<p>그렇다면, 이 모델은 얼마나 정확할까요?</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_loss</span><span class="p">,</span> <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">ind</span><span class="p">,</span> <span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_loader</span><span class="p">)</span> <span class="p">:</span>
    <span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">label</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
    <span class="n">test_loss</span> <span class="o">+=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">).</span><span class="n">item</span><span class="p">()</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">correct</span> <span class="o">+=</span> <span class="n">pred</span><span class="p">.</span><span class="n">eq</span><span class="p">(</span><span class="n">label</span><span class="p">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">pred</span><span class="p">)).</span><span class="nb">sum</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="s">f'''[Test set]</span><span class="se">\n</span><span class="s">Average loss: </span><span class="si">{</span><span class="n">test_loss</span> <span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="p">):.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, 
Accuracy: </span><span class="si">{</span><span class="n">correct</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="p">)</span><span class="si">}</span><span class="s"> (</span><span class="si">{</span><span class="mf">100.</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="p">):.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%)'''</span><span class="p">)</span>
</code></pre></div></div>
<p>이 코드는 테스트셋을 모두 한바퀴 돌리면서, test loss의 값과 결과의 정확도를 확인합니다.</p>

<p>초기화 상황 등에 따라 조금 달라질수는 있을텐데, 저는 2번의 epoch로 92.11%의 정확도를 얻을 수 있었습니다.</p>

<h2 id="multi-layer-perceptron">Multi-Layer Perceptron</h2>
<p>Pytorch는 굉장히 쓰기 편한 모듈인데, 위 코드에서 정말 <code class="language-plaintext highlighter-rouge">model</code>만 바꾸면 바로 MLP를 사용해볼수 있습니다. MLP를 수행하면서 점점 개수가 줄어들어야 하는데, 여기서는 크게 중요하지 않으므로 그냥 편의상 점점 줄어드는 이쁜 값을 몇개 적어넣겠습니다.</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">)</span> <span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,)</span> <span class="p">:</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear4</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nb">float</span><span class="p">().</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">linear3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">linear4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
    
<span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">().</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div>
<p>이 모델은 activation function으로 ReLU를 쓰는 depth 4짜리 MLP인데, 784 -&gt; 256 -&gt; 128 -&gt; 64 -&gt; 10으로 단계적으로 개수를 줄여나갑니다. 순서대로 Linear-&gt;ReLU-&gt;…-&gt;Linear로 끝납니다.</p>

<p>위 코드에서 <code class="language-plaintext highlighter-rouge">loss_function</code> 정의부터는 그대로 실행하면 됩니다. 저는 로컬에서 정확도가 96% 정도 나오고, 참을성을 갖고 Epoch를 10으로 바꿨을 때는 98%의 정확도를 얻을 수 있었습니다.</p>

<p>Q. 왜 4단계 layer를 (3단계, 5단계가 아니라…) 쓰나요?<br />
-&gt; 일반적으로 Layer가 깊어질수록 파라미터가 많아져서 training 시간이 오래 걸리고 capacity가 커집니다. 반대로, layer가 얕으면 non-linearity를 충분히 주지 못해서 underfitting할 우려가 있습니다. 
그렇다고 해서 문제로부터 바로 몇 Layer짜리 MLP를 쓸지 결정할 수 있는 것은 아닙니다. 해보니까 저는 4 Layer 정도가 가장 적당해 보였습니다.</p>

        
      </section>

      <footer class="page__meta">
        
        


  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> 카테고리: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/categories/#deep-learning-study" class="page__taxonomy-item" rel="tag">deep-learning-study</a>
    
    </span>
  </p>


        
  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> 업데이트:</strong> <time datetime="2021-10-06T00:00:00+09:00">October 6, 2021</time></p>


      </footer>
      
  <nav class="pagination">
    
      <a href="/cp-rounds/team-practice-gcpc-2020/" class="pagination--pager" title="GCPC 2020 팀연습
">이전</a>
    
    
      <a href="/deep-learning-study/multilayer-perceptron/" class="pagination--pager" title="[P] Multi Layer Perceptron
">다음</a>
    
  </nav>

      
      <div id="disqus_thread"></div>
      <script>
        /**
        *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
        *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */

        var disqus_config = function () {
            this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
            this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
        };

        (function() { // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        s.src = 'https://gratus907-github-io.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
        })();
      </script>
      <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    
    
    

    
      <div class="page__related">
        <h4 class="page__related-title">참고</h4>
        <div class="grid__wrapper">
          
            



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deep-learning-study/convolutionary-neural-networks/" rel="permalink">Convolutionary Neural Networks : Introduction
</a>
      
    </h2>
    

  <p class="page__meta">
    

    
    

    
      
      
      
      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
          949 words
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
  Contents


  Convolution
  Pooling
  Convolutionary Neural Network
  Why CNN?
  Next posts



심층 신경망의 수학적 기초 7강 (9월 28일) 에 기반합니다.

매우 유명한 Standford CS231n...</p>
  </article>
</div>

          
            



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/cp-rounds/icpc-2021-prelim/" rel="permalink">ICPC Korea First Round 2021 후기 / 풀이
</a>
      
    </h2>
    

  <p class="page__meta">
    

    
    

    
      
      
      
      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
          3102 words
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
  Contents


  Preperation / Our Team
  본 대회    
      Problem I : Sport Climbing Combined
      Problem E : Histogram
      Problem J : Ten
      Problem H...</p>
  </article>
</div>

          
            



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deep-learning-study/softmax-regression/" rel="permalink">[P] Softmax Regression
</a>
      
    </h2>
    

  <p class="page__meta">
    

    
    

    
      
      
      
      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
          499 words
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
  Contents


  Softmax Regression



심층 신경망의 수학적 기초 6강 (9월 23일) 에 기반합니다.

이 글은 SVM과 Logistic Regression 링크 에 이어지는 내용입니다.

나중에 설명을 보강해서 다시 작성될 예정입니다.



데이터 ...</p>
  </article>
</div>

          
            



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deep-learning-study/multilayer-perceptron/" rel="permalink">[P] Multi Layer Perceptron
</a>
      
    </h2>
    

  <p class="page__meta">
    

    
    

    
      
      
      
      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
          472 words
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
  Contents


  Linear Layer
  Multi Layer Perceptron
  Weight Initialization
  Gradient Computation : Back propagation



심층 신경망의 수학적 기초 6강 (9월 23일) 에 기반합니다...</p>
  </article>
</div>

          
        </div>
      </div>
    
    
  </div>
</article>
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>팔로우:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> 피드</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2021 Wonseok Shin. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>. Version 1e741c265186799df1a292e6cb2508b1f5f49ca7</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>





  <script>
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'G-TNVQ3G5D5B']);
  
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>







    <script>
    
    var elements = document.querySelectorAll('p');
    Array.prototype.forEach.call(elements, function(el, i){
        if(el.innerHTML=='[expand]') {
            var parentcontent = el.parentNode.innerHTML.replace('<p>[expand]</p>','<div class="expand" style="display: none; height: 0; overflow: hidden;">').replace('<p>[/expand]</p>','</div>');
            el.parentNode.innerHTML = parentcontent;
        }
    });

    var elements = document.querySelectorAll('div.expand');
    Array.prototype.forEach.call(elements, function(el, i){
        el.previousElementSibling.innerHTML = el.previousElementSibling.innerHTML + '<span>..&nbsp; <a href="#" style="cursor: pointer;" onclick="this.parentNode.parentNode.nextElementSibling.style.display = \'block\'; this.parentNode.parentNode.nextElementSibling.style.height = \'auto\'; this.parentNode.style.display = \'none\';">read&nbsp;more&nbsp;&rarr;</a></span>';
    });

</script>

  </body>
</html>
