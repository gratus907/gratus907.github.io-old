<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.21.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="ko" class="no-js">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-TNVQ3G5D5B"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-TNVQ3G5D5B');
</script>

    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>[P] Shallow Neural Networks - Introduction - Gratus907’s Study Note</title>
<meta name="description" content="Contents     Shallow Neural Network : Introduction   KL-Divergence    심층 신경망의 수학적 기초 3강 (9월 9일), 4강 (9월 14일) 에 기반합니다.  이 문서는 $\LaTeX$를 pandoc으로 변환하여 작성하였기 때문에, 레이아웃 등이 깔끔하지 않을 수 있습니다. 언젠가 pdf 버전의 노트를 공개한다면 그쪽을 참고하면 좋을 것 같습니다.  Shallow Neural Network : Introduction  데이터 $X_1, \dots X_n \in \mathcal{X}$이 있고, 이에 대한 정답 라벨 $Y_1, \dots Y_n \in \mathcal{Y}$이 주어진 경우를 생각해 보자. 이때, 어떤 True Unknown Function $f_\star : \mathcal{X} \to \mathcal{Y}$ 가 있다고 생각하면, $Y_i = f_\star(X_i)$ 를 만족한다.  우리는, $X_i, Y_i$로부터, $f_\star$과 가까운 어떤 함수 $f$를 찾아내는 작업을 수행하고 싶다. $X_i$들에 대해 $Y_i$는 사람이 수집한 데이터를 쓰기 때문에, 이를 Supervised Learning이라고 부른다.  뭔가를 시작하기 전에, 일단 $f_\star$과 가까운 $f$가 도대체 무슨 말인지를 명확히 해야 한다. 뭔가를 최소화하는 문제로 만들고 싶은데… 가장 자명한 방법으로 생각하면 어떤 손실함수 $\ell$을 도입해서, 이렇게 쓰고 싶다. (\underset{f \in \mathcal{F}}{\minimize}\ \sup_{x \in \mathcal{X}} \ell(f(x), f_\star(x))) 이 문제는, (1) 모든 가능한 함수들의 공간 위에서 뭔가를 최적화한다는 것은 알고리즘적으로 말이 안 되고, (2) 이 최적화 문제의 해는 $f_\star$이니까, 사실 최적화 문제도 딱히 아니다. 모든 $x$에 대해 $f_\star$를 알고 있으면 최적화를 생각할 이유가 없다.  대신에, 함수들의 공간을 제약하자. 어떤 파라미터 $\theta$를 이용하여, 우리는 다음과 같은 최적화 문제로 바꾸고 싶다. (\underset{\theta \in \Theta}{\minimize}\ \sup_{x \in \mathcal{X}} \ell(f_\theta(x), f_\star(x)))  여전히, 일단 우리는 모든 $x$에 대해 $f_\star$를 알고 있지 않다. 우리가 알고 있는 $x_1, x_2, \dots$ 에 대한 답 $y_1, y_2 \dots$ 들을 맞춰낼 수 있는 함수를 일단 만드는 정도가 최선이 아닐까? 그리고, 최악의 경우를 최소화하는 대신, 평균을 최적화하는게 뭔가 ‘일반적으로’ 좋은 솔루션을 제공할 것 같다. supremum을 최소화한다는 것은 너무 지나친 목표이다. (\underset{\theta \in \Theta}{\minimize}\ \frac{1}{N}\sum_{i = 1}^{N} \ell(f_\theta(x_i), f_\star(x_i))) 우리는 $f_\star(x_i) = y_i$ 임을 알고 있으므로, 이제 뭔가가 가능하다.  이제, $\theta$를 이용하여 표현되는 $f_\theta$를 model 또는 neural network라고 부를 것이다. 또한, 이 최적화 문제를 푸는 작업을 training 이라고 부를 것이다. 즉, 파라미터를 이용해서 표현한 모델 $f_\theta$를 SGD와 같은 알고리즘을 이용하여 training한다는 표현이 된다. 현재 거의 모든 방법들이 SGD에 기반하고 있다.  Example : Least square regression  $\mathcal{X} = \R^p, \mathcal{Y} = \R, \Theta = \R^p$이고, 모델 $f_\theta(x) = x^T \theta$, $L(y_1, y_2) = \frac{1}{2}(y_1 - y_2)^2$ 인 문제를 Least square라고 부른다. 즉, 주어진 데이터들을 비슷하게 맞춰내는 Linear한 함수를 찍는 것.  KL-Divergence  As a mathematical tool, 어떤 $p, q \in \R^n$이 probability mass vector일 때, 즉 $p_i, q_i \geq 0$ 이고 $\sum p_i = \sum q_i = 1$일 때, 우리는 두 distribution의 차이를 생각하고 싶다.  Kullback-Leibler Divergence (KL-Divergence)를 다음과 같이 정의한다. (\DKL{p}{q} = \sum_{i = 1}^{n} p_i \log\frac{p_i}{q_i} = -\sum_{i = 1}^{n} p_i \log q_i + \sum_{i = 1}^{n} p_i \log p_i)          이는 다시, 정보이론의 용어로는 Cross entropy $H(p, q)$ 와 Entropy $H(p)$의 합으로 쓰여진다.           편의를 위해 (자연스럽게), $0 \log (0 / 0) = 0$ 으로, $0 \log 0 = 0$ 으로, $x &gt; 0$이면 $x \log (x / 0) = \infty$ 으로 둔다.      몇가지 성질들을 살펴보면…     $\DKL{p}{q}$ 는 일반적으로 $\DKL{q}{p}$ 와 같지 않다. (그래서   metric은 아님)        $\DKL{p}{q} \geq 0$ 이고, $p \neq q$ 이면 $\DKL{p}{q} &gt; 0$ (과제)      $\DKL{p}{q} = \infty$ 인 경우도 가능.   KL-Divergence를 확률론의 notation으로 쓰면, random variable $I$가 $p_i$의 확률분포를 가질 때, (\DKL{p}{q} = \expectwith{I}{\log\left(\frac{p_i}{q_i}\right)}) 이렇게 expectation으로 쓸 수도 있다.  Symmetrized version $(\DKL{p}{q} + \DKL{q}{p}) / 2$ 같은 것을 생각하면? $\Rightarrow$ Jensen-Shannon Divergence라고 부르는데, 그래도 여전히 infinity라는 문제가 남아서 메트릭이 되지는 않는다.">


  <meta name="author" content="Wonseok Shin">
  
  <meta property="article:author" content="Wonseok Shin">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="ko_KR">
<meta property="og:site_name" content="Gratus907's Study Note">
<meta property="og:title" content="[P] Shallow Neural Networks - Introduction">
<meta property="og:url" content="http://localhost:4000/deep-learning-study/shallow-nn/">


  <meta property="og:description" content="Contents     Shallow Neural Network : Introduction   KL-Divergence    심층 신경망의 수학적 기초 3강 (9월 9일), 4강 (9월 14일) 에 기반합니다.  이 문서는 $\LaTeX$를 pandoc으로 변환하여 작성하였기 때문에, 레이아웃 등이 깔끔하지 않을 수 있습니다. 언젠가 pdf 버전의 노트를 공개한다면 그쪽을 참고하면 좋을 것 같습니다.  Shallow Neural Network : Introduction  데이터 $X_1, \dots X_n \in \mathcal{X}$이 있고, 이에 대한 정답 라벨 $Y_1, \dots Y_n \in \mathcal{Y}$이 주어진 경우를 생각해 보자. 이때, 어떤 True Unknown Function $f_\star : \mathcal{X} \to \mathcal{Y}$ 가 있다고 생각하면, $Y_i = f_\star(X_i)$ 를 만족한다.  우리는, $X_i, Y_i$로부터, $f_\star$과 가까운 어떤 함수 $f$를 찾아내는 작업을 수행하고 싶다. $X_i$들에 대해 $Y_i$는 사람이 수집한 데이터를 쓰기 때문에, 이를 Supervised Learning이라고 부른다.  뭔가를 시작하기 전에, 일단 $f_\star$과 가까운 $f$가 도대체 무슨 말인지를 명확히 해야 한다. 뭔가를 최소화하는 문제로 만들고 싶은데… 가장 자명한 방법으로 생각하면 어떤 손실함수 $\ell$을 도입해서, 이렇게 쓰고 싶다. (\underset{f \in \mathcal{F}}{\minimize}\ \sup_{x \in \mathcal{X}} \ell(f(x), f_\star(x))) 이 문제는, (1) 모든 가능한 함수들의 공간 위에서 뭔가를 최적화한다는 것은 알고리즘적으로 말이 안 되고, (2) 이 최적화 문제의 해는 $f_\star$이니까, 사실 최적화 문제도 딱히 아니다. 모든 $x$에 대해 $f_\star$를 알고 있으면 최적화를 생각할 이유가 없다.  대신에, 함수들의 공간을 제약하자. 어떤 파라미터 $\theta$를 이용하여, 우리는 다음과 같은 최적화 문제로 바꾸고 싶다. (\underset{\theta \in \Theta}{\minimize}\ \sup_{x \in \mathcal{X}} \ell(f_\theta(x), f_\star(x)))  여전히, 일단 우리는 모든 $x$에 대해 $f_\star$를 알고 있지 않다. 우리가 알고 있는 $x_1, x_2, \dots$ 에 대한 답 $y_1, y_2 \dots$ 들을 맞춰낼 수 있는 함수를 일단 만드는 정도가 최선이 아닐까? 그리고, 최악의 경우를 최소화하는 대신, 평균을 최적화하는게 뭔가 ‘일반적으로’ 좋은 솔루션을 제공할 것 같다. supremum을 최소화한다는 것은 너무 지나친 목표이다. (\underset{\theta \in \Theta}{\minimize}\ \frac{1}{N}\sum_{i = 1}^{N} \ell(f_\theta(x_i), f_\star(x_i))) 우리는 $f_\star(x_i) = y_i$ 임을 알고 있으므로, 이제 뭔가가 가능하다.  이제, $\theta$를 이용하여 표현되는 $f_\theta$를 model 또는 neural network라고 부를 것이다. 또한, 이 최적화 문제를 푸는 작업을 training 이라고 부를 것이다. 즉, 파라미터를 이용해서 표현한 모델 $f_\theta$를 SGD와 같은 알고리즘을 이용하여 training한다는 표현이 된다. 현재 거의 모든 방법들이 SGD에 기반하고 있다.  Example : Least square regression  $\mathcal{X} = \R^p, \mathcal{Y} = \R, \Theta = \R^p$이고, 모델 $f_\theta(x) = x^T \theta$, $L(y_1, y_2) = \frac{1}{2}(y_1 - y_2)^2$ 인 문제를 Least square라고 부른다. 즉, 주어진 데이터들을 비슷하게 맞춰내는 Linear한 함수를 찍는 것.  KL-Divergence  As a mathematical tool, 어떤 $p, q \in \R^n$이 probability mass vector일 때, 즉 $p_i, q_i \geq 0$ 이고 $\sum p_i = \sum q_i = 1$일 때, 우리는 두 distribution의 차이를 생각하고 싶다.  Kullback-Leibler Divergence (KL-Divergence)를 다음과 같이 정의한다. (\DKL{p}{q} = \sum_{i = 1}^{n} p_i \log\frac{p_i}{q_i} = -\sum_{i = 1}^{n} p_i \log q_i + \sum_{i = 1}^{n} p_i \log p_i)          이는 다시, 정보이론의 용어로는 Cross entropy $H(p, q)$ 와 Entropy $H(p)$의 합으로 쓰여진다.           편의를 위해 (자연스럽게), $0 \log (0 / 0) = 0$ 으로, $0 \log 0 = 0$ 으로, $x &gt; 0$이면 $x \log (x / 0) = \infty$ 으로 둔다.      몇가지 성질들을 살펴보면…     $\DKL{p}{q}$ 는 일반적으로 $\DKL{q}{p}$ 와 같지 않다. (그래서   metric은 아님)        $\DKL{p}{q} \geq 0$ 이고, $p \neq q$ 이면 $\DKL{p}{q} &gt; 0$ (과제)      $\DKL{p}{q} = \infty$ 인 경우도 가능.   KL-Divergence를 확률론의 notation으로 쓰면, random variable $I$가 $p_i$의 확률분포를 가질 때, (\DKL{p}{q} = \expectwith{I}{\log\left(\frac{p_i}{q_i}\right)}) 이렇게 expectation으로 쓸 수도 있다.  Symmetrized version $(\DKL{p}{q} + \DKL{q}{p}) / 2$ 같은 것을 생각하면? $\Rightarrow$ Jensen-Shannon Divergence라고 부르는데, 그래도 여전히 infinity라는 문제가 남아서 메트릭이 되지는 않는다.">







  <meta property="article:published_time" content="2021-09-24T00:00:00+09:00">





  

  


<link rel="canonical" href="http://localhost:4000/deep-learning-study/shallow-nn/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Wonseok Shin",
      "url": "http://localhost:4000/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Gratus907's Study Note Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

    <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [ ['$$', '$$'], ['\\[', '\\]'], ['\\(', '\\)']],
      packages: {'[+]': ['physics']}
    },
    loader: {
      load: ["input/tex", "output/chtml", '[tex]/physics']
    },
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>
<p style="display: none;">$$\newcommand{\Z}{\mathbb{Z}} \newcommand{\R}{\mathbb{R}} \newcommand{\Q}{\mathbb{Q}} \newcommand{\N}{\mathbb{N}}\newcommand{\C}{\mathbb{C}}  \newcommand{\oiv}[1]{\left] #1 \right[} \newcommand{\civ}[1]{\left[ #1 \right]} \newcommand{\ad}[1]{\text{ad}(#1)} \newcommand{\acc}[1]{\text{acc}(#1)} \newcommand{\Setcond}[2]{ \left\{\, #1 \mid #2 \, \right\}} \newcommand{\Set}[1]{ \left\{ #1 \right\}} \newcommand{\abs}[1]{ \left\lvert #1 \right\rvert}\newcommand{\prt}{\mathcal{P}}\newcommand{\st}{\text{ such that }}\newcommand{\for}{\text{ for }} \newcommand{\cl}[1]{\text{cl}(#1)}\newcommand{\oiv}[1]{\left] #1 \right[}\newcommand{\interior}[1]{\text{int}(#1)}\newcommand{\di}{\mathrel{|}}\newcommand{\limply}{\rightarrow}\newcommand{\fa}{\forall}\newcommand{\e}{\exists}\newcommand{\lxor}{\oplus}\newcommand{\liff}{\leftrightarrow}\newcommand{\lequiv}{\Leftrightarrow}\newcommand{\E}[1]{\mathbb{E}\left[#1\right]}\newcommand{\minimize}{\mathrm{minimize}}\newcommand{\subto}{\mathrm{subject\ to}}\newcommand{\minimize}{\mathrm{minimize}}
  \newcommand{\subto}{\mathrm{subject\ to}}
  \newcommand{\DKL}[2]{D_{\mathrm{KL}}\left(#1 \di\di #2\right)}
  \newcommand{\E}{\mathbb{E}}
  \newcommand{\expect}[1]{\E\left[#1\right]}
  \newcommand{\expectwith}[2]{\E_{#1}\left[#2\right]}
  \renewcommand{\P}{\mathbb{P}}
  \newcommand{\uniform}[2]{\mathrm{Uniform}\left(#1 \dots #2\right)}
  \newcommand{\sgn}{\text{sign}}
  \newcommand{\argmin}{\mathrm{argmin}}
  \newcommand{\argmax}{\mathrm{argmax}}
  \newcommand{\x}{\times}$$</p>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-9M2LK7DWFS"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-9M2LK7DWFS');
    </script>
  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Gratus907's Study Note
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/postings/">Posts</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/">Categories</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tags</a>
            </li><li class="masthead__menu-item">
              <a href="/about-me/">About me</a>
            </li><li class="masthead__menu-item">
              <a href="/about-blog/">About Blog</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">토글 메뉴</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Wonseok Shin</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>2018- SNU CSE, interested in algorithms, mathematics</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">팔로우</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Seoul, Korea</span>
        </li>
      

      
        
          
        
          
        
          
            <li><a href="https://github.com/gratus907" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
      

      

      
        <li>
          <a href="mailto:gratus907@snu.ac.kr">
            <meta itemprop="email" content="gratus907@snu.ac.kr" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">이메일</span>
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
    
      
      
      
      
    
    
      

<nav class="nav__list">
  
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">토글 메뉴</label>
  <ul class="nav__items">
    
      <li>
        
          <a href="/cs-adventure/"><span class="nav__sub-title">CS 논문읽기</span></a>
        

        
      </li>
    
      <li>
        
          <span class="nav__sub-title">Problem Solving</span>
        

        
        <ul>
          
            <li><a href="/ps-weekly/">PS Weekly</a></li>
          
            <li><a href="/find-contest/">문제 출처별로 보기</a></li>
          
            <li><a href="/ps-teatime/">PS Teatime</a></li>
          
            <li><a href="/competitive-programming-rounds/">Competitive Programming</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          <span class="nav__sub-title">주제별 노트정리</span>
        

        
        <ul>
          
            <li><a href="/ds-alg-note/">자료구조/알고리즘</a></li>
          
            <li><a href="/ds-alg-advanced/">고급 자료구조/알고리즘</a></li>
          
            <li><a href="/deep-learning-study/">Deep Learning Notes</a></li>
          
        </ul>
        
      </li>
    
  </ul>
</nav>

    
    
    <nav class="toc">
      <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On This Page</h4></header>
      <ul class="toc__menu">
  <li><a href="#shallow-neural-network--introduction">Shallow Neural Network : Introduction</a></li>
  <li><a href="#kl-divergence">KL-Divergence</a></li>
</ul>

    </nav>
    
    
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="[P] Shallow Neural Networks - Introduction">
    <meta itemprop="description" content="  Contents  Shallow Neural Network : Introduction  KL-Divergence심층 신경망의 수학적 기초 3강 (9월 9일), 4강 (9월 14일) 에 기반합니다.이 문서는 $\LaTeX$를 pandoc으로 변환하여 작성하였기 때문에, 레이아웃 등이 깔끔하지 않을 수 있습니다. 언젠가 pdf 버전의 노트를 공개한다면 그쪽을 참고하면 좋을 것 같습니다.Shallow Neural Network : Introduction데이터 $X_1, \dots X_n \in \mathcal{X}$이 있고, 이에 대한 정답 라벨$Y_1, \dots Y_n \in \mathcal{Y}$이 주어진 경우를 생각해 보자. 이때, 어떤True Unknown Function $f_\star : \mathcal{X} \to \mathcal{Y}$ 가있다고 생각하면, $Y_i = f_\star(X_i)$ 를 만족한다.우리는, $X_i, Y_i$로부터, $f_\star$과 가까운 어떤 함수 $f$를 찾아내는작업을 수행하고 싶다. $X_i$들에 대해 $Y_i$는 사람이 수집한 데이터를 쓰기때문에, 이를 Supervised Learning이라고 부른다.뭔가를 시작하기 전에, 일단 $f_\star$과 가까운 $f$가 도대체 무슨 말인지를명확히 해야 한다. 뭔가를 최소화하는 문제로 만들고 싶은데… 가장 자명한방법으로 생각하면 어떤 손실함수 $\ell$을 도입해서, 이렇게 쓰고 싶다.(\underset{f \in \mathcal{F}}{\minimize}\ \sup_{x \in \mathcal{X}} \ell(f(x), f_\star(x)))이 문제는, (1) 모든 가능한 함수들의 공간 위에서 뭔가를 최적화한다는 것은알고리즘적으로 말이 안 되고, (2) 이 최적화 문제의 해는 $f_\star$이니까,사실 최적화 문제도 딱히 아니다. 모든 $x$에 대해 $f_\star$를 알고 있으면최적화를 생각할 이유가 없다.대신에, 함수들의 공간을 제약하자. 어떤 파라미터 $\theta$를 이용하여,우리는 다음과 같은 최적화 문제로 바꾸고 싶다.(\underset{\theta \in \Theta}{\minimize}\ \sup_{x \in \mathcal{X}} \ell(f_\theta(x), f_\star(x)))여전히, 일단 우리는 모든 $x$에 대해 $f_\star$를 알고 있지 않다. 우리가알고 있는 $x_1, x_2, \dots$ 에 대한 답 $y_1, y_2 \dots$ 들을 맞춰낼 수있는 함수를 일단 만드는 정도가 최선이 아닐까? 그리고, 최악의 경우를최소화하는 대신, 평균을 최적화하는게 뭔가 ‘일반적으로’ 좋은 솔루션을제공할 것 같다. supremum을 최소화한다는 것은 너무 지나친 목표이다.(\underset{\theta \in \Theta}{\minimize}\ \frac{1}{N}\sum_{i = 1}^{N} \ell(f_\theta(x_i), f_\star(x_i)))우리는 $f_\star(x_i) = y_i$ 임을 알고 있으므로, 이제 뭔가가 가능하다.이제, $\theta$를 이용하여 표현되는 $f_\theta$를 model 또는 neuralnetwork라고 부를 것이다. 또한, 이 최적화 문제를 푸는 작업을training 이라고 부를 것이다. 즉, 파라미터를 이용해서 표현한 모델$f_\theta$를 SGD와 같은 알고리즘을 이용하여 training한다는 표현이 된다.현재 거의 모든 방법들이 SGD에 기반하고 있다.Example : Least square regression$\mathcal{X} = \R^p, \mathcal{Y} = \R, \Theta = \R^p$이고, 모델$f_\theta(x) = x^T \theta$, $L(y_1, y_2) = \frac{1}{2}(y_1 - y_2)^2$ 인문제를 Least square라고 부른다. 즉, 주어진 데이터들을 비슷하게 맞춰내는Linear한 함수를 찍는 것.KL-DivergenceAs a mathematical tool, 어떤 $p, q \in \R^n$이 probability mass vector일때, 즉 $p_i, q_i \geq 0$ 이고 $\sum p_i = \sum q_i = 1$일 때, 우리는 두distribution의 차이를 생각하고 싶다.Kullback-Leibler Divergence (KL-Divergence)를 다음과 같이 정의한다.(\DKL{p}{q} = \sum_{i = 1}^{n} p_i \log\frac{p_i}{q_i} = -\sum_{i = 1}^{n} p_i \log q_i + \sum_{i = 1}^{n} p_i \log p_i)      이는 다시, 정보이론의 용어로는 Cross entropy $H(p, q)$ 와 Entropy$H(p)$의 합으로 쓰여진다.        편의를 위해 (자연스럽게), $0 \log (0 / 0) = 0$ 으로, $0 \log 0 = 0$으로, $x &gt; 0$이면 $x \log (x / 0) = \infty$ 으로 둔다.  몇가지 성질들을 살펴보면…  $\DKL{p}{q}$ 는 일반적으로 $\DKL{q}{p}$ 와 같지 않다. (그래서  metric은 아님)      $\DKL{p}{q} \geq 0$ 이고, $p \neq q$ 이면 $\DKL{p}{q} &gt; 0$ (과제)    $\DKL{p}{q} = \infty$ 인 경우도 가능.KL-Divergence를 확률론의 notation으로 쓰면, random variable $I$가$p_i$의 확률분포를 가질 때,(\DKL{p}{q} = \expectwith{I}{\log\left(\frac{p_i}{q_i}\right)}) 이렇게expectation으로 쓸 수도 있다.Symmetrized version $(\DKL{p}{q} + \DKL{q}{p}) / 2$ 같은 것을 생각하면?$\Rightarrow$ Jensen-Shannon Divergence라고 부르는데, 그래도 여전히infinity라는 문제가 남아서 메트릭이 되지는 않는다.">
    <meta itemprop="datePublished" content="2021-09-24T00:00:00+09:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">[P] Shallow Neural Networks - Introduction
</h1>
          

  <p class="page__meta">
    

    
    

    
      
      
      
      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
          530 words
      </span>
    
  </p>


        </header>
      
      <img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgratus907.github.io/deep-learning-study/shallow-nn/&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false"/></a> </div> <br/>
      <div style="display:none;"> <span id="busuanzi_container_site_pv" style="display:none;"><span id="busuanzi_value_site_pv" style="display:none;"></span></span></div>
      <section class="page__content" itemprop="text"> 
        <div id="toc">
  <p>Contents</p>
</div>
<ul id="markdown-toc">
  <li><a href="#shallow-neural-network--introduction" id="markdown-toc-shallow-neural-network--introduction">Shallow Neural Network : Introduction</a></li>
  <li><a href="#kl-divergence" id="markdown-toc-kl-divergence">KL-Divergence</a></li>
</ul>
<hr />

<p><strong>심층 신경망의 수학적 기초</strong> 3강 (9월 9일), 4강 (9월 14일) 에 기반합니다.</p>

<p>이 문서는 $\LaTeX$를 pandoc으로 변환하여 작성하였기 때문에, 레이아웃 등이 깔끔하지 않을 수 있습니다. 언젠가 pdf 버전의 노트를 공개한다면 그쪽을 참고하면 좋을 것 같습니다.</p>

<h2 id="shallow-neural-network--introduction">Shallow Neural Network : Introduction</h2>

<p>데이터 $X_1, \dots X_n \in \mathcal{X}$이 있고, 이에 대한 정답 라벨
$Y_1, \dots Y_n \in \mathcal{Y}$이 주어진 경우를 생각해 보자. 이때, 어떤
<strong>True Unknown Function</strong> $f_\star : \mathcal{X} \to \mathcal{Y}$ 가
있다고 생각하면, $Y_i = f_\star(X_i)$ 를 만족한다.</p>

<p>우리는, $X_i, Y_i$로부터, $f_\star$과 가까운 어떤 함수 $f$를 찾아내는
작업을 수행하고 싶다. $X_i$들에 대해 $Y_i$는 사람이 수집한 데이터를 쓰기
때문에, 이를 <strong>Supervised Learning</strong>이라고 부른다.</p>

<p>뭔가를 시작하기 전에, 일단 $f_\star$과 가까운 $f$가 도대체 무슨 말인지를
명확히 해야 한다. 뭔가를 최소화하는 문제로 만들고 싶은데... 가장 자명한
방법으로 생각하면 어떤 손실함수 $\ell$을 도입해서, 이렇게 쓰고 싶다.
\(\underset{f \in \mathcal{F}}{\minimize}\ \sup_{x \in \mathcal{X}} \ell(f(x), f_\star(x))\)
이 문제는, (1) 모든 가능한 함수들의 공간 위에서 뭔가를 최적화한다는 것은
알고리즘적으로 말이 안 되고, (2) 이 최적화 문제의 해는 $f_\star$이니까,
사실 최적화 문제도 딱히 아니다. 모든 $x$에 대해 $f_\star$를 알고 있으면
최적화를 생각할 이유가 없다.</p>

<p>대신에, 함수들의 공간을 제약하자. 어떤 파라미터 $\theta$를 이용하여,
우리는 다음과 같은 최적화 문제로 바꾸고 싶다.
\(\underset{\theta \in \Theta}{\minimize}\ \sup_{x \in \mathcal{X}} \ell(f_\theta(x), f_\star(x))\)</p>

<p>여전히, 일단 우리는 모든 $x$에 대해 $f_\star$를 알고 있지 않다. 우리가
알고 있는 $x_1, x_2, \dots$ 에 대한 답 $y_1, y_2 \dots$ 들을 맞춰낼 수
있는 함수를 일단 만드는 정도가 최선이 아닐까? 그리고, 최악의 경우를
최소화하는 대신, 평균을 최적화하는게 뭔가 ‘일반적으로’ 좋은 솔루션을
제공할 것 같다. supremum을 최소화한다는 것은 너무 지나친 목표이다.
\(\underset{\theta \in \Theta}{\minimize}\ \frac{1}{N}\sum_{i = 1}^{N} \ell(f_\theta(x_i), f_\star(x_i))\)
우리는 $f_\star(x_i) = y_i$ 임을 알고 있으므로, 이제 뭔가가 가능하다.</p>

<p>이제, $\theta$를 이용하여 표현되는 $f_\theta$를 <strong>model</strong> 또는 <strong>neural
network</strong>라고 부를 것이다. 또한, 이 최적화 문제를 푸는 작업을
<strong>training</strong> 이라고 부를 것이다. 즉, 파라미터를 이용해서 표현한 모델
$f_\theta$를 SGD와 같은 알고리즘을 이용하여 training한다는 표현이 된다.
현재 거의 모든 방법들이 SGD에 기반하고 있다.</p>

<p><strong>Example : Least square regression</strong></p>

<p>$\mathcal{X} = \R^p, \mathcal{Y} = \R, \Theta = \R^p$이고, 모델
$f_\theta(x) = x^T \theta$, $L(y_1, y_2) = \frac{1}{2}(y_1 - y_2)^2$ 인
문제를 Least square라고 부른다. 즉, 주어진 데이터들을 비슷하게 맞춰내는
Linear한 함수를 찍는 것.</p>

<h2 id="kl-divergence">KL-Divergence</h2>

<p>As a mathematical tool, 어떤 $p, q \in \R^n$이 probability mass vector일
때, 즉 $p_i, q_i \geq 0$ 이고 $\sum p_i = \sum q_i = 1$일 때, 우리는 두
distribution의 차이를 생각하고 싶다.</p>

<p>Kullback-Leibler Divergence (KL-Divergence)를 다음과 같이 정의한다.
\(\DKL{p}{q} = \sum_{i = 1}^{n} p_i \log\frac{p_i}{q_i} = -\sum_{i = 1}^{n} p_i \log q_i + \sum_{i = 1}^{n} p_i \log p_i\)</p>

<ul>
  <li>
    <p>이는 다시, 정보이론의 용어로는 Cross entropy $H(p, q)$ 와 Entropy
$H(p)$의 합으로 쓰여진다.</p>
  </li>
  <li>
    <p>편의를 위해 (자연스럽게), $0 \log (0 / 0) = 0$ 으로, $0 \log 0 = 0$
으로, $x &gt; 0$이면 $x \log (x / 0) = \infty$ 으로 둔다.</p>
  </li>
</ul>

<p>몇가지 성질들을 살펴보면...</p>

<ul>
  <li>$\DKL{p}{q}$ 는 일반적으로 $\DKL{q}{p}$ 와 같지 않다. (그래서
  metric은 아님)</li>
  <li>
    <p>$\DKL{p}{q} \geq 0$ 이고, $p \neq q$ 이면 $\DKL{p}{q} &gt; 0$ (과제)</p>
  </li>
  <li>$\DKL{p}{q} = \infty$ 인 경우도 가능.</li>
</ul>

<p>KL-Divergence를 확률론의 notation으로 쓰면, random variable $I$가
$p_i$의 확률분포를 가질 때,
\(\DKL{p}{q} = \expectwith{I}{\log\left(\frac{p_i}{q_i}\right)}\) 이렇게
expectation으로 쓸 수도 있다.</p>

<p>Symmetrized version $(\DKL{p}{q} + \DKL{q}{p}) / 2$ 같은 것을 생각하면?<br />
$\Rightarrow$ Jensen-Shannon Divergence라고 부르는데, 그래도 여전히
infinity라는 문제가 남아서 메트릭이 되지는 않는다.</p>

        
      </section>

      <footer class="page__meta">
        
        


  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> 카테고리: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/categories/#deep-learning-study" class="page__taxonomy-item" rel="tag">deep-learning-study</a>
    
    </span>
  </p>


        
  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> 업데이트:</strong> <time datetime="2021-09-24T00:00:00+09:00">September 24, 2021</time></p>


      </footer>
      
  <nav class="pagination">
    
      <a href="/deep-learning-study/sgd/" class="pagination--pager" title="[P] Stochastic Gradient Descent
">이전</a>
    
    
      <a href="/deep-learning-study/svm-and-lr/" class="pagination--pager" title="[P] Binary Classification : Support Vector Machine / Logistic Regression
">다음</a>
    
  </nav>

      
      <div id="disqus_thread"></div>
      <script>
        /**
        *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
        *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */

        var disqus_config = function () {
            this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
            this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
        };

        (function() { // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        s.src = 'https://gratus907-github-io.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
        })();
      </script>
      <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    
    
    

    
      <div class="page__related">
        <h4 class="page__related-title">참고</h4>
        <div class="grid__wrapper">
          
            



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deep-learning-study/softmax-regression/" rel="permalink">[P] Softmax Regression
</a>
      
    </h2>
    

  <p class="page__meta">
    

    
    

    
      
      
      
      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
          493 words
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
  Contents


  Softmax Regression



심층 신경망의 수학적 기초 6강 (9월 23일) 에 기반합니다.

이 글은 SVM과 Logistic Regression 링크 에 이어지는 내용입니다.



데이터 $X_1, \dots X_n \in \mathcal...</p>
  </article>
</div>

          
            



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deep-learning-study/multilayer-perceptron/" rel="permalink">[P] Multi Layer Perceptron
</a>
      
    </h2>
    

  <p class="page__meta">
    

    
    

    
      
      
      
      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
          472 words
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
  Contents


  Linear Layer
  Multi Layer Perceptron
  Weight Initialization
  Gradient Computation : Back propagation



심층 신경망의 수학적 기초 6강 (9월 23일) 에 기반합니다...</p>
  </article>
</div>

          
            



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deep-learning-study/mnist-mlp/" rel="permalink">Softmax Regression / MLP로 MNIST 풀어보기
</a>
      
    </h2>
    

  <p class="page__meta">
    

    
    

    
      
      
      
      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
          747 words
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
  Contents


  Problem / Dataset
  Softmax Regression
  Multi-Layer Perceptron



심층 신경망의 수학적 기초 5강, 6강 (9월 16일, 23일) 에 기반합니다. 이번 내용은 대부분이 코드에 대한 내용이라서, $\L...</p>
  </article>
</div>

          
            



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/cp-rounds/team-practice-gcpc-2020/" rel="permalink">GCPC 2020 팀연습
</a>
      
    </h2>
    

  <p class="page__meta">
    

    
    

    
      
      
      
      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
          1339 words
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
  Contents


  F : Flip Flow (10분 AC)
  A : Adolescent Architecture / K : Knightly Knowledge (45 / 85분 AC, 각 1WA)
  C : Confined Catching (67분 AC)
  I : Imp...</p>
  </article>
</div>

          
        </div>
      </div>
    
    
  </div>
</article>
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>팔로우:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> 피드</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2021 Wonseok Shin. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>. Version 801650d5908d23d33fabf8bf453fb26a7c58dabb</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>





  <script>
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'G-TNVQ3G5D5B']);
  
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>







    <script>
    
    var elements = document.querySelectorAll('p');
    Array.prototype.forEach.call(elements, function(el, i){
        if(el.innerHTML=='[expand]') {
            var parentcontent = el.parentNode.innerHTML.replace('<p>[expand]</p>','<div class="expand" style="display: none; height: 0; overflow: hidden;">').replace('<p>[/expand]</p>','</div>');
            el.parentNode.innerHTML = parentcontent;
        }
    });

    var elements = document.querySelectorAll('div.expand');
    Array.prototype.forEach.call(elements, function(el, i){
        el.previousElementSibling.innerHTML = el.previousElementSibling.innerHTML + '<span>..&nbsp; <a href="#" style="cursor: pointer;" onclick="this.parentNode.parentNode.nextElementSibling.style.display = \'block\'; this.parentNode.parentNode.nextElementSibling.style.height = \'auto\'; this.parentNode.style.display = \'none\';">read&nbsp;more&nbsp;&rarr;</a></span>';
    });

</script>

  </body>
</html>
