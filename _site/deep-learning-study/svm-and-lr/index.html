<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.21.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="ko" class="no-js">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-TNVQ3G5D5B"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-TNVQ3G5D5B');
</script>

    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>[P] Binary Classification : Support Vector Machine / Logistic Regression - Gratus907’s Study Note</title>
<meta name="description" content="Contents     Binary Classification   Linear Classification   Support Vector Machine   Logistic Regression    심층 신경망의 수학적 기초 3강 (9월 9일), 4강 (9월 14일) 에 기반합니다.  이 문서는 $\LaTeX$를 pandoc으로 변환하여 작성하였기 때문에, 레이아웃 등이 깔끔하지 않을 수 있습니다. 언젠가 pdf 버전의 노트를 공개한다면 그쪽을 참고하면 좋을 것 같습니다.    Binary Classification  잠시 앞서의 정의를 돌아보자.  데이터 $X_1, \dots X_n \in \mathcal{X}$이 있고, 이에 대한 정답 라벨 $Y_1, \dots Y_n \in \mathcal{Y}$이 주어진 경우를 생각해 보자. 이때, 어떤 True Unknown Function $f_\star : \mathcal{X} \to \mathcal{Y}$ 가 있다고 생각하면, $Y_i = f_\star(X_i)$ 를 만족한다.  우리는, $X_i, Y_i$로부터, $f_\star$과 가까운 어떤 함수 $f$를 찾아내는 작업을 수행하고 싶다. $X_i$들에 대해 $Y_i$는 사람이 수집한 데이터를 쓰기 때문에, 이를 Supervised Learning이라고 부른다.  Supervised Learning을 위해, 우리는 다음과 같은 최적화 문제를 생각할 것이다. (\underset{\theta \in \Theta}{\minimize}\ \frac{1}{N}\sum_{i = 1}^{N} \ell(f_\theta(x_i), f_\star(x_i)))  특히, 이번에는 $\mathcal{X} = \R^p$, $\mathcal{Y} = \Set{-1, +1}$ 인 문제를 생각하자. 즉, 데이터를 두 클래스로 분리해내는 것이다. 이때, 특별히 이 데이터가 linearly seperable한지를 생각한다. 어떤 초평면 $a^T x + b$ 가 존재하여, $y$값을 $a^T x + b$의 부호에 따라 찍어낼 수 있으면 linearly seperable하다고 정의한다.  Linear Classification  Binary classifcation, 특히 linear classifcation 문제를 해결하기 위해 다음과 같은 affine model을 생각한다. (f_{a, b}(x) = \sgn(a^T x + b)) 여기에 loss function으로, 틀린 라벨의 개수를 세는 것이 매우 자연스럽다. 이렇게 컴팩트하게 쓸 수 있다. (\ell(y_1, y_2) = \frac{1}{2}\abs{1 - y_1 y_2})  이제, 다음의 최적화 문제를 풀고 싶다. (\underset{a \in \R^p, b \in \R}{\minimize}\ \frac{1}{N}\sum_{i = 1}^{N} \ell(f_{a, b}(x_i), y_i)) 그러면 Linearly seperable한지는 이 최적화 문제의 최적해가 0인지와 동치이다. 그런데, 이 함수는 연속함수가 아니기 때문에 (정확히는 대충 미분가능하다는 조건을 요구한다) SGD같은 알고리즘을 돌릴수가 없다.  Support Vector Machine  따라서, 이 문제를 continuous하게 relaxation하고자 한다. 관점을 바꾸면, 이 라벨이 1일 / -1일 ‘Confidence’를 반환하도록 모델을 좀 잘 확장하고자 한다. 0.5이면 ‘아마도 1일 것으로 보인다’ 같은 느낌으로.  이를 위해서는 $y_i f_{a, b}(x_i) &gt; 0$ 을 만족해야 한다.  그런데, 실제로는 이렇게 하면 $f$값이 0 근처에서만 왔다갔다하는 문제가 있고, 이는 numerical한 면에서나 neural network의 confidence라는 해석으로나 적절하지 않으므로 적당히 margin을 주는 것이 바람직하다.  적당히 margin을 1만큼 줘서, $y_i f_{a, b}(x_i) \geq 1$ 을 만족하면 좋을 것 같다. 여기서 ‘좋을 것 같다’ 는 말은 반대로 저 성질을 만족하지 않으면 페널티를 부과하겠다는 발상으로도 해석될 수 있고… 이 페널티 함수를 최소화하는 문제로 쓰면, (\underset{a \in \R^p, b \in \R}{\minimize}\ \frac{1}{N}\sum_{i = 1}^{N} \max(0, 1 - y_i f_{a, b}(x_i)) = \frac{1}{N}\sum_{i = 1}^{N} \max(0, 1 - y_i (a^T x_i + b)))  데이터가 linearly seperable하면, 이 식도 optimal value가 0임을 알 수 있다. 이 방법을 Support Vector Machine 이라고 부르며, 흔히 regularizer를 추가한 아래 식으로 쓴다.(\underset{a \in \R^p, b \in \R}{\minimize}\ \frac{1}{N}\sum_{i = 1}^{N} \max(0, 1 - y_i (a^T x_i + b)) + \frac{\lambda}{2}\norm{a}^2)  이 최적화 문제 (Relaxation 넣기 전!)가 원본 문제의 relaxation이라는 사실을 보이는 것은 어렵지 않다. 원래 문제의 최적해를 $p_1^\star$ 라 하고, SVM의 최적해를 $p_2^\star$ 라 하면, $p_1^\star = 0 \iff p_2^\star = 0$ 임을 알 수 있다.  결국, relaxed supervised learning은 point prediction을 relaxation 해서 label value 대신 그 label의 probability를 예측하는 방향으로 생각하는 것. Single prediction보다 훨씬 realistic한 세팅으로 생각할 수 있다.  Logistic Regression  Linear binary classification에 대한 또다른 방법. 여전히 Decision boundary $a^T x + b$ 를 알고자 한다. 먼저…  Binary classification에서, 우리가 확인한 데이터의 Label을 확률벡터로 만들어서 (만약 완전히 label이 하나라면, (1, 0) 과 (0, 1) 처럼) 표현한 것을 empirical distribution $\mathcal{P}(y)$ 라고 정의하기로 한다.  다음과 같은 모델을 이용하여 최적화하는 supervised learning을 Logistic Regression이라 한다. (f_{a, b}(x) = \begin{bmatrix}     \frac{1}{1 + e^{a^T x + b}}      \frac{1}{1 + e^{-(a^Tx + b)}} \end{bmatrix})  이 모델을 이용하여, 다음과 같은 최적화 문제를 해결하고자 한다. (\underset{a \in \R^p, b \in \R}{\minimize}\ \sum_{i = 1}^{N} \DKL{\mathcal{P}(Y_i)}{f_{a, b}(X_i)}) 즉, 우리는 empirical distribution과의 KL-Divergence를 최소화하고 싶다. 이 식을 정리하면… (\underset{a \in \R^p, b \in \R}{\minimize}\ \sum_{i = 1}^{N} H(\mathcal{P}(Y_i), f_{a, b}(X_i)) + \text{ Terms independent of } a, b) 정확히 Cross entropy $H$를 전개하고, 오른쪽 term들을 다 버리면… (\underset{a \in \R^p, b \in \R}{\minimize}\ - \frac{1}{N}\sum_{i = 1}^{N} \P(y_i = -1) \log\left(\frac{1}{1 + e^{a^Tx_i + b}}\right) + \P(y_i = 1)\log\left(\frac{1}{1 + e^{-a^Tx_i - b}}\right)) 이는 다시, $\P(y_i = 1)$ 과 $\P(y_i = -1)$ 이 one-hot이므로, 둘중에 어느쪽이 1인지를 깔끔하게 정리하여, (\underset{a \in \R^p, b \in \R}{\minimize}\ - \frac{1}{N}\sum_{i = 1}^{N} \log\left(\frac{1}{1 + e^{-y_i(a^Tx_i + b)}}\right)) 단조감소함수인 Loss function $\ell(z) = \log(1 + e^{-z})$를 도입하여 부호를 떼고 깔끔하게 정리할 수 있다. (\underset{a \in \R^p, b \in \R}{\minimize}\ \frac{1}{N}\sum_{i = 1}^{N}\ell(y_i(a^T x_i + b))) 이 문제를 해결한 후, $a^T x + b$ 의 부호에 따라 prediction한다.  SVM과 비교하면, 출발점이 달랐지만 결국은 같은 문제가 되는데, $\ell(z)$ 를 어떻게 정의하느냐의 문제가 된다. SVM은 $\max(0, 1-z)$이고, Logistic regression은 $\log(1 + e^{-z})$ 를 쓰는 경우로 생각할 수 있다. 좌표에 그려보면 두 함수가 사실 굉장히 비슷하게 생겼다.  SVM과 LR은 둘다 (Decision boundary가 hyperplane이라는 관점에서) Linear classifier이지만, LR이 좀더 자연스럽게 multiclass classification으로 확장된다. (Softmax Regression)">


  <meta name="author" content="Wonseok Shin">
  
  <meta property="article:author" content="Wonseok Shin">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="ko_KR">
<meta property="og:site_name" content="Gratus907's Study Note">
<meta property="og:title" content="[P] Binary Classification : Support Vector Machine / Logistic Regression">
<meta property="og:url" content="http://localhost:4000/deep-learning-study/svm-and-lr/">


  <meta property="og:description" content="Contents     Binary Classification   Linear Classification   Support Vector Machine   Logistic Regression    심층 신경망의 수학적 기초 3강 (9월 9일), 4강 (9월 14일) 에 기반합니다.  이 문서는 $\LaTeX$를 pandoc으로 변환하여 작성하였기 때문에, 레이아웃 등이 깔끔하지 않을 수 있습니다. 언젠가 pdf 버전의 노트를 공개한다면 그쪽을 참고하면 좋을 것 같습니다.    Binary Classification  잠시 앞서의 정의를 돌아보자.  데이터 $X_1, \dots X_n \in \mathcal{X}$이 있고, 이에 대한 정답 라벨 $Y_1, \dots Y_n \in \mathcal{Y}$이 주어진 경우를 생각해 보자. 이때, 어떤 True Unknown Function $f_\star : \mathcal{X} \to \mathcal{Y}$ 가 있다고 생각하면, $Y_i = f_\star(X_i)$ 를 만족한다.  우리는, $X_i, Y_i$로부터, $f_\star$과 가까운 어떤 함수 $f$를 찾아내는 작업을 수행하고 싶다. $X_i$들에 대해 $Y_i$는 사람이 수집한 데이터를 쓰기 때문에, 이를 Supervised Learning이라고 부른다.  Supervised Learning을 위해, 우리는 다음과 같은 최적화 문제를 생각할 것이다. (\underset{\theta \in \Theta}{\minimize}\ \frac{1}{N}\sum_{i = 1}^{N} \ell(f_\theta(x_i), f_\star(x_i)))  특히, 이번에는 $\mathcal{X} = \R^p$, $\mathcal{Y} = \Set{-1, +1}$ 인 문제를 생각하자. 즉, 데이터를 두 클래스로 분리해내는 것이다. 이때, 특별히 이 데이터가 linearly seperable한지를 생각한다. 어떤 초평면 $a^T x + b$ 가 존재하여, $y$값을 $a^T x + b$의 부호에 따라 찍어낼 수 있으면 linearly seperable하다고 정의한다.  Linear Classification  Binary classifcation, 특히 linear classifcation 문제를 해결하기 위해 다음과 같은 affine model을 생각한다. (f_{a, b}(x) = \sgn(a^T x + b)) 여기에 loss function으로, 틀린 라벨의 개수를 세는 것이 매우 자연스럽다. 이렇게 컴팩트하게 쓸 수 있다. (\ell(y_1, y_2) = \frac{1}{2}\abs{1 - y_1 y_2})  이제, 다음의 최적화 문제를 풀고 싶다. (\underset{a \in \R^p, b \in \R}{\minimize}\ \frac{1}{N}\sum_{i = 1}^{N} \ell(f_{a, b}(x_i), y_i)) 그러면 Linearly seperable한지는 이 최적화 문제의 최적해가 0인지와 동치이다. 그런데, 이 함수는 연속함수가 아니기 때문에 (정확히는 대충 미분가능하다는 조건을 요구한다) SGD같은 알고리즘을 돌릴수가 없다.  Support Vector Machine  따라서, 이 문제를 continuous하게 relaxation하고자 한다. 관점을 바꾸면, 이 라벨이 1일 / -1일 ‘Confidence’를 반환하도록 모델을 좀 잘 확장하고자 한다. 0.5이면 ‘아마도 1일 것으로 보인다’ 같은 느낌으로.  이를 위해서는 $y_i f_{a, b}(x_i) &gt; 0$ 을 만족해야 한다.  그런데, 실제로는 이렇게 하면 $f$값이 0 근처에서만 왔다갔다하는 문제가 있고, 이는 numerical한 면에서나 neural network의 confidence라는 해석으로나 적절하지 않으므로 적당히 margin을 주는 것이 바람직하다.  적당히 margin을 1만큼 줘서, $y_i f_{a, b}(x_i) \geq 1$ 을 만족하면 좋을 것 같다. 여기서 ‘좋을 것 같다’ 는 말은 반대로 저 성질을 만족하지 않으면 페널티를 부과하겠다는 발상으로도 해석될 수 있고… 이 페널티 함수를 최소화하는 문제로 쓰면, (\underset{a \in \R^p, b \in \R}{\minimize}\ \frac{1}{N}\sum_{i = 1}^{N} \max(0, 1 - y_i f_{a, b}(x_i)) = \frac{1}{N}\sum_{i = 1}^{N} \max(0, 1 - y_i (a^T x_i + b)))  데이터가 linearly seperable하면, 이 식도 optimal value가 0임을 알 수 있다. 이 방법을 Support Vector Machine 이라고 부르며, 흔히 regularizer를 추가한 아래 식으로 쓴다.(\underset{a \in \R^p, b \in \R}{\minimize}\ \frac{1}{N}\sum_{i = 1}^{N} \max(0, 1 - y_i (a^T x_i + b)) + \frac{\lambda}{2}\norm{a}^2)  이 최적화 문제 (Relaxation 넣기 전!)가 원본 문제의 relaxation이라는 사실을 보이는 것은 어렵지 않다. 원래 문제의 최적해를 $p_1^\star$ 라 하고, SVM의 최적해를 $p_2^\star$ 라 하면, $p_1^\star = 0 \iff p_2^\star = 0$ 임을 알 수 있다.  결국, relaxed supervised learning은 point prediction을 relaxation 해서 label value 대신 그 label의 probability를 예측하는 방향으로 생각하는 것. Single prediction보다 훨씬 realistic한 세팅으로 생각할 수 있다.  Logistic Regression  Linear binary classification에 대한 또다른 방법. 여전히 Decision boundary $a^T x + b$ 를 알고자 한다. 먼저…  Binary classification에서, 우리가 확인한 데이터의 Label을 확률벡터로 만들어서 (만약 완전히 label이 하나라면, (1, 0) 과 (0, 1) 처럼) 표현한 것을 empirical distribution $\mathcal{P}(y)$ 라고 정의하기로 한다.  다음과 같은 모델을 이용하여 최적화하는 supervised learning을 Logistic Regression이라 한다. (f_{a, b}(x) = \begin{bmatrix}     \frac{1}{1 + e^{a^T x + b}}      \frac{1}{1 + e^{-(a^Tx + b)}} \end{bmatrix})  이 모델을 이용하여, 다음과 같은 최적화 문제를 해결하고자 한다. (\underset{a \in \R^p, b \in \R}{\minimize}\ \sum_{i = 1}^{N} \DKL{\mathcal{P}(Y_i)}{f_{a, b}(X_i)}) 즉, 우리는 empirical distribution과의 KL-Divergence를 최소화하고 싶다. 이 식을 정리하면… (\underset{a \in \R^p, b \in \R}{\minimize}\ \sum_{i = 1}^{N} H(\mathcal{P}(Y_i), f_{a, b}(X_i)) + \text{ Terms independent of } a, b) 정확히 Cross entropy $H$를 전개하고, 오른쪽 term들을 다 버리면… (\underset{a \in \R^p, b \in \R}{\minimize}\ - \frac{1}{N}\sum_{i = 1}^{N} \P(y_i = -1) \log\left(\frac{1}{1 + e^{a^Tx_i + b}}\right) + \P(y_i = 1)\log\left(\frac{1}{1 + e^{-a^Tx_i - b}}\right)) 이는 다시, $\P(y_i = 1)$ 과 $\P(y_i = -1)$ 이 one-hot이므로, 둘중에 어느쪽이 1인지를 깔끔하게 정리하여, (\underset{a \in \R^p, b \in \R}{\minimize}\ - \frac{1}{N}\sum_{i = 1}^{N} \log\left(\frac{1}{1 + e^{-y_i(a^Tx_i + b)}}\right)) 단조감소함수인 Loss function $\ell(z) = \log(1 + e^{-z})$를 도입하여 부호를 떼고 깔끔하게 정리할 수 있다. (\underset{a \in \R^p, b \in \R}{\minimize}\ \frac{1}{N}\sum_{i = 1}^{N}\ell(y_i(a^T x_i + b))) 이 문제를 해결한 후, $a^T x + b$ 의 부호에 따라 prediction한다.  SVM과 비교하면, 출발점이 달랐지만 결국은 같은 문제가 되는데, $\ell(z)$ 를 어떻게 정의하느냐의 문제가 된다. SVM은 $\max(0, 1-z)$이고, Logistic regression은 $\log(1 + e^{-z})$ 를 쓰는 경우로 생각할 수 있다. 좌표에 그려보면 두 함수가 사실 굉장히 비슷하게 생겼다.  SVM과 LR은 둘다 (Decision boundary가 hyperplane이라는 관점에서) Linear classifier이지만, LR이 좀더 자연스럽게 multiclass classification으로 확장된다. (Softmax Regression)">







  <meta property="article:published_time" content="2021-09-24T00:00:00+09:00">





  

  


<link rel="canonical" href="http://localhost:4000/deep-learning-study/svm-and-lr/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Wonseok Shin",
      "url": "http://localhost:4000/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Gratus907's Study Note Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

    <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [ ['$$', '$$'], ['\\[', '\\]'], ['\\(', '\\)']],
      packages: {'[+]': ['physics']}
    },
    loader: {
      load: ["input/tex", "output/chtml", '[tex]/physics']
    },
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>
<p style="display: none;">$$\newcommand{\Z}{\mathbb{Z}} \newcommand{\R}{\mathbb{R}} \newcommand{\Q}{\mathbb{Q}} \newcommand{\N}{\mathbb{N}}\newcommand{\C}{\mathbb{C}}  \newcommand{\oiv}[1]{\left] #1 \right[} \newcommand{\civ}[1]{\left[ #1 \right]} \newcommand{\ad}[1]{\text{ad}(#1)} \newcommand{\acc}[1]{\text{acc}(#1)} \newcommand{\Setcond}[2]{ \left\{\, #1 \mid #2 \, \right\}} \newcommand{\Set}[1]{ \left\{ #1 \right\}} \newcommand{\abs}[1]{ \left\lvert #1 \right\rvert}\newcommand{\prt}{\mathcal{P}}\newcommand{\st}{\text{ such that }}\newcommand{\for}{\text{ for }} \newcommand{\cl}[1]{\text{cl}(#1)}\newcommand{\oiv}[1]{\left] #1 \right[}\newcommand{\interior}[1]{\text{int}(#1)}\newcommand{\di}{\mathrel{|}}\newcommand{\limply}{\rightarrow}\newcommand{\fa}{\forall}\newcommand{\e}{\exists}\newcommand{\lxor}{\oplus}\newcommand{\liff}{\leftrightarrow}\newcommand{\lequiv}{\Leftrightarrow}\newcommand{\E}[1]{\mathbb{E}\left[#1\right]}\newcommand{\minimize}{\mathrm{minimize}}\newcommand{\subto}{\mathrm{subject\ to}}\newcommand{\minimize}{\mathrm{minimize}}
  \newcommand{\subto}{\mathrm{subject\ to}}
  \newcommand{\DKL}[2]{D_{\mathrm{KL}}\left(#1 \di\di #2\right)}
  \newcommand{\E}{\mathbb{E}}
  \newcommand{\expect}[1]{\E\left[#1\right]}
  \newcommand{\expectwith}[2]{\E_{#1}\left[#2\right]}
  \renewcommand{\P}{\mathbb{P}}
  \newcommand{\uniform}[2]{\mathrm{Uniform}\left(#1 \dots #2\right)}
  \newcommand{\sgn}{\text{sign}}
  \newcommand{\argmin}{\mathrm{argmin}}
  \newcommand{\argmax}{\mathrm{argmax}}
  \newcommand{\x}{\times}$$</p>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-9M2LK7DWFS"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-9M2LK7DWFS');
    </script>
  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Gratus907's Study Note
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/postings/">Posts</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/">Categories</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tags</a>
            </li><li class="masthead__menu-item">
              <a href="/about-me/">About me</a>
            </li><li class="masthead__menu-item">
              <a href="/about-blog/">About Blog</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">토글 메뉴</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Wonseok Shin</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>2018- SNU CSE, interested in algorithms, mathematics</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">팔로우</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Seoul, Korea</span>
        </li>
      

      
        
          
        
          
        
          
            <li><a href="https://github.com/gratus907" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
      

      

      
        <li>
          <a href="mailto:gratus907@snu.ac.kr">
            <meta itemprop="email" content="gratus907@snu.ac.kr" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">이메일</span>
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
    
      
      
      
      
    
    
      

<nav class="nav__list">
  
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">토글 메뉴</label>
  <ul class="nav__items">
    
      <li>
        
          <a href="/cs-adventure/"><span class="nav__sub-title">CS 논문읽기</span></a>
        

        
      </li>
    
      <li>
        
          <span class="nav__sub-title">Problem Solving</span>
        

        
        <ul>
          
            <li><a href="/ps-weekly/">PS Weekly</a></li>
          
            <li><a href="/find-contest/">문제 출처별로 보기</a></li>
          
            <li><a href="/ps-teatime/">PS Teatime</a></li>
          
            <li><a href="/competitive-programming-rounds/">Competitive Programming</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          <span class="nav__sub-title">주제별 노트정리</span>
        

        
        <ul>
          
            <li><a href="/ds-alg-note/">자료구조/알고리즘</a></li>
          
            <li><a href="/ds-alg-advanced/">고급 자료구조/알고리즘</a></li>
          
            <li><a href="/deep-learning-study/">Deep Learning Notes</a></li>
          
        </ul>
        
      </li>
    
  </ul>
</nav>

    
    
    <nav class="toc">
      <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On This Page</h4></header>
      <ul class="toc__menu">
  <li><a href="#binary-classification">Binary Classification</a></li>
  <li><a href="#linear-classification">Linear Classification</a></li>
  <li><a href="#support-vector-machine">Support Vector Machine</a></li>
  <li><a href="#logistic-regression">Logistic Regression</a></li>
</ul>

    </nav>
    
    
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="[P] Binary Classification : Support Vector Machine / Logistic Regression">
    <meta itemprop="description" content="  Contents  Binary Classification  Linear Classification  Support Vector Machine  Logistic Regression심층 신경망의 수학적 기초 3강 (9월 9일), 4강 (9월 14일) 에 기반합니다.이 문서는 $\LaTeX$를 pandoc으로 변환하여 작성하였기 때문에, 레이아웃 등이 깔끔하지 않을 수 있습니다. 언젠가 pdf 버전의 노트를 공개한다면 그쪽을 참고하면 좋을 것 같습니다.Binary Classification잠시 앞서의 정의를 돌아보자.데이터 $X_1, \dots X_n \in \mathcal{X}$이 있고, 이에 대한 정답 라벨$Y_1, \dots Y_n \in \mathcal{Y}$이 주어진 경우를 생각해 보자. 이때, 어떤True Unknown Function $f_\star : \mathcal{X} \to \mathcal{Y}$ 가있다고 생각하면, $Y_i = f_\star(X_i)$ 를 만족한다.우리는, $X_i, Y_i$로부터, $f_\star$과 가까운 어떤 함수 $f$를 찾아내는작업을 수행하고 싶다. $X_i$들에 대해 $Y_i$는 사람이 수집한 데이터를 쓰기때문에, 이를 Supervised Learning이라고 부른다.Supervised Learning을 위해, 우리는 다음과 같은 최적화 문제를 생각할 것이다.(\underset{\theta \in \Theta}{\minimize}\ \frac{1}{N}\sum_{i = 1}^{N} \ell(f_\theta(x_i), f_\star(x_i)))특히, 이번에는 $\mathcal{X} = \R^p$, $\mathcal{Y} = \Set{-1, +1}$ 인 문제를 생각하자.즉, 데이터를 두 클래스로 분리해내는 것이다. 이때, 특별히 이 데이터가linearly seperable한지를 생각한다. 어떤 초평면 $a^T x + b$ 가존재하여, $y$값을 $a^T x + b$의 부호에 따라 찍어낼 수 있으면 linearlyseperable하다고 정의한다.Linear ClassificationBinary classifcation, 특히 linear classifcation 문제를 해결하기 위해다음과 같은 affine model을 생각한다. (f_{a, b}(x) = \sgn(a^T x + b))여기에 loss function으로, 틀린 라벨의 개수를 세는 것이 매우 자연스럽다.이렇게 컴팩트하게 쓸 수 있다.(\ell(y_1, y_2) = \frac{1}{2}\abs{1 - y_1 y_2})이제, 다음의 최적화 문제를 풀고 싶다.(\underset{a \in \R^p, b \in \R}{\minimize}\ \frac{1}{N}\sum_{i = 1}^{N} \ell(f_{a, b}(x_i), y_i))그러면 Linearly seperable한지는 이 최적화 문제의 최적해가 0인지와동치이다. 그런데, 이 함수는 연속함수가 아니기 때문에 (정확히는 대충미분가능하다는 조건을 요구한다) SGD같은 알고리즘을 돌릴수가 없다.Support Vector Machine따라서, 이 문제를 continuous하게 relaxation하고자 한다. 관점을 바꾸면,이 라벨이 1일 / -1일 ‘Confidence’를 반환하도록 모델을 좀 잘 확장하고자한다. 0.5이면 ‘아마도 1일 것으로 보인다’ 같은 느낌으로.이를 위해서는 $y_i f_{a, b}(x_i) &gt; 0$ 을 만족해야 한다.그런데, 실제로는 이렇게 하면 $f$값이 0 근처에서만 왔다갔다하는 문제가 있고, 이는 numerical한 면에서나 neural network의 confidence라는 해석으로나 적절하지 않으므로 적당히margin을 주는 것이 바람직하다.적당히 margin을 1만큼 줘서, $y_i f_{a, b}(x_i) \geq 1$ 을 만족하면좋을 것 같다. 여기서 ‘좋을 것 같다’ 는 말은 반대로 저 성질을 만족하지않으면 페널티를 부과하겠다는 발상으로도 해석될 수 있고… 이 페널티 함수를 최소화하는 문제로 쓰면,(\underset{a \in \R^p, b \in \R}{\minimize}\ \frac{1}{N}\sum_{i = 1}^{N} \max(0, 1 - y_i f_{a, b}(x_i)) = \frac{1}{N}\sum_{i = 1}^{N} \max(0, 1 - y_i (a^T x_i + b)))데이터가 linearly seperable하면, 이 식도 optimal value가 0임을 알 수있다. 이 방법을 Support Vector Machine 이라고 부르며, 흔히regularizer를 추가한 아래 식으로쓴다.(\underset{a \in \R^p, b \in \R}{\minimize}\ \frac{1}{N}\sum_{i = 1}^{N} \max(0, 1 - y_i (a^T x_i + b)) + \frac{\lambda}{2}\norm{a}^2)이 최적화 문제 (Relaxation 넣기 전!)가 원본 문제의 relaxation이라는사실을 보이는 것은 어렵지 않다. 원래 문제의 최적해를 $p_1^\star$ 라 하고,SVM의 최적해를 $p_2^\star$ 라 하면, $p_1^\star = 0 \iff p_2^\star = 0$ 임을 알 수있다.결국, relaxed supervised learning은 point prediction을 relaxation 해서label value 대신 그 label의 probability를 예측하는 방향으로 생각하는 것.Single prediction보다 훨씬 realistic한 세팅으로 생각할 수 있다.Logistic RegressionLinear binary classification에 대한 또다른 방법. 여전히 Decisionboundary $a^T x + b$ 를 알고자 한다. 먼저…Binary classification에서, 우리가 확인한 데이터의 Label을 확률벡터로만들어서 (만약 완전히 label이 하나라면, (1, 0) 과 (0, 1) 처럼) 표현한것을 empirical distribution $\mathcal{P}(y)$ 라고 정의하기로 한다.다음과 같은 모델을 이용하여 최적화하는 supervised learning을 LogisticRegression이라 한다. (f_{a, b}(x) = \begin{bmatrix}    \frac{1}{1 + e^{a^T x + b}}     \frac{1}{1 + e^{-(a^Tx + b)}}\end{bmatrix})이 모델을 이용하여, 다음과 같은 최적화 문제를 해결하고자 한다.(\underset{a \in \R^p, b \in \R}{\minimize}\ \sum_{i = 1}^{N} \DKL{\mathcal{P}(Y_i)}{f_{a, b}(X_i)})즉, 우리는 empirical distribution과의 KL-Divergence를 최소화하고 싶다.이 식을 정리하면…(\underset{a \in \R^p, b \in \R}{\minimize}\ \sum_{i = 1}^{N} H(\mathcal{P}(Y_i), f_{a, b}(X_i)) + \text{ Terms independent of } a, b)정확히 Cross entropy $H$를 전개하고, 오른쪽 term들을 다 버리면…(\underset{a \in \R^p, b \in \R}{\minimize}\ - \frac{1}{N}\sum_{i = 1}^{N} \P(y_i = -1) \log\left(\frac{1}{1 + e^{a^Tx_i + b}}\right) + \P(y_i = 1)\log\left(\frac{1}{1 + e^{-a^Tx_i - b}}\right))이는 다시, $\P(y_i = 1)$ 과 $\P(y_i = -1)$ 이 one-hot이므로, 둘중에어느쪽이 1인지를 깔끔하게 정리하여,(\underset{a \in \R^p, b \in \R}{\minimize}\ - \frac{1}{N}\sum_{i = 1}^{N} \log\left(\frac{1}{1 + e^{-y_i(a^Tx_i + b)}}\right))단조감소함수인 Loss function $\ell(z) = \log(1 + e^{-z})$를 도입하여부호를 떼고 깔끔하게 정리할 수 있다.(\underset{a \in \R^p, b \in \R}{\minimize}\ \frac{1}{N}\sum_{i = 1}^{N}\ell(y_i(a^T x_i + b)))이 문제를 해결한 후, $a^T x + b$ 의 부호에 따라 prediction한다.SVM과 비교하면, 출발점이 달랐지만 결국은 같은 문제가 되는데, $\ell(z)$를 어떻게 정의하느냐의 문제가 된다. SVM은 $\max(0, 1-z)$이고, Logisticregression은 $\log(1 + e^{-z})$ 를 쓰는 경우로 생각할 수 있다. 좌표에그려보면 두 함수가 사실 굉장히 비슷하게 생겼다.SVM과 LR은 둘다 (Decision boundary가 hyperplane이라는 관점에서) Linearclassifier이지만, LR이 좀더 자연스럽게 multiclass classification으로확장된다. (Softmax Regression)">
    <meta itemprop="datePublished" content="2021-09-24T00:00:00+09:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">[P] Binary Classification : Support Vector Machine / Logistic Regression
</h1>
          

  <p class="page__meta">
    

    
    

    
      
      
      
      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
          775 words
      </span>
    
  </p>


        </header>
      
      <img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgratus907.github.io/deep-learning-study/svm-and-lr/&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false"/></a> </div> <br/>
      <div style="display:none;"> <span id="busuanzi_container_site_pv" style="display:none;"><span id="busuanzi_value_site_pv" style="display:none;"></span></span></div>
      <section class="page__content" itemprop="text"> 
        <div id="toc">
  <p>Contents</p>
</div>
<ul id="markdown-toc">
  <li><a href="#binary-classification" id="markdown-toc-binary-classification">Binary Classification</a></li>
  <li><a href="#linear-classification" id="markdown-toc-linear-classification">Linear Classification</a></li>
  <li><a href="#support-vector-machine" id="markdown-toc-support-vector-machine">Support Vector Machine</a></li>
  <li><a href="#logistic-regression" id="markdown-toc-logistic-regression">Logistic Regression</a></li>
</ul>
<hr />

<p><strong>심층 신경망의 수학적 기초</strong> 3강 (9월 9일), 4강 (9월 14일) 에 기반합니다.</p>

<p>이 문서는 $\LaTeX$를 pandoc으로 변환하여 작성하였기 때문에, 레이아웃 등이 깔끔하지 않을 수 있습니다. 언젠가 pdf 버전의 노트를 공개한다면 그쪽을 참고하면 좋을 것 같습니다.</p>

<hr />
<h2 id="binary-classification">Binary Classification</h2>

<p>잠시 앞서의 정의를 돌아보자.</p>

<p>데이터 $X_1, \dots X_n \in \mathcal{X}$이 있고, 이에 대한 정답 라벨
$Y_1, \dots Y_n \in \mathcal{Y}$이 주어진 경우를 생각해 보자. 이때, 어떤
<strong>True Unknown Function</strong> $f_\star : \mathcal{X} \to \mathcal{Y}$ 가
있다고 생각하면, $Y_i = f_\star(X_i)$ 를 만족한다.</p>

<p>우리는, $X_i, Y_i$로부터, $f_\star$과 가까운 어떤 함수 $f$를 찾아내는
작업을 수행하고 싶다. $X_i$들에 대해 $Y_i$는 사람이 수집한 데이터를 쓰기
때문에, 이를 <strong>Supervised Learning</strong>이라고 부른다.</p>

<p>Supervised Learning을 위해, 우리는 다음과 같은 최적화 문제를 생각할 것이다.
\(\underset{\theta \in \Theta}{\minimize}\ \frac{1}{N}\sum_{i = 1}^{N} \ell(f_\theta(x_i), f_\star(x_i))\)</p>

<p>특히, 이번에는 $\mathcal{X} = \R^p$, $\mathcal{Y} = \Set{-1, +1}$ 인 문제를 생각하자.
즉, 데이터를 두 클래스로 분리해내는 것이다. 이때, 특별히 이 데이터가
<strong>linearly seperable</strong>한지를 생각한다. 어떤 초평면 $a^T x + b$ 가
존재하여, $y$값을 $a^T x + b$의 부호에 따라 찍어낼 수 있으면 linearly
seperable하다고 정의한다.</p>

<h2 id="linear-classification">Linear Classification</h2>

<p>Binary classifcation, 특히 linear classifcation 문제를 해결하기 위해
다음과 같은 affine model을 생각한다. \(f_{a, b}(x) = \sgn(a^T x + b)\)
여기에 loss function으로, 틀린 라벨의 개수를 세는 것이 매우 자연스럽다.
이렇게 컴팩트하게 쓸 수 있다.
\(\ell(y_1, y_2) = \frac{1}{2}\abs{1 - y_1 y_2}\)</p>

<p>이제, 다음의 최적화 문제를 풀고 싶다.
\(\underset{a \in \R^p, b \in \R}{\minimize}\ \frac{1}{N}\sum_{i = 1}^{N} \ell(f_{a, b}(x_i), y_i)\)
그러면 Linearly seperable한지는 이 최적화 문제의 최적해가 0인지와
동치이다. 그런데, 이 함수는 연속함수가 아니기 때문에 (정확히는 대충
미분가능하다는 조건을 요구한다) SGD같은 알고리즘을 돌릴수가 없다.</p>

<h2 id="support-vector-machine">Support Vector Machine</h2>

<p>따라서, 이 문제를 continuous하게 relaxation하고자 한다. 관점을 바꾸면,
이 라벨이 1일 / -1일 ‘Confidence’를 반환하도록 모델을 좀 잘 확장하고자
한다. 0.5이면 ‘아마도 1일 것으로 보인다’ 같은 느낌으로.</p>

<p>이를 위해서는 $y_i f_{a, b}(x_i) &gt; 0$ 을 만족해야 한다.</p>

<p>그런데, 실제로는 이렇게 하면 $f$값이 0 근처에서만 왔다갔다하는 문제가 있고, 이는 numerical한 면에서나 neural network의 confidence라는 해석으로나 적절하지 않으므로 적당히
margin을 주는 것이 바람직하다.</p>

<p>적당히 margin을 1만큼 줘서, $y_i f_{a, b}(x_i) \geq 1$ 을 만족하면
좋을 것 같다. 여기서 ‘좋을 것 같다’ 는 말은 반대로 저 성질을 만족하지
않으면 페널티를 부과하겠다는 발상으로도 해석될 수 있고… 이 페널티 함수를 최소화하는 문제로 쓰면,
\(\underset{a \in \R^p, b \in \R}{\minimize}\ \frac{1}{N}\sum_{i = 1}^{N} \max(0, 1 - y_i f_{a, b}(x_i)) = \frac{1}{N}\sum_{i = 1}^{N} \max(0, 1 - y_i (a^T x_i + b))\)</p>

<p>데이터가 linearly seperable하면, 이 식도 optimal value가 0임을 알 수
있다. 이 방법을 <strong>Support Vector Machine</strong> 이라고 부르며, 흔히
regularizer를 추가한 아래 식으로
쓴다.\(\underset{a \in \R^p, b \in \R}{\minimize}\ \frac{1}{N}\sum_{i = 1}^{N} \max(0, 1 - y_i (a^T x_i + b)) + \frac{\lambda}{2}\norm{a}^2\)</p>

<p>이 최적화 문제 (Relaxation 넣기 전!)가 원본 문제의 relaxation이라는
사실을 보이는 것은 어렵지 않다. 원래 문제의 최적해를 $p_1^\star$ 라 하고,
SVM의 최적해를 $p_2^\star$ 라 하면, $p_1^\star = 0 \iff p_2^\star = 0$ 임을 알 수
있다.</p>

<p>결국, relaxed supervised learning은 point prediction을 relaxation 해서
label value 대신 그 label의 probability를 예측하는 방향으로 생각하는 것.
Single prediction보다 훨씬 realistic한 세팅으로 생각할 수 있다.</p>

<h2 id="logistic-regression">Logistic Regression</h2>

<p>Linear binary classification에 대한 또다른 방법. 여전히 Decision
boundary $a^T x + b$ 를 알고자 한다. 먼저...</p>

<p>Binary classification에서, 우리가 확인한 데이터의 Label을 확률벡터로
만들어서 (만약 완전히 label이 하나라면, (1, 0) 과 (0, 1) 처럼) 표현한
것을 empirical distribution $\mathcal{P}(y)$ 라고 정의하기로 한다.</p>

<p>다음과 같은 모델을 이용하여 최적화하는 supervised learning을 Logistic
Regression이라 한다. \(f_{a, b}(x) = \begin{bmatrix}
    \frac{1}{1 + e^{a^T x + b}} \\
    \frac{1}{1 + e^{-(a^Tx + b)}}
\end{bmatrix}\)</p>

<p>이 모델을 이용하여, 다음과 같은 최적화 문제를 해결하고자 한다.
\(\underset{a \in \R^p, b \in \R}{\minimize}\ \sum_{i = 1}^{N} \DKL{\mathcal{P}(Y_i)}{f_{a, b}(X_i)}\)
즉, 우리는 empirical distribution과의 KL-Divergence를 최소화하고 싶다.
이 식을 정리하면...
\(\underset{a \in \R^p, b \in \R}{\minimize}\ \sum_{i = 1}^{N} H(\mathcal{P}(Y_i), f_{a, b}(X_i)) + \text{ Terms independent of } a, b\)
정확히 Cross entropy $H$를 전개하고, 오른쪽 term들을 다 버리면...
\(\underset{a \in \R^p, b \in \R}{\minimize}\ - \frac{1}{N}\sum_{i = 1}^{N} \P(y_i = -1) \log\left(\frac{1}{1 + e^{a^Tx_i + b}}\right) + \P(y_i = 1)\log\left(\frac{1}{1 + e^{-a^Tx_i - b}}\right)\)
이는 다시, $\P(y_i = 1)$ 과 $\P(y_i = -1)$ 이 one-hot이므로, 둘중에
어느쪽이 1인지를 깔끔하게 정리하여,
\(\underset{a \in \R^p, b \in \R}{\minimize}\ - \frac{1}{N}\sum_{i = 1}^{N} \log\left(\frac{1}{1 + e^{-y_i(a^Tx_i + b)}}\right)\)
단조감소함수인 Loss function $\ell(z) = \log(1 + e^{-z})$를 도입하여
부호를 떼고 깔끔하게 정리할 수 있다.
\(\underset{a \in \R^p, b \in \R}{\minimize}\ \frac{1}{N}\sum_{i = 1}^{N}\ell(y_i(a^T x_i + b))\)
이 문제를 해결한 후, $a^T x + b$ 의 부호에 따라 prediction한다.</p>

<p>SVM과 비교하면, 출발점이 달랐지만 결국은 같은 문제가 되는데, $\ell(z)$
를 어떻게 정의하느냐의 문제가 된다. SVM은 $\max(0, 1-z)$이고, Logistic
regression은 $\log(1 + e^{-z})$ 를 쓰는 경우로 생각할 수 있다. 좌표에
그려보면 두 함수가 사실 굉장히 비슷하게 생겼다.</p>

<p>SVM과 LR은 둘다 (Decision boundary가 hyperplane이라는 관점에서) Linear
classifier이지만, LR이 좀더 자연스럽게 multiclass classification으로
확장된다. (Softmax Regression)</p>


        
      </section>

      <footer class="page__meta">
        
        


  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> 카테고리: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/categories/#deep-learning-study" class="page__taxonomy-item" rel="tag">deep-learning-study</a>
    
    </span>
  </p>


        
  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> 업데이트:</strong> <time datetime="2021-09-24T00:00:00+09:00">September 24, 2021</time></p>


      </footer>
      
  <nav class="pagination">
    
      <a href="/deep-learning-study/shallow-nn/" class="pagination--pager" title="[P] Shallow Neural Networks - Introduction
">이전</a>
    
    
      <a href="/deep-learning-study/semantic-segmentation/" class="pagination--pager" title="Semantic Segmentation : Introduction
">다음</a>
    
  </nav>

      
      <div id="disqus_thread"></div>
      <script>
        /**
        *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
        *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */

        var disqus_config = function () {
            this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
            this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
        };

        (function() { // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        s.src = 'https://gratus907-github-io.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
        })();
      </script>
      <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    
    
    

    
      <div class="page__related">
        <h4 class="page__related-title">참고</h4>
        <div class="grid__wrapper">
          
            



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deep-learning-study/softmax-regression/" rel="permalink">[P] Softmax Regression
</a>
      
    </h2>
    

  <p class="page__meta">
    

    
    

    
      
      
      
      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
          499 words
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
  Contents


  Softmax Regression



심층 신경망의 수학적 기초 6강 (9월 23일) 에 기반합니다.

이 글은 SVM과 Logistic Regression 링크 에 이어지는 내용입니다.

나중에 설명을 보강해서 다시 작성될 예정입니다.



데이터 ...</p>
  </article>
</div>

          
            



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deep-learning-study/multilayer-perceptron/" rel="permalink">[P] Multi Layer Perceptron
</a>
      
    </h2>
    

  <p class="page__meta">
    

    
    

    
      
      
      
      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
          472 words
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
  Contents


  Linear Layer
  Multi Layer Perceptron
  Weight Initialization
  Gradient Computation : Back propagation



심층 신경망의 수학적 기초 6강 (9월 23일) 에 기반합니다...</p>
  </article>
</div>

          
            



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/cp-rounds/icpc-2021-prelim/" rel="permalink">ICPC Korea First Round 2021 후기 / 풀이
</a>
      
    </h2>
    

  <p class="page__meta">
    

    
    

    
      
      
      
      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
          3102 words
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
  Contents


  Preperation / Our Team
  본 대회    
      Problem I : Sport Climbing Combined
      Problem E : Histogram
      Problem J : Ten
      Problem H...</p>
  </article>
</div>

          
            



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deep-learning-study/mnist-mlp/" rel="permalink">Softmax Regression / MLP로 MNIST 풀어보기
</a>
      
    </h2>
    

  <p class="page__meta">
    

    
    

    
      
      
      
      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
          747 words
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
  Contents


  Problem / Dataset
  Softmax Regression
  Multi-Layer Perceptron



심층 신경망의 수학적 기초 5강, 6강 (9월 16일, 23일) 에 기반합니다. 이번 내용은 대부분이 코드에 대한 내용이라서, $\L...</p>
  </article>
</div>

          
        </div>
      </div>
    
    
  </div>
</article>
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>팔로우:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> 피드</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2021 Wonseok Shin. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>. Version 06051a3b3b961ab25d3a3695579dd7b075a69858</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>





  <script>
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'G-TNVQ3G5D5B']);
  
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>







    <script>
    
    var elements = document.querySelectorAll('p');
    Array.prototype.forEach.call(elements, function(el, i){
        if(el.innerHTML=='[expand]') {
            var parentcontent = el.parentNode.innerHTML.replace('<p>[expand]</p>','<div class="expand" style="display: none; height: 0; overflow: hidden;">').replace('<p>[/expand]</p>','</div>');
            el.parentNode.innerHTML = parentcontent;
        }
    });

    var elements = document.querySelectorAll('div.expand');
    Array.prototype.forEach.call(elements, function(el, i){
        el.previousElementSibling.innerHTML = el.previousElementSibling.innerHTML + '<span>..&nbsp; <a href="#" style="cursor: pointer;" onclick="this.parentNode.parentNode.nextElementSibling.style.display = \'block\'; this.parentNode.parentNode.nextElementSibling.style.height = \'auto\'; this.parentNode.style.display = \'none\';">read&nbsp;more&nbsp;&rarr;</a></span>';
    });

</script>

  </body>
</html>
