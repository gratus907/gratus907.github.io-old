<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-08-13T18:01:50+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Gratus907’s Study Note</title><subtitle>Hello World!</subtitle><author><name>Wonseok Shin</name><email>gratus907@snu.ac.kr</email></author><entry><title type="html">논문읽기 : Chan-Vese Algorithm</title><link href="http://localhost:4000/cs-adventure/chan-vese/" rel="alternate" type="text/html" title="논문읽기 : Chan-Vese Algorithm" /><published>2021-08-13T00:00:00+09:00</published><updated>2021-08-13T00:00:00+09:00</updated><id>http://localhost:4000/cs-adventure/chan-vese</id><content type="html" xml:base="http://localhost:4000/cs-adventure/chan-vese/">&lt;div id=&quot;toc&quot;&gt;
  &lt;p&gt;Contents&lt;/p&gt;
&lt;/div&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot; id=&quot;markdown-toc-introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#key-ideas&quot; id=&quot;markdown-toc-key-ideas&quot;&gt;Key Ideas&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#energy-functional&quot; id=&quot;markdown-toc-energy-functional&quot;&gt;Energy functional&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#integral-formulation&quot; id=&quot;markdown-toc-integral-formulation&quot;&gt;Integral formulation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#relaxation--euler-lagrange&quot; id=&quot;markdown-toc-relaxation--euler-lagrange&quot;&gt;Relaxation &amp;amp; Euler-Lagrange&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#partial-differential-equation&quot; id=&quot;markdown-toc-partial-differential-equation&quot;&gt;Partial Differential Equation&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion&quot; id=&quot;markdown-toc-conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#thoughts&quot; id=&quot;markdown-toc-thoughts&quot;&gt;Thoughts&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;이번에 읽은 논문은 &lt;strong&gt;Active Contours Without Edges&lt;/strong&gt; 라는, 2001년의 논문입니다. 2001년 IEEE Transactions on Image Processing, Vol.10, No.2 에 발표된 논문으로, 이 분야 - image processing - 에서는 엄청나게 중요한 논문으로, 현재까지 1만 3천 회 가량 인용되었습니다.&lt;/p&gt;

&lt;p&gt;목표는 어떤 이미지가 주어졌을 때, 이 이미지의 외곽선 &lt;strong&gt;“Contour”&lt;/strong&gt; 를 따는 것입니다. 특히, 여기서는 segmentation이라고 해서 그림의 픽셀을 몇개의 클래스로 구분하는 문제를 해결하는 것으로 보고 있습니다. 예를 들어, 배경 앞에 사람이 서 있다면, 사람과 배경을 구분하는 문제를 classification이라고 할 수 있겠습니다. 꽤 오래 전 (저널 발표일 기준 2001) 논문이므로, 본격적인 Deep Learning의 시대가 오기 전의 방법론을 볼 수 있었습니다.&lt;/p&gt;

&lt;p&gt;먼저, 용어를 간단히 정의합니다. Segmentation과 Contour detection은 원래 약간 다른 문제지만, 여기서는 Segmentation의 방법으로 Level set (등고선) 의 Contour를 따는 방법을 생각하기로 합니다. 이를 위해, 예를 들어 어떤 grayscale 이미지가 주어지면, 이 선을 대충 색이 진한 쪽과 흐린 쪽으로 나누기 위해 진한 점들이 이루는 Contour를 찾고자 한다고 이해하면 되겠습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;key-ideas&quot;&gt;Key Ideas&lt;/h1&gt;
&lt;h2 id=&quot;energy-functional&quot;&gt;Energy functional&lt;/h2&gt;
&lt;p&gt;이 문제에서는, Segmentation 문제를 Functional Optimization의 문제로 환원합니다. 여기서 Functional이란, 정의역이 함수의 집합인 함수를 말합니다.&lt;/p&gt;

&lt;p&gt;특히, 우리는 결과물의 외곽선이 Smooth하기를 원하므로, $X$에서 Lipschitz Continuous 한 함수의 집합 $\mathcal{L}$ 에서 $\R$로 가는 함수열을 생각할 것입니다. 여기서 Lipschitz 연속이란 연속성보다 더 강한 개념으로, 점 $x, y$ 와 어떤 상수 $K$에 대해 $\norm{f(x) - f(y)} \leq K \norm{x - y}$ 를 만족하는 함수들을 의미합니다.&lt;/p&gt;

&lt;p&gt;함수의 Level set에 대해 논의하기 위해, 우리는 함수 $\phi \in \mathcal{L}$ 에 대해, $\phi = 0$ 인 점들을 이은 곡선을 $C$라고 정의합니다. 또한, $\phi(x) &amp;gt; 0$ 인 공간을 $A$, $\phi(x) &amp;lt; 0$ 인 공간을 $B$라고 쓰겠습니다. 마지막으로, 원래의 이미지 픽셀값을 $u_0(x, y)$ 함수로 나타냅니다.&lt;/p&gt;

&lt;p&gt;이제, 다음과 같은 Functional들을 정의합니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$Len(C)$ : 곡선의 길이. 곡선의 길이가 길면 $\phi$ 가 덜 smooth하기 때문에 (해석학적인 term이라기보다는, 기하적인 smooth함), 매끄러운 곡선을 그리도록 페널티를 통해 incentivise 합니다.&lt;/li&gt;
  &lt;li&gt;$Area(A)$ : $\phi(x) &amp;gt; 0$ 인 부분의 넓이. 길이와 기본적인 의미는 같습니다.&lt;/li&gt;
  &lt;li&gt;$\int_{A} \abs{u_0(x, y) - c_1}^2 \dd{x}\dd{y}$ : 어떤 실수값 $c_1$ 을 잡아서, $\phi$ 안쪽에서 $u_0$ 의 평균을 나타내고 싶습니다. 이때 이 평균값이 가급적 정확하기를 바란다는 의미입니다.&lt;/li&gt;
  &lt;li&gt;$\int_{B} \abs{u_0(x, y) - c_2}^2 \dd{x}\dd{y}$ : $\phi$ 바깥쪽에서도 똑같은 작업을 합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;직관적으로, 저 네 값 모두 작았으면 좋겠다는것을 알 수 있습니다. 앞 두개가 작으려면 함수가 대충 곡선으로 쭉 매끄럽게 이어져야 하고, 뒤 두개가 작으려면 그 안쪽과 바깥쪽에 어떤 intensity 값을 잡아서 그 값에 가깝게 잘려야 합니다.&lt;br /&gt;
우리는 저 네 Functional의 선형결합을 “Energy Functional” 이라고 부르기로 하고, 저 값을 minimize하는 $c_1, c_2, \phi$ 를 찾는 것을 목표로 합니다.&lt;/p&gt;

&lt;h2 id=&quot;integral-formulation&quot;&gt;Integral formulation&lt;/h2&gt;
&lt;p&gt;그러나, 저 식은 저대로는 상당히 계산하기가 어렵습니다. 좀더 계산을 잘 하기 위해, 식을 살짝 조절해 봅시다. 이를 위해, 헤비사이드 함수 $H$를 도입합니다. $H$는 $x \geq 0$ 일 때 1, $x &amp;lt; 0$ 일 때 0인 함수입니다. 이를 도입하면 $H(\phi(x, y)) = 1$ iff $\phi(x, y) \geq 0$ 가 성립합니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Length : 길이는 스토크스 정리와 헤비사이드 함수의 정의를 이용하면, 아래와 같이 쓸 수 있습니다. 
  \(L(\phi) = Len(C) = \int_{\R^2} \abs{\nabla H(\phi(x, y))} \dd{x}\dd{y}\) 
  당연히 일반적으로 $H(\phi(x, y))$ 는 미분이 불가능하지만, 우리는 Heaviside의 도함수를 Dirac-delta로 쓰고 있으므로 (in distribution function sense) 적분은 잘 됩니다. &lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
  &lt;li&gt;Area : 간단한 다변수 적분입니다. 
  \(S(\phi) = Area(A) = \int_{\R^2} H(\phi(x, y)) \dd{x}\dd{y}\)&lt;/li&gt;
  &lt;li&gt;역시 간단한 두 개의 다변수 적분식을 쓸 수 있습니다. 
  \(F_i(C) = \int_{\R^2} \abs{u_0(x, y) - c_1}^2 H(\phi(x, y))\dd{x}\dd{y}\) 
  \(F_o(C) = \int_{\R^2} \abs{u_0(x, y) - c_2}^2 (1 - H(\phi(x, y)))\dd{x}\dd{y}\)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;하나 관찰할 수 있는 것은, $c_1$과 $c_2$는 $\phi$를 고정하고 최적화하면 최적화할 수 있다는 점입니다. 구체적으로, 첫번째 식을 다시 앞서의 형태인 
\(\int_{A} \abs{u_0(x, y) - c_1}^2 \dd{x}\dd{y}\) 이렇게 돌려놓고 보면, $c_1$은 자명하게 $u_0$의 $A$에서의 ‘평균’ 이 되어야 합니다. 따라서, 앞으로 $c_1, c_2$는 $\phi$로부터 간단한 적분을 통해 계산 가능하므로, 위 식을 $\phi$로만 최소화한다고 문제를 단순화하겠습니다.&lt;br /&gt;
또한, 실제 알고리즘은 1, 2, 3에 각각 적당한 상수를 붙여서 계산합니다. 특히, (3) 의 $F_i$ 와 $F_o$에 다른 상수를 붙여서 계산하는데, 실제로는 원본 논문의 저자들도 상수를 대충 잡았고, 이 상수를 어떻게 잡아야 하는지에 대해서는 많은 논증이 없었으므로 저는 여기서 (1) * $\mu$ + (2) * $\nu$ + (3) * $\lambda$ 로 놓고 계산하겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;relaxation--euler-lagrange&quot;&gt;Relaxation &amp;amp; Euler-Lagrange&lt;/h2&gt;
&lt;p&gt;이제, 여기까지 오면서 우리가 최종적으로 무엇을 최소화하는지 보겠습니다. 
\(\int_{\R^2} \mu L(\phi) + \nu S(\phi) + \lambda(F_i(\phi) + F_o(\phi)) \dd{x}\dd{y}\)&lt;/p&gt;

&lt;h2 id=&quot;partial-differential-equation&quot;&gt;Partial Differential Equation&lt;/h2&gt;
&lt;p&gt;우리는 이런 편미분방정식은 풀 방법이 없기 때문에, 마지막으로 수치해석을 적용합니다. 구체적으로, Finite Differnce method를 이용해야 합니다.&lt;/p&gt;

&lt;p&gt;Finite Difference method는 별로 어렵지 않은데, $\Delta_{-}^{x}$ 같은 식으로 $x, y$ 방향 $+, -$ 로 네개의 time differnece를 정의하고 여러 공식들을 적용하기만 하면 됩니다. 편미방이 복잡하게 생겼지만 각 term은 그렇게 어렵지 않습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;이 알고리즘은 Noisy image에서도 생각보다 훌륭한 성능을 보여주고, vector-valued 같은 확장도 그렇게 어렵지 않습니다. 특히, 이런 식의 Energy Functional을 잘 정의하기에 따라서 범용성이 굉장히 높고 원하는 Feature가 있다면 추가로 embed 할 수도 있어서 확장성도 높습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;thoughts&quot;&gt;Thoughts&lt;/h1&gt;
&lt;ol&gt;
  &lt;li&gt;Parameter 가 상당히 중요해 보입니다. $\mu, \nu, \lambda$ 의 어떤 조합이 좋은 결과를 내는지에 대해서는 원본 논문에서는 별로 Discuss하지 않았는데, 실험적으로 확인해야 하는 걸까요? $\mu, \nu$ 는 어떻게 실험적으로 검증할 수 있어 보이는데, $\lambda$는 좀 오바인것 같습니다. 검색을 좀 해보니, 다른 논문 몇편에서 이미지의 어떤 computable한 성질들로부터 parameter를 자동으로 튜닝하는 논문들이 있었습니다.&lt;/li&gt;
  &lt;li&gt;Functional Optimization은 미적분학 II에서 변분법을 배운 이후로 처음인데, 일반적인 optimization의 방법론들과는 좀 다르다보니 어렵습니다. 특히 $f$ 에서 $x$를 움직여서 최적화하는 경우는 $f’$ 같은 정보들이 있는데, Functional은 어떻게 최적화하는지 잘 모르겠습니다. 
듣기로는 Banach space위에서의 Lagrange Multiplier같은 해괴한게 있다고 합니다. Functional도 결국 Banach space위에서 어떤 함수를 최적화하는 거니까, 비슷한 개념이 있을까요? 구체적으로는 Banach space $X$에서 정의된 Functional $F$의 도함수?&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;해석개론을 배우고 나서부터 디랙-델타를 적분에 활용하는게 오히려 정말 이해가 안 갔었는데, 이 개념은 Measure, Distribution function 등 실해석학 및 그 이상의 해석학을 배우면 다시 make sense 합니다. 잠시 공학수학의 관점으로 돌아가서 이 식을 받아들이기로 합니다. 어차피, 이 적분을 실제로 계산할 것은 아니니까요. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Wonseok Shin</name><email>gratus907@snu.ac.kr</email></author><category term="cs-adventure" /><summary type="html">Contents</summary></entry><entry><title type="html">MATH TESTBED</title><link href="http://localhost:4000/math-test/" rel="alternate" type="text/html" title="MATH TESTBED" /><published>2021-08-13T00:00:00+09:00</published><updated>2021-08-13T00:00:00+09:00</updated><id>http://localhost:4000/math-test</id><content type="html" xml:base="http://localhost:4000/math-test/">&lt;div id=&quot;toc&quot;&gt;
Contents
&lt;/div&gt;
* TOC
{:toc}
----------

정리 테스트

&lt;div class=&quot;definition&quot; data-title=&quot;위상&quot;&gt; 집합 $S$에 대해 다음 조건을 만족하는 $\mathcal{T} \subset \mathcal{P}(S)$ 를 위상(Topology) 라 한다. 
&lt;/div&gt;
이렇게 정리를 쓸 수 있다. 
&lt;div class=&quot;definition&quot;&gt; 집합 $S$에 대해 다음 조건을 만족하는 $\mathcal{T} \subset \mathcal{P}(S)$ 를 위상(Topology) 라 한다. 
&lt;/div&gt;
정리에 자동으로 번호를 붙일 수 있다.
&lt;div class=&quot;theorem&quot;&gt; 함수 $f$가 닫힌 구간 $[a, b]$ 에서 연속이면, 함수 $F(x) = \int_{a}^{x} f(t) \dd{t}$ 는 닫힌 구간 $[a, b]$ 에서 연속이고, 열린 구간 $(a, b)$ 에서 미분가능하며, $F' = f$ 이다. 
&lt;/div&gt;
Definition과 Theorem은 따로 번호를 붙인다.
&lt;div class=&quot;lemma&quot; data-title=&quot;lemma1&quot;&gt; 보조정리1
&lt;/div&gt;
보조정리도 쓸 수 있다.

- 증명은 CSS가 깨지므로, 이렇게 쓴다.
- 귀납법을 쓰자. $n = 1$일 때 성립한다.
- $n = k$ 일 때 성립하면 $n = k+1$ 일 때 성립한다.
- 단어 몇개를 추가했다.

&lt;div class=&quot;lemma&quot; data-title=&quot;Fatou Lemma&quot;&gt; 가측함수열 $f_n$이 $f_n \geq 0$ 을 만족하면, 다음이 성립한다.
$$\int_X \liminf_{n} f_n \dd{\mu} \leq \liminf_{n} \int_X f_n \dd{\mu}$$
&lt;/div&gt;
이름 있는 보조정리</content><author><name>Wonseok Shin</name><email>gratus907@snu.ac.kr</email></author><summary type="html">Contents</summary></entry><entry><title type="html">BOJ 15744, USACO 2018 Feb P3 Cow Gymnasts</title><link href="http://localhost:4000/ps-problems/BOJ15744/" rel="alternate" type="text/html" title="BOJ 15744, USACO 2018 Feb P3 Cow Gymnasts" /><published>2021-08-11T00:00:00+09:00</published><updated>2021-08-11T00:00:00+09:00</updated><id>http://localhost:4000/ps-problems/BOJ15744</id><content type="html" xml:base="http://localhost:4000/ps-problems/BOJ15744/">&lt;div id=&quot;toc&quot;&gt;
Contents
&lt;/div&gt;
* TOC
{:toc}
----------

출처 : USACO 2018 February Platinum Problem 3 - Cow Gymnasts (BOJ 15744)  
난이도 : solved.ac Diamond 3  
[문제 링크](https://www.acmicpc.net/problem/15744)
## 풀이 : 관찰
DHdroid가 '재밌는 문제' 라면서 들고왔습니다. 나름 재밌게 풀었는데 후반에 좀 허탈했습니다.
- 먼저, $N$개의 자리를 통틀어 최소값이 $m$ 이라고 하고, 그 최솟값이 있는 자리를 $k$번이라고 합시다. 
- $k - 1, k - 2, \dots k - m + 1$ 번 까지는 어쩔 수 없이 $k$번에 Contribute합니다. 
- $k-m$번은 $k$번에 Contribute해서는 안 되므로, 값이 정확히 $m$이여야 합니다. 
- 이와 같은 논증을 귀납적으로 반복하면, 다음 조건을 만족하는 조합이 '성공하는 조합' 임을 알 수 있습니다. 
  - 먼저 최솟값 $m$에 대해 주기 $\gcd(m, N)$ 을 갖고, 
  - 각 주기 내에서의 값은 $m$ 또는 $m+1$ 이어야 합니다. 
- 따라서, 다음 식을 계산하는 것으로 끝납니다.
  $$1 + \sum_{i = 1}^{N - 1} \left(2^{\gcd(i, N)} - 1\right)$$
  이 관찰을 하는 것까지만이었으면 아마도 플래티넘 정도의 문제였을 것입니다. 


## 풀이 : 시간 복잡도 줄이기
$N = 10^{12}$ 이기 때문에, 이 식을 시간 내에 단순하게는 계산할 수 없습니다.  
앞의 잡다한 부분은 떼고 $\sum_{i = 1}^{N} 2^{\gcd(i, N)}$ 을 빠르게 계산하는 방법에 대해 생각해 보겠습니다.

먼저, $\gcd(i, N) = g$ 인 $i \leq N$의 개수를 빠르게 셀 수 있습니다. 
- $i$는 $g$의 배수여야 하므로 $i = gk$ 라고 쓰면, $\gcd(k, N/g)$ 이 1이어야 하며
- $k &lt; N / g$ 여야 합니다.  
- 따라서, 이러한 $k$는 정확히 $\phi(N / g)$ 개 있고, 다음 식을 계산하면 됩니다. 
  $$\sum_{g \di N} 2^g \phi(N / g)$$
- $g \di N$ 을 모두 구하는 것은 $O(\sqrt{N})$ 시간이 걸리며, $g$가 몇 개 있는지는 정확히 쓰기 매우 어렵습니다.
  - 다만, PS에서 일반적으로 쓰는 바운드는 $O(n^{1/3})$ 입니다. 재밌는 사실은, 이 바운드는 수학적으로 찾은 바운드가 아니라, $10^{9}$ 까지의 수들 중 약수가 가장 많은 수가 1,344개, $10^{18}$ 까지의 수들 중 가장 많은 수가 103,680개의 약수를 갖기 때문에 대충 저만큼 잡으면 된다는 것이 알려져 있다는 점입니다. 
  - $O(n^{1/3})$ 개의 약수 각각 $\phi(g)$ 를 구하는 시간은 조금 빡빡합니다. 구체적으로는 개당 $O(n^{1/2})$ 시간인데
  - 실제로는 $g$들 중 상당수가 $N$에 비해 많이 작기 때문에, $O(n^{1/3})$개의 약수에 대해 각각 $\phi$함수를 구하는 풀이는 시간 제한을 통과합니다.
- 더 최적화하는 방법은, $\phi$ 함수는 그 수의 소인수분해 형태에 의해 정해지며, $N$의 소인수는 많아야 수십 개 선이라는 점입니다. 
- 각각의 $g \di N$ 은 $N$의 소인수의 부분집합을 갖기 때문에, $p_1^{e_1} p_2^{e_2} p_3^{e_3}\dots$ 에 대해 각 소인수의 개수를 iterate하면 빠르게 구할 수 있습니다.
- 잘 짜면 $O(n^{1/2} \log n)$ 에 구할 수 있다고 합니다.

## 풀이 : 더 고통받기
- 저는 저 식으로 통과할 자신이 없었기 때문에, 뭔가 새로운 아이디어를 찾아야 한다고 생각했습니다. 
- Identity function $\iota$, Mobius function $\mu$ 에 대해, $\phi = \mu * \iota$ 임이 알려져 있습니다. (디리클레 합성곱)
- $f(n) = 2^n$ 을 $p$ 함수라고 하면, 우리의 문제는 $p * \phi$ 의 값을 구하는 것이므로, $p * \mu * \iota$ 를 구하는 것이고
- 다시 이를 $\mu * (p * \iota)$ 로 바꾸어 계산할 수 있습니다.

처참하게도, $\mu$를 계산하기 위해 포함-배제의 원리를 이용해야 하기 때문에 시간 복잡도는 전혀 줄어들지 않고 $\log$ 항이 하나 더 붙습니다 (뫼비우스 함수의 값을 모두 계산하는게 아니라서, map 같은 자료구조를 이용해야 하기 때문. 심지어 결과적으로 이 풀이는 위 풀이의 $O(n^{5/6}))$ 보다도 훨씬 느렸습니다. 간신히 시간 제한을 통과할 수 있었습니다. :(</content><author><name>Wonseok Shin</name><email>gratus907@snu.ac.kr</email></author><category term="ps-problems" /><category term="number-theory" /><summary type="html">Contents</summary></entry><entry><title type="html">V. Graph Basics, Heaps</title><link href="http://localhost:4000/ds-alg-note/05-graph-basics/" rel="alternate" type="text/html" title="V. Graph Basics, Heaps" /><published>2021-08-09T00:00:00+09:00</published><updated>2021-08-09T00:00:00+09:00</updated><id>http://localhost:4000/ds-alg-note/05-graph-basics</id><content type="html" xml:base="http://localhost:4000/ds-alg-note/05-graph-basics/">&lt;div id=&quot;toc&quot;&gt;
Contents
&lt;/div&gt;
* TOC
{:toc}
----------

저번 세션에 이어서, 자료구조를 공부하고자 합니다.

Graphs
------

추상적으로, 그래프는 다음과 같이 정의된 $G = (V, E)$를 의미합니다.

-   $V$는 정점의 집합으로, 그냥 원소들을 모은 집합으로 생각하면 됩니다.

-   $E$는 $V \times V$의 어떤 부분집합으로, 원소들 간의 연결성을
    표현합니다.

즉, 점 (정점, 노드) 들과 그들 간의 연결선 (간선, 에지) 들의
configuration을 그래프라고 정의한다는 의미입니다. 몇가지 용어를
짚자면\...

-   Directed/Undirected graph : Directed graph는 간선 $(u, v)$ 와
    $(v, u)$ 를 다른 것으로 보고, Undirected graph는 같지 않은 것으로
    봅니다.

-   Multigraph : $(u, v)_1$ 과 $(u, v)_2$로, 같은 간선이 여러 개 있을 수
    있는 - 즉, $E$가 set이 아니라 multiset인 - 그래프를 말합니다.
    일반적으로 우리는 고려하지 않을 것입니다.

-   Simple graph : $E$가 중복을 허용하지 않을 뿐 아니라, $(u, u)$ 도
    허용하지 않는 그래프를 말합니다.

-   Path : 간선들을 따라 돌 수 있는 '경로' 를 말합니다.

-   Circuit / Cycle : Path의 시작점이 끝점과 같은 경우를 말합니다.

-   Adjacent : 정점 $u, v$에 대해, $(u, v) \in E$ 이면 adjacent라고
    말합니다. 또한 $u, v$는 서로의 neighbor입니다.

-   Connected Component : 정점 $u, v$에 대해, $u$에서 시작해서 $v$에
    도착하는 path가 존재하면 같은 connected component에 있다고 말합니다.
    특히 모든 정점이 하나의 connected component를 이루면 connected
    graph라고 말합니다.

Unless otherwise specified, $V$는 정점의 집합이면서 간혹 정점의 개수를
$V$개라고 부를 것입니다. $E$도 마찬가지입니다. (Abuse of notation) 또한,
앞으로 그냥 그래프 $G$라고 하면 $n$개의 정점과 $m$개의 간선으로 이루어진
Undirected Connected Simple Graph를 생각하겠습니다. 그래프에 대한 많은
논증은 각 Connected Component를 독립적으로 생각할 수 있으며,
Multigraph도 대충 비슷하게 다루어질 수 있기 때문입니다.

Implementation of Graphs
------------------------

그래프에 대해서는 후에 다시 자세히 보겠지만, 여기서는 그래프를 어떻게
구현할지만 생각해 보겠습니다.

흔히 사용하는 그래프 구현은 두 가지가 있습니다.

-   Adjacent matrix : 2차원 $n \times n$ 배열을 잡고, $A_{ij}$ 를
    $(i, j)$ 간선의 존재 여부를 인코딩하는 것입니다. Directed graph를
    기준으로 생각하는 것이 조금 더 자연스럽고, Undirected graph일 때는
    $A$가 대칭행렬이 될 것입니다.

    이론적으로 이 방법이 조금더 자연스럽게 그래프를 대할 수 있는데, 특히
    Adjacency matrix $A$의 선형대수학적 성질로부터 (Eigenvalue 등)
    그래프의 성질을 알 수 있는 것들이 많이 있기 때문입니다.

-   Adjacent list : Vector나 List같은 다이나믹한 뭔가를 정점마다 하나씩
    $n$개 잡고, &quot;이 정점에 인접한 정점&quot; 들의 리스트 (배열) 을 관리하는
    관점입니다.

그래프의 Density에 대해 생각해 봅시다. 일상에서 만나는 대부분의 (큰)
그래프들은, 굉장히 sparse합니다. 예를 들어, 페이스북 전체의 친구 관계를
그래프로 그린다면, 페이스북 유저 10억 명 중 여러분의 페친은 많아야 천 명
단위일 것이므로 전체 가능한 간선들 중 100만 분의 1 정도밖에 사용되지
않는다는 의미입니다. 대충 간선이 $n^2$개 근처일 때 dense, 그보다 많이
작으면 sparse라고 하겠습니다.

Adjacent matrix가 이론적으로 보다 아름답게 느껴질 수 있지만, 그래프가
sparse할 때 adjacent matrix는 $O(n^2)$ 메모리를 소모한다는 심각한 단점이
있습니다. 우리는 많은 경우에 sparse한 그래프를 다루고 싶고, 간선을
따라가면서 작업을 하고 싶습니다. 예를 들어\...

    for (int nxt = 0; nxt &lt; n; nxt++)
        if (A[i][nxt])
            traverse(A[i][nxt]);

    for (int j = 0; j &lt; A[i].size(); j++)
        traverse(A[i][j]);

이 코드는 둘 다 $i$의 neighbor들을 돌면서 traverse함수를 호출하지만,
전자의 경우에는 if문이 추가될 뿐 아니라 $O(n)$ 개만큼 확인해야 합니다.
만약 페이스북 친구 그래프에 대해 코드를 돌리면, 위쪽 경우에는 내
친구들을 찾기 위해 10억명의 모든 유저를 탐색하는 반면 후자는 그런 필요가
없습니다.

다만 인접행렬이 구현이 좀더 간단하고, 행렬 곱셈을 통해 연결성을 본다던가
하는 연산들이 가능하기 때문에, 필요한 경우에는 사용할 수 있어야 합니다.
그러나 위 이유들 때문에, 우리는 기본적으로 인접리스트를 그래프의 기본
표현으로 보겠습니다.

## Trees / Binary Trees


정점 $n$개 중 어떤 루트가 있고, 루트로부터 모든 정점까지의 경로가
유일하게 존재하는 그래프를 Tree라고 합니다. 이때 루트로부터 경로를
내렸을 때 내 바로 이전 노드를 parent, 그 이전 노드들을 ancestor라고
합니다 (반대는 child, descendant) 트리의 경우, 인접 리스트 표현 외에도
그냥 $n$칸 배열에 각 tree의 parent node를 저장하는 방법으로도 저장할 수
있습니다.

Binary Tree란, 모든 노드의 Child node가 최대 2개인 트리를 의미합니다.
구현의 편의와, 다양한 활용처 때문에 매우 자주 활용되는 자료 구조입니다.
우리는 앞으로 Binary tree 노드를 다음과 같이 생각할 것입니다.

    struct node {
        int val, id;
        node * left;
        node * right;
    } root;

즉, 각 노드가 id와 어떤 값을 하나 가지고 있고, 자신의 left / right
child로 가는 포인터를 가지고 있음을 의미합니다.

이진 트리의 Special case로 다음과 같은 경우들이 있습니다.

-   Full Binary Tree : 모든 노드가 0개 또는 2개의 자식 노드를 갖습니다.

-   Complete Binary Tree : 가장 아래 층을 제외한 모든 층이 최대한 노드가
    차 있고, 가장 아래 층에서도 최대한 왼쪽으로 노드가 몰려있는
    트리입니다.

-   Perfect Binary Tree : Complete 이면서 Full 인 binary tree입니다.
    높이가 $h$인 Perfect Binary Tree의 노드는 항상 $2^h$임을 기억하세요.

일반적으로, Balance가 잘 잡혀 있는 binary tree는 높이가 $\log n$
정도이고, 한 줄에 가까운 binary tree는 높이가 $n$ 정도임을 기억하면
좋습니다.

## Heaps

Heap이라는 자료 구조는, 다음과 같은 성질을 만족하는 트리를 의미합니다.

-   Complete Binary Tree. 즉, 최대한 균형이 잡혀 있고, 노드가 남는다면 왼쪽으로 몰아넣은 상태의 트리여야 합니다.

-   힙 성질. 부모 노드에 쓰여 있는 값은, 자식 노드에 쓰여 있는 값보다
    항상 작거나 같다.

이제, Complete Binary Tree의 구조를 생각해 보면, 한 층씩 밑으로 내려올 때마다
노드의 개수가 2배씩 늘어나므로, 전체 노드의 개수가 $n$개 정도일 때, 높이
$h$ 는 $h \in \Theta(\log n)$ 입니다. 또한, Heap의 성질 상, Heap의
임의의 노드를 하나 잡으면, 그 노드를 root로 하는 subtree도 다시 heap임을
알 수 있습니다.

### Heap Operation 

Heap의 기본 operation으로, 다음과 같은 두가지 연산을 생각합니다.

-   Heap에 어떤 수 $x$를 삽입하는 Push 연산

-   Heap의 노드를 삭제하는 연산

Pop 연산에서 루트만 생각해도 되는 이유는 앞서 말한 바와 같이, 모든
노드가 자신을 subtree로 하는 heap의 루트이기 때문입니다. 따라서, 위 두
연산만 있으면 Heap에 임의의 원소를 삽입하고 삭제할 수 있다는
의미입니다.\
이 연산을 어떻게 구현할지 생각해 봅시다. Push의 경우, 다음과 같은
과정으로 수행합니다.

-   무조건 완전 이진 트리 성질을 만족하는 것을 우선하여, 끝 자리에
    삽입합니다.

-   끝자리의 삽입으로 인해 힙 성질이 깨졌을 수 있으므로, 그 자리부터
    올라오면서 힙을 수선합니다.

힙을 수선한다고 하는 말은, 실제로는 삽입한 자리부터 올라오거나
내려가면서, 자식 노드가 부모 노드보다 큰 값을 가졌으면 두 노드를
교환한다는 의미입니다. 앞으로 이와 같은 &quot;힙 수선&quot; 이라는 말을 계속 쓸
텐데, 기본적으로 특정 노드에서 힙을 수선한다면, 루트부터 리프까지
내려가면서 매 단계 최대 2개씩, 많아야 $2h \in O(\log n)$ 개의 노드만
보면 됩니다.\
\
힙에서 노드를 삭제하는 연산은 다음과 같습니다.

-   완전 이진 트리 성질을 만족하는 것을 우선하여, 끝 자리 노드와 삭제할
    노드의 자리를 바꿉니다.

-   끝 자리의 노드를 지워도 완전 이진 트리 성질이 깨지지 않습니다.

-   이제, 방금 교환에 의해 힙 성질이 깨졌을 수 있으므로, 위아래로
    오가면서 힙을 수선합니다.

과정을 보면, 수선 외의 모든 Operation은 구현을 잘 하면 $O(1)$에 해결할
수 있을 것 같아 보입니다. 수선에서는 값 간의 비교가 최대 $O(\log n)$ 번
일어나기 때문에, 전체 알고리즘의 수행 시간은 $O(\log n)$ 입니다.

### Heap Sort

빈 Heap과 $n$ 크기의 배열에서 시작해서, 모든 element를 Heap에 넣습니다.
그다음, 루트가 전체 heap의 최솟값이므로, 루트를 확인하고 삭제하는 연산을
$n$번 반복하면, 작은 원소부터 순서대로 나오게 됩니다. 넣고 빼는데 매번
$O(\log n)$씩이므로 항상 $O(n \log n)$ 정렬임이 보장됩니다!

### Heap Implementation 

힙이 트리 구조라고 해서, 실제로 포인터 세개짜리 노드로 (Parent,
Left-Child, Right-Child) 구현해야 할 필요는 없습니다. 실제로는, 다음과
같은 원리로 배열에 트리를 얹는 느낌으로 구현하면 유용합니다.

-   1번 노드를 루트로 한다.

-   $n$번 노드의 두 자식 노드는 $2n$, $2n+1$번으로 한다.

-   그러면, 자동으로 $n$번 노드의 부모 노드는 $n/2$번 노드 (정수
    나눗셈만 하면 바로 나옵니다!).

이 '배열에 이진트리를 얹는' 구현은 나중에 이진트리 기반의 자료구조를 구현할 때 정말 많이 쓰게 됩니다.


## Additional topics and Problems 

1.  Heap sort에서는 어차피 복잡도가 달라지지 않아서 빈 힙에 $n$개의
    원소를 순서대로 삽입하는 식으로 힙을 구성했지만, 실제로는 이미 있는
    배열을 그대로 Heap으로 만드는 Heapify() 연산은 이보다 빨리 할 수
    있습니다. 다음과 같은 과정을 이용합니다.

    -   가장 아래 노드 ($N$번) 부터 시작합니다.

    -   자신의 값이 부모노드보다 높다면, 부모노드와 값을 교환합니다.

    -   이때의 값이 자식 정점보다 작다면, 자식노드 중 작은 쪽과 값을
        교환합니다. (리프까지 내려가면서)

    -   재귀적으로 반복합니다.

    Heap 수선 과정과 똑같아 보이지만, 약간의 차이는 위쪽으로는 끝까지
    확인하지 않는다는 점입니다. 아래쪽으로는 재귀적으로 반복해야 하지만,
    부모노드와 값을 교환했다고 해서 한번에 위쪽까지 힙을 수선하지 말고,
    부모노드의 값은 나중에 그 노드 차례가 될 때 확인해도 됩니다.\
    이 알고리즘의 시간 복잡도를 생각해 보면, 자신의 높이에 비례하는
    정도의 연산이 필요함을 알 수 있습니다. 대략 각 노드마다 높이가
    $h$라면 대략 $O(h)$번 연산이 필요합니다. 높이가 $k$인 노드가
    $2^k$개임을 이용하여, 이 알고리즘이 주어진 Array를 Heap으로 고치는
    데 $O(n)$ 시간밖에 걸리지 않음을 보이세요.

    힌트) 각 노드는 '최대 얼마나' 내려갈 수 있나요? 

## Programming Practice

1.  BOJ 19535번을 해결해 보세요.

2.  BOJ 11279번을 (STL의 priority queue를 쓰지 말고) 해결해 보세요.

3.  BOJ 1655번을 해결해 보세요.</content><author><name>Wonseok Shin</name><email>gratus907@snu.ac.kr</email></author><category term="ds-alg-note" /><summary type="html">Contents</summary></entry><entry><title type="html">IV. Binary Search</title><link href="http://localhost:4000/ds-alg-note/04-binary-search/" rel="alternate" type="text/html" title="IV. Binary Search" /><published>2021-08-09T00:00:00+09:00</published><updated>2021-08-09T00:00:00+09:00</updated><id>http://localhost:4000/ds-alg-note/04-binary-search</id><content type="html" xml:base="http://localhost:4000/ds-alg-note/04-binary-search/">&lt;div id=&quot;toc&quot;&gt;
Contents
&lt;/div&gt;
* TOC
{:toc}
----------


## Binary Search

Up &amp; Down 게임을 해 보셨나요? 1부터 1000까지의 수 중 하나를, 10번 정도면
맞출 수 있다는 사실을 알고 계시나요? 아마도, 모두가 절반씩 잘라서
확인하는 전략을 사용할 것입니다. 이 전략을 우리는 Binary Search 라고
부릅니다.\
$N$ 크기의 배열이 정렬되어 있음을 안다면, 이 Up &amp; Down 전략을 이용하여,
우리는 한번 질문할 때마다 절반씩 문제의 크기를 줄일 수 있습니다. 예를
들어, 10개짜리 배열의 5번을 열어 봤는데 우리가 원하는 값보다 크다면,
1번부터 4번 사이에 원하는 값이 있음을 아니까 2번이나 3번을 물어보면
됩니다. 이렇게 하면, $\order{\log n}$ 시간에 정렬된 배열에서 특정한 값을
찾을 수 있습니다. 이 알고리즘은 정말 유용합니다! Programming
Practice들을 풀면서 확인해 보세요 :)

## Bisection (parametric) Search 

Binary Search를 확인하여, 다음과 같은 질문에 빠르게 답할 수 있습니다.\
어떤 함수 $f : \R \to \R$가 단조 증가하는 연속 함수일 때, $f(x) = k$ 인
$x$를 찾아라.\
정확히 말하자면, 이 질문에 정확하게 답할 필요까지는 없고, 충분히 가까운
$x$를 찾으면 됩니다. 이제, $f$가 단조증가한다는 성질을 이용하여, 충분히
작은 수와 충분히 큰 수로 양쪽 끝값을 잡아 놓고, 그 사이를 구간으로 이분
탐색합니다.\
예를 들어, $2^{1/3}$ 을 계산해야 한다고 생각해 봅시다. 이때, 우리는 답이
되는 $x$가 $x^3 = 2$ 를 만족하며, 이 값이 $0$과 $2$ 사이에 있음을
압니다. 이 구간의 중간인 $1$을 이용하여, $1^3 = 1 &lt; 2$ 이므로, 답이
$1$과 $2$ 사이임을 압니다. 이제, $1.5^3 &gt; 2$ 이므로, 답이 1과 $1.5$
사이임을 압니다. 이제 $1.25$를 시도하고\...\
이 알고리즘을 Parametric Search라고 합니다 (사실은, 이 단어는 더
일반적인 상황에서 쓰는 말입니다. 굳이 말하자면 Bisection search 정도가
정확할 것 같은데, 우리가 관심있는 Parametric Search가 이쪽이
대부분이라서 그런 것 같기도 합니다). Parametric Search는 Binary Search의
일반화이면서, 정말 많은 것들을 할 수 있습니다. 앞으로 대회 준비나 공부를
더 하다 보면, 이 아이디어를 이용하는 문제를 정말 많이 만나게 됩니다.  
Parametric과 Binary 를 구분하지 않고 Binary Search를 이용한다고
말하는 사람도 많이 있고, 사실 이걸 굳이 구분할 필요가 있다고 생각하지는
않습니다. Binary라고 쓰더라도 맥락에 따라 이해해 주세요.

## Ternary Search

\* 이 subsection을 skip해도 뒤 내용에 영향이 없습니다.  
어떤 함수 $f : \R \to \R$가 볼록함수라고 할 때, $f(x)$ 의 최솟값을 찾는
문제를 생각해 봅시다. 만약 $f$가 미분 가능한 함수라면, $f$의 도함수를
생각했을 때, $f'(x) = 0$이 되는 $x$를 Binary Search로 찾는 방법을 생각할
수 있겠습니다. 그러나 우리가 생각하는 함수가, '대충 볼록하게 생기긴
했지만' 미분이 가능하진 않을 수도 있습니다. 이경우에, $\log$ 시간에
최솟값을 찾는 방법을 생각해 봅시다. 단, 이 함수 $f$가 평평한 구간을 갖지
않는다고 합시다.

-   충분히 작은 $L$과 큰 $R$을 생각합니다.

-   $[L, R]$의 삼등분점인 $p, q$를 생각합니다.

-   일반성을 잃지 않고, $f(p) &gt; f(q)$ 라고 가정합시다. 이때, 만약
    최솟값이 $[L, p]$에 있고, 그 최소점이 $x = t$라면, 함수가 적어도
    $t$에서 $p$까지 사이에서 증가하는 구간이 있고, 다시 $q$까지 감소해야
    합니다. 이는 $f$의 볼록성과 모순이므로, 최솟값이 $[p, R]$ 사이에
    있습니다.

-   우리가 보는 구간의 길이가 $\frac{2}{3}$으로 줄어들었습니다!

이제, 생각해 보면 매번 $2/3$으로 구간을 줄이는 것이므로, $\log_{3/2} n$
시간에 ($n$은 우리가 필요한 정밀도와, 구간 길이의 비율입니다. 간단히
말해, $[0, 10]$ 사이에서 $0.001$ 수준의 정밀도를 얻는 것은 1만 칸 중
하나를 찾는 느낌으로 접근하면 된다는 얘기입니다) 답을 얻을 수 있고,
Asymptotic하게는 로그의 밑은 무의미하므로 $\log$ 시간 알고리즘이 됩니다.
이 알고리즘을 Ternary Search, 삼분 탐색이라고 부릅니다.

이분 탐색이나 삼분 탐색은 그 자체로도 매우 의미있지만, 다른 알고리즘과
결합되었을 때 그 유용성이 더욱 부각됩니다. 그리고 처음 접했을 때
생각해내기 어렵기 때문에, 많은 연습이 필요합니다.</content><author><name>Wonseok Shin</name><email>gratus907@snu.ac.kr</email></author><category term="ds-alg-note" /><summary type="html">Contents</summary></entry><entry><title type="html">III. 정렬과 탐색</title><link href="http://localhost:4000/ds-alg-note/03-sorting-and-searching/" rel="alternate" type="text/html" title="III. 정렬과 탐색" /><published>2021-08-09T00:00:00+09:00</published><updated>2021-08-09T00:00:00+09:00</updated><id>http://localhost:4000/ds-alg-note/03-sorting-and-searching</id><content type="html" xml:base="http://localhost:4000/ds-alg-note/03-sorting-and-searching/">&lt;div id=&quot;toc&quot;&gt;
Contents
&lt;/div&gt;
* TOC
{:toc}
----------

우리의 목표는, $n$개의 원소를 가진 배열을 정렬하는 것입니다. 편의상,
다음과 같은 사실을 가정합니다.

-   정렬은 작은 것부터 큰 것 순서로 늘어놓는 것을 말합니다. ex) 1, 2, 3,
    4, 5

-   $n$개의 원소에 대한 사전 정보는 전혀 없습니다.

-   $n$개의 원소는 모두 Random-access 가능한 형태로 저장되어 있습니다.
    [^1]

## Slow Sorting Algorithm

자연스럽게, 정렬을 처음 생각하면 보통 $\order{n^2}$ 알고리즘을 생각하게
됩니다. 여기서는 비교적 간단하지만 유용한 Insertion sort 에 대해서만
알아보겠습니다. 한번 for loop을 실행할 때마다, 새 원소의 올바른 자리를
찾아 준다고 생각하면 됩니다.

    int key, j;
    for(int i = 1; i &lt; size; i++) {
        key = array[i];
        j = i;
        while(j &gt; 0 &amp;&amp; array[j-1]&gt;key) {
            array[j] = array[j-1];
            j--;
        }
        array[j] = key;
    }

이 알고리즘을 잘 살펴보면, 다음과 같은 사실들을 확인할 수 있습니다.[^2]

-   Stable : 같은 원소 두 개, 즉 $x_1$ 과 $x_2$ 가 있을 때, 이 둘의
    순서가 정렬 후에 바뀌지 않습니다. 이게 왜 필요하냐면, 이름 순으로
    정렬된 사람의 목록을 나이 순으로 정렬한 다음에도, 같은 나이인
    사람들끼리는 이름순으로 정렬되어 있음이 보장된다는 뜻입니다.

-   In-place : 추가적인 메모리를 거의 소모하지 않습니다. 정의에 따라
    다르지만, $O(n)$ 보다 작은 추가 메모리를 소모한다는 의미로
    받아들이기도 합니다.

-   Adaptive : 좋은 데이터가 주어지면, 그 성질을 활용해서 정렬을 더 빨리
    끝낼 수 있습니다. [^3]

## Fast Sorting Algorithms
### Heap Sort

Heap 이라는 자료구조를 이용하는 정렬 방법으로, 나중에 Heap을 다룰 때 공부할 예정입니다.

### Merge Sort

Merge sort (합병 정렬) 은 대표적인 Divide and Conquer 알고리즘입니다. 이
분할 정복 방법론 자체는 나중에 본격적으로 다루겠지만, 간단히 말하자면
다음과 같습니다.

-   큰 문제를 한 번 푸는 대신, 이를 작은 문제로 나누어서

-   각각의 작은 문제를 풀고

-   이 작은 문제의 결과들을 합치는 방법

앞서 공부한 Master theorem을 이용하여, 이러한 유형의 알고리즘들이 왜 더
빨라지는지, 어떻게, 얼만큼 더 빨라지는지 이해할 수 있습니다. 여기서
주목할 만한 점은 크게 두 가지입니다.

-   작은 문제로 나누어 풀고 합치는게 정말 더 빠른가?

-   작은 문제는 어떻게 푸는가?

1번 질문의 답은, Master theorem이나 이를 확장한 정리를 이용해 답하면
됩니다. 2번 질문은, 작은 문제가 큰 문제와 같지만 $n$만 작아진 버전이라는
사실에 주목하여, **더 작은 문제** 로 나누어서 더 작은 문제를 풀고, \....
하면 됩니다. 간단히 말해서, **푸는 방법이 자명해질 때까지** 문제를
줄이면 됩니다! 예를 들어, 정렬의 경우 배열의 원소가 2개 남으면, 자명하게
해결할 수 있습니다. [^4] 이제, Merge sort의 구체적인 방법을 생각해
봅시다.

-   배열을 두개로 나눕니다.

-   각각의 배열을 Merge sort 합니다.

-   정렬된 두 배열을 합칩니다.


![picture 1](../../images/498956df70149475da97a6b92fa5729569fc056992112f16692130a63a8ae869.png)  

시간 복잡도를 생각해 봅시다. $n$개의 원소를 가진 배열을 정렬하는데
$T(n)$ 의 시간이 걸린다면, 두개로 나누어 각각을 정렬하는데 한번에
$T(n/2)$ 씩 2번이 필요할 것입니다. 이제 정렬된 두 배열을 합치면 됩니다.\
합치는 과정은, 두 배열이 이미 정렬되어 있음을 이용합니다. 두 배열 각각의
시작점을 표시하고, 더 작은 쪽을 챙기고, 챙긴 원소를 빼는 방법으로
생각하면 됩니다. 구현을 처음 해본다면 상당한 주의가 필요하지만,
$\Theta(n)$에 가능하다는 것은 쉽게 이해할 수 있을 것입니다.\
따라서, $T(n) = 2T(n/2) + \Theta(n)$이고, 이를 마스터 정리로 풀면
$T(n) \in \Theta(n \log n)$ 을 얻습니다.

### Quick sort

Quick sort는 비슷하게 Divide and Conquer 방식을 이용하는 알고리즘으로,
더 간단하게 설명할 수 있습니다. 다음 세 줄이면 충분합니다.

-   적당한 Pivot을 고릅니다.

-   Pivot보다 작은 원소는 왼쪽으로, 큰 원소는 오른쪽으로 보냅니다.

-   Pivot의 좌우에 대해 반복합니다.

이 과정을 반복하여 전체 배열을 정렬하는 방법입니다. 이 방법이 왜 Quick
이라고 불리냐면, 실제로 돌려보면 평균적으로 Merge나 다른 정렬보다 빠르기
때문입니다. 그러나 이 방법에는 치명적인 문제가 있습니다.\
적당한 Pivot을 어떻게 고르느냐에 따라, 알고리즘의 성능이 크게
좌우됩니다. 이 알고리즘의 성능은, Pivot보다 작은 원소가 $k$개, 큰 원소가
$n-k-1$개라고 할 때, $T(n) = T(k)+T(n-k-1)+\Theta(n)$ 형태로 나타난다는
사실을 어렵지 않게 알 수 있습니다. 모든 지점에서 $k = 0$이라면, 즉 운이
매우 없어서 / 또는 데이터가 매우 불리하게 주어져서, 매번 가장 작은
원소가 Pivot에 걸린다면 수행 시간이 $\Theta(n^2)$가 됩니다. 이를 막기
위해서는, 비교적 중간값에 가까운 Pivot을 골라야 합니다. 그러나 이것은
필연적으로 Pivot 고르는 시간을 필요로 하게 됩니다. 크게 다음의 방법이
**주로** 쓰입니다.

-   Random : 배열의 **임의의** 원소를 씁니다.

-   Median-of-3 (9) : 3 (9)개를 임의로 뽑아 보고, 그중 중간값을
    Pivot으로 씁니다.


### Hybrid Sort

Quick sort는 $n$이 작을 때 상대적으로 느립니다. 반면, $O(n^2)$ Sorting
algorithm들은 단순하기 때문에 $n$이 작으면 매우 빠릅니다. 그래서, Hybrid
sort 라고 해서, Quick sort 같은 알고리즘을 쓰다가 원소의 개수가 적어지면
[^5] Insertion으로 마무리하는 알고리즘이 있고, 이것의 성능이 단순
Quick보다 훨씬 좋습니다.

## Selection Algorithms

우리의 다음 목표는, $n$개의 원소 중 $k$번째로 큰 원소를 찾는 것입니다.
이것을 'selection', 선택 문제라고 부르기로 합니다. 잠깐 생각해 보면,
정렬하고 나면, $\order{1}$ 에 선택 문제를 풀 수 있으므로, 선택 문제에
대한 알고리즘의 후보로 정렬을 쓸 수 있습니다. 다시 말해, 선택 알고리즘은
적어도 정렬보다는 쉬운 문제 (Computationally, 시간 복잡도가 같거나 더
작은 문제) 일 것이라는 생각을 할 수 있습니다. 구체적으로, 우리는 어떤
$k$에 대해서든, $\order{n}$에 해결하고 싶습니다. 가장 작은 원소, 가장 큰
원소 등은 $\order{n}$에 찾을 수 있음이 자명하기 때문입니다.

### Quickselect

Quickselect algorithm은 Quicksort를 응용한 방법입니다. 먼저, Quicksort
알고리즘을 크게 다음과 같이 정리합시다.

-   Partition : pivot을 기준으로 좌우로 원소들을 분할하는 작업.

-   Recursive Quicksort $\times 2$

이제, 생각해 보면, Partition을 똑같이 해보고 나서, 왼쪽 또는 오른쪽 중에
어느 쪽에 우리가 원하는 $k$번째 원소가 있는지를 알 수 있습니다. 예를
들어, 10개의 원소가 3개-pivot-6개로 나누어져 있고, 우리가 7번째 작은
원소를 원한다면, 왼쪽은 버리고 오른쪽에 대해서만 재귀적으로 들어갑니다.
즉, 오른쪽 part에 대해 3번째로 작은 원소를 고르면 됩니다. (5, 6, 7, 8,
9, 10 중 3번째를 고른다는 의미)\
이 방법은 정말 빠를까요? 얼마나 빠른지를 생각해 봅시다. 만약에, 한번
실행마다 절반씩 원소를 줄일 수 있다면 (최적의 Partition), $n$, $n/2$,
$n/4$ $\dots$ 에 대한 Partition만 하면 끝나므로, 이를 무한급수로
생각하면 $2n \in \order{n}$ 시간에 해결할 수 있습니다. 즉, Pivot을 잘
고르면 $\order{n}$ 시간에 해결할 수 있음을 의미합니다. 그러나, Quick
sort와 마찬가지로, Pivot을 계속 못 고르면 $n^2$ 시간이 걸릴 수도
있습니다. 이런 알고리즘에 어떤 의미가 있는지는 문제 iv 번을 참고하세요.

### Worst-Case Linear Selection

우리의 목표에 있어서 별로 중요하지 않기 때문에, 매우 간단하게만
설명하겠습니다. 아이디어는 약간의 시간을 써서 Quickselect의 pivot이
나쁘지 않게 하는 것입니다. [^6]

1.  전체 $n$ 크기의 배열을, $n/5$개의 5개짜리 배열로 나눕니다.

2.  5개짜리 배열 각각의 중간값을 찾습니다.

3.  $n/5$개의 수들 중, 중간값을 재귀적으로 구합니다.

4.  이제, 이 수를 $M$이라고 합시다. 이 수는 **중간값들의 중간값**입니다.

5.  이 $M$을 pivot으로, Partition합니다.

이 방법이 정말 빠를까요? (Additional 5 참고)

## Additional topics and Problems 

1.  (Almost-Sorted) Insertion sort의 Adaptive한 성질을 더 생각해 봅시다.
    구체적으로, $n$개의 원소를 가진 배열에서, 각 원소가 sorting된
    자리에서 $k$ 자리 이상 벗어나 있지 않음이 보장되어 있다고 가정하고,
    Insertion sort가 $O(nk)$에 정렬을 완료함을 보이세요.

2.  STL의 sort가 어떻게 구현되어 있는지 찾아보세요.

3.  (Tip) C++ sort의 comparator란, 두 원소의 크고 작음을 비교하는 기준을
    제공하면 이를 기준으로 정렬해 준다는 의미입니다. 예를 들어, 두
    String을 길이를 기준으로 정렬할 수 있습니다. 유용하니 반드시 익히길
    권합니다. Comparator는 (이산수학을 수강했다면) strict partial
    order여야 합니다.

4.  다음 조건 하에서, 흐름을 따라가며 Quick sort의 시간 복잡도가 average $O(n \log n)$ 임을 보이세요.

    -   입력은 임의의 중복 없는 수열.

    -   즉, 위 설명에서, $k$가 0부터 $n-1$까지의 임의의 수일 확률이 모두
        일정하다.

    이때, Average의 개념을 생각하면, $T(n)$ 에 대한 평균은
    $$T(n) = \frac{1}{n}\left(\sum_{k = 0}^{n-1} T(k)+T(n-k-1)\right) + \Theta(n)$$
    이 식을 어떻게 계산할지 생각해 보세요. 다양한 방법이 있습니다.\
    거의 비슷한데, Quick select 알고리즘의 평균 시간 복잡도가 $O(n)$
    임을 보이세요. 다시 말해,
    $$T(n) = \frac{1}{n} \left(\sum_{k = 0}^{n-1} \max\{T(k), T(n-k-1)\}\right) + \Theta(n)$$
    이 식을 계산해 보세요.\
    **힌트** : $T(n) \in O(f(n))$ 을 증명하는 방법으로, 수학적 귀납법을
    생각해 보세요. $n$보다 작은 $m$에 대해, $T(m) \leq cf(m)$ 인 상수
    $c$가 존재한다고 가정하고 $n$에 대해서도 성립함을 보입니다.

5.  Quick sort에서 각 과정에서 항상 좌우의 배열 크기가 1:9로
    나누어진다고 가정하고, 시간 복잡도를 계산해 보세요. 1:99, 1:999에
    대해서도 생각해 보세요.

6.  (제약의 중요성) 맨 앞 장으로 돌아가, **가정**을 보세요. 모든 수가
    -100만부터 +100만까지라는 사실이 주어져 있을때, $n$개의 수가 주어질
    때 $O(n)$에 정렬할 수 있는 방법을 제시하세요.

    힌트 : 이 방법을 **Counting Sort** 라고 부릅니다.

7.  앞서 제시한 Linear-Time Selection 알고리즘이 정말 Linear-time
    selection을 보장함을 보이세요.

8.  **(Sorting Lower Bound)** Decision tree에 대해 찾아보고, Worst
    case에 $O(n \log n)$ 보다 좋은 비교 기반의 정렬 알고리즘이 없음을 납득하세요. 이를 Sorting lower bound라고 부릅니다. (6번에서 다룬 Counting sort는 비교 기반 정렬이 아니므로 (&lt; 연산의 결과를 쓰지 않습니다) 이 내용과 상관 없습니다)

## Programming Practice


1.  STL의 도움을 전혀 받지 말고 (필요하다면 벡터 정도까지는 써도 됩니다)
    편한 방법대로 quick sort와 merge sort를 구현해 보세요. Baekjoon OJ의
    2751번에 제출해서 구현을 확인해 보세요.

2.  Quickselect를 구현해서 Baekjoon OJ의 11004번에 제출하고, 시간 초과를
    받았다면 개선해 보세요. [^7] Worst case linear selection을 구현해
    보세요.

3.  BOJ 11650, 11651을 통해 STL의 sort 함수 사용법을 익히세요.


[^1]: Small exercise) Asymptotic analysis를 이용하여, 이 가정이
    불필요함을 보이세요.

[^2]: 직접 확인해 보세요.

[^3]: Additional 의 i 를 참고하세요

[^4]: 한 줄로, 한번에 풀 수 있을 때까지 나누면 된다고 생각하기 쉽습니다.
    사실은, 충분히 빨리 풀 수 있는 방법이 있다면 굳이 그럴 필요는
    없습니다.

[^5]: Typically, 8, 16, 32, 64 정도 값을 택합니다.

[^6]: 목표는 **좋게** 가 아니라 **나쁘지 않게**. (Additional 5) 참고.

[^7]: 항상 Worst case에 대비하는 mind를 준비하세요! ㅋㅋ</content><author><name>Wonseok Shin</name><email>gratus907@snu.ac.kr</email></author><category term="ds-alg-note" /><summary type="html">Contents</summary></entry><entry><title type="html">II. 기본 자료구조</title><link href="http://localhost:4000/ds-alg-note/02-basic-ds/" rel="alternate" type="text/html" title="II. 기본 자료구조" /><published>2021-08-09T00:00:00+09:00</published><updated>2021-08-09T00:00:00+09:00</updated><id>http://localhost:4000/ds-alg-note/02-basic-ds</id><content type="html" xml:base="http://localhost:4000/ds-alg-note/02-basic-ds/">&lt;div id=&quot;toc&quot;&gt;
Contents
&lt;/div&gt;
* TOC
{:toc}
----------

## 자료 구조 : Data structures

이 장은 자료 구조에 대한 이야기입니다. 자료 구조는 알고리즘과 매우
밀접한 관련이 있는데, 어떤 식으로 자료가 저장되어 있는지에 따라
알고리즘의 소요 시간이 크게 달라지기 때문입니다. 특정한 자료구조를
이용해야만 빠르게 작동하는 알고리즘들도 수없이 많고, 알고리즘 자체에
명세처럼 이런 자료구조를 써야 한다고 말하는 경우도 많을 것입니다.

## Linked Lists

가장 기본적인 자료구조인 Linked List (연결 리스트) 는, 한 줄로 쭉 연결된
**노드** 들의 연결로 구성됩니다. 대표적은 Dynamic data structure라고 볼
수 있겠습니다. 우리가 앞서 본 바와 같이, 1개가 있을지 5개가 있을지
10만개가 있을지 모르는 상황에서 배열을 쓰려면 가능한 최대를 잡아야
합니다. 이는 매우 비효율적이기 때문에, 이러한 Dynamic data structure에
대한 고민은 매우 중요하다고 할 수 있습니다.

링크드 리스트를 구현하는 방법은 매우 간단합니다. 각 노드가 **데이터**
와, **다음 노드로 가는 포인터** 를 들고 있으면 됩니다. 노드를 **struct
node** 로 관리하면 좋겠네요. 이제\...

-   원소를 추가할 때는, 먼저 새 노드를 만든 다음, 추가하려는 위치를
    잡고, 그 앞에 있는 노드의 next pointer를 지금 추가하려는 노드로
    잡아주고, 새로 추가한 노드의 next pointer를 관리하면 간단합니다.

-   원소의 삭제는, 삭제하려는 노드 이전 노드를 찾아서 그 next pointer를
    삭제하려는 노드의 다음 노드로 보내주면 됩니다. Dynamic memory
    alloc/dealloc에 주의해 주세요.

자료구조 / 알고리즘을 공부할 때 좋은 rule of thumb 중 하나는, 뭔가를
얻었으면 뭔가를 지불해야 한다는 점입니다. 이 손해는 구현의 난이도일수도
있고, 상수가 크다는 점일 수도 있고, 다른 연산이 느릴 수도 있는데, 링크드
리스트는 이 대표적인 사례 중 하나입니다. 먼저\...

-   10만개짜리 리스트 중 5만 6천번째가 뭔지 확인하고 싶으면, head부터
    next포인터를 5만6천번 따라가야 합니다.

-   삭제할때, 이전 노드를 찾는 작업이 굉장히 귀찮습니다. '지금 보고 있는
    노드' 뿐 아니라, '그 직전 노드' 가 뭔지도 관리하면서 Loop를 돌려야
    하기 때문입니다.

후자는 구현상의 문제니 뭐 그렇다고 치더라도, 시간 복잡도로 생각해 보면
임의 위치의 삽입/삭제/접근이 모두 $O(n)$ 시간이 걸린다고 할 수 있습니다.

배열의 경우, 임의 삽입은 개수가 넘어가면 안 될수도 있고, 되더라도 삽입
삭제는 $O(n)$이 걸리는데 비해 접근은 $O(1)$에 할 수 있으므로, 동적
리스트를 얻기 위해 임의접근 속도를 포기했다고 보면 되겠습니다.

변형된 형태의 동적 리스트로, 다음 노드 뿐 아니라 이전 노드로 가는
포인터도 관리하는 double linked list, 구현상의 편의를 위해 Head와 Tail을
이어 버리는 circular linked list 등등이 있습니다.

## Stacks / Queues / Deques

스택은 Last-in First-out (LIFO) 형태의 자료구조입니다. 즉, 다음 두
연산을 지원하는 자료구조를 말합니다.

-   Element를 맨 위에 추가 (push)

-   맨 위에 쌓은 element를 빠르게 확인하거나 (peek), 가져오기 (pop)

여기서, **할 수 있다** 는 말은 사실 **빠르게 할 수 있다** 는 말입니다.
당연히 스택에서 맨 밑 원소를 보는것도 그 위에 $n$개의 원소를 다 뽑아내면
할 수는 있습니다. 다만 위 두 연산을 $O(1)$에 할 수 있음이 중요합니다.

Queue는 First-in First-out (FIFO) 형태의 자료구조입니다. Stack과는 달리,
Element를 위에서 넣고 뺄 때는 아래에서만 뺀다고 생각하면 됩니다.

Deque는 Queue와 Stack의 기능을 합쳐, Front/Rear access와 insert가 모두
$O(1)$에 작동하는 자료구조입니다.

이 세 가지에 대한 자세한 설명은 워낙 좋은 내용들이 많고, 어려운 내용도 아니라서 패스하겠습니다. 

## Implementation

세 자료구조 모두 개념적으로는 Doubly Linked List로 구현하면 됩니다.
구현의 용이성을 위해 Dynamic함을 포기하고 1차원 배열에다가 배열의 양
끝을 표시하는 index variable 2개를 쓰는 구현도 많이 사용하는데, 이
경우에는 메모리에만 들어가면 원하는 만큼 많이 넣을 수 있다는 기본적인
가정을 위배하는데 비해 시간복잡도상의 이득이 있지는 않으므로 우리의
관심사는 아닙니다.[^1]

## Standard Library

STL에는 stack, queue, deque가 다 있습니다. `deque&lt;int&gt; dq;` 와 같이
선언하고 쓸 수 있습니다.

-   `{push/pop}_{back/front}()` 의 4개 함수를 제공합니다. 당연히
    stack이나 queue는 제약이 있습니다.

-   `top(), front(), back()` 등 peek-형 함수들이 있습니다.

-   미세하게 문법이 다른데, 쓰면서 익히면 됩니다. 특히, stack이나
    queue는 어차피 넣고빼는 위치가 정해져 있으므로 그냥 push/pop으로
    사용하도록 되어 있습니다. 위 설명은 deque가 기준입니다.

-   vector는 스택이 할 수 있는 모든 것을 동일한 시간 복잡도에 할 수
    있으므로, stack임을 명시하고 싶은 상황이 아니라면 vector를 써도
    됩니다.

------

[^1]: 다만\... 링크드 리스트는 실제 퍼포먼스가 매우 느립니다. 링크드
    리스트의 $O(1)$ 과 배열의 $O(1)$은 좀 다르긴 합니다.</content><author><name>Wonseok Shin</name><email>gratus907@snu.ac.kr</email></author><category term="ds-alg-note" /><summary type="html">Contents</summary></entry><entry><title type="html">I. 시간 복잡도와 Big-O Notation</title><link href="http://localhost:4000/ds-alg-note/01-time-complexity/" rel="alternate" type="text/html" title="I. 시간 복잡도와 Big-O Notation" /><published>2021-08-09T00:00:00+09:00</published><updated>2021-08-09T00:00:00+09:00</updated><id>http://localhost:4000/ds-alg-note/01-time-complexity</id><content type="html" xml:base="http://localhost:4000/ds-alg-note/01-time-complexity/">&lt;div id=&quot;toc&quot;&gt;
Contents
&lt;/div&gt;
* TOC
{:toc}
----------

## Time Complexity Analysis

알고리즘 (Algorithm) 과 자료구조 (Data Structure) 를 공부하는 우리의
목표는, 같은 문제를 효율적으로 해결하는 것입니다. 뭔가를 효율적으로 하기
위해서는 항상 효율을 측정하는 기준이 필요할 것입니다.

일반적으로 이 기준에는 다음 두 가지가 가장 중요합니다.

-   프로그램이 얼마나 오래 걸리는가

-   프로그램이 얼마나 많은 자원 (대표적으로 메모리 등) 을 요구하는가

당연하게도, 이 부분은 입력이 무엇인지에 따라 달라집니다. 대표적으로 원소
10개짜리 배열을 정렬하는 것과 100만개짜리 배열을 정렬하는 것의 소요
시간과 소모 메모리는 다를 수밖에 없기 때문입니다. 우리는 이를 위해,
일반적으로 **입력이 증가함에 따라, 소요 시간과 메모리가 어떻게
변화하는지** 를 알고자 합니다.

특히, 현대의 컴퓨팅 환경에서 중요한 것은 시간입니다. (메모리가 중요하지
않다는 뜻은 아닙니다) 따라서 우리는 알고리즘의 시간 복잡도를 가장
중요하게 볼 것입니다. 즉, 입력의 크기 $n$에 대해, 소요 시간 $T(n)$ 이
얼마나 빨리 증가하는가? 라는 질문을 알고리즘의 효율성으로 이해하고자
합니다. 그런데, 입력의 크기가 $n$으로 같다고 해서 정말 소요 시간이
같을까요?

정렬하는 문제를 생각해 봅시다. 정렬하는 문제에서, 원소 10개짜리 배열을
정렬하는데\...

-   1, 2, 3, 4, \... 로 이미 정렬된 배열이 들어오면, 아무것도 할 필요가
    없습니다.

-   어쩌면 거꾸로 정렬된 배열이 들어와서 다 엎어야 할수도 있습니다.

-   그렇다면 크기가 $n$인 모든 입력에 대해 평균을 내는것이 합당할까요?

즉, 크기가 $n$인 모든 입력의 집합 $S_n$에 대해, 다음 세 가지는 모두
중요할 수 있습니다.
$$\min_{s \in S_n} T(s) \quad\quad\quad \max_{s \in S_n}T(s) \quad\quad\quad \frac{\sum_{s \in S_n}T(s)}{\abs{S_n}}$$
이를 우리는, **Best / Worst / Average** 시간복잡도라고 부릅니다.
알고리즘에 관한 연구는 수학적으로 엄밀한 것을 목표로 하기 때문에,
일반적으로는 Worst case를 가지고 다루고자 하는 tendency가 있습니다.
그러나 현실에서는 Average case를 잘 푸는 알고리즘도 매우 중요하기
떄문에, 필요하다면 이를 명시하고 사용할 것입니다. Unless otherwise
stated, 앞으로 나오는 복잡도는 worst case입니다. 즉, $T(n)$이란, 입력이
$n$인 입력들 중, worst case에 필요한 시간을 의미할 것입니다. 우리는
문제를 단순화하기 위해 $T(n)$을 $\N \to \N$함수로 받아들이겠습니다.

## Asymptotic Notation

중요한 사실 하나는, 정확한 $T(n)$은 사실 그렇게 중요하지 않다는
부분입니다. **컴퓨터 구조** 를 배우면 알수 있는데, 두 실수의 곱셈은 두
정수의 덧셈보다 수 배 이상 느립니다. 즉 정수덧셈 100만번이 실수곱셈
20만번보다 빠를지도 모른다는 의미입니다. 수학적으로는 보통 FLOP count 라
하여, 실수 연산 몇번으로 bound 되는지를 가지고 관찰하는데, 정수연산만
쓰는 알고리즘을 개발하였다면 이는 조금 불합리하게 느껴질 수 있습니다.

그러나 $n$이 커질 때, $n^2$, $n^2 \log n$, $n^3$ 등은 상당히 큰 차이를
불러일으킵니다. 따라서, 우리는 다음과 같은 Big-O notation을 정의할
것입니다. Big-O notation $O(g)$란, $g : \N \to \N$ 에 대하여,
$$f \in O(g) \iff ^\exists N, C \in \N \st ^\forall n \geq N, f(n) \leq Cg(n)$$
이를 잘 생각해 보면 다음과 동치임을 알 수 있습니다.
$$f \in O(g) \iff \limsup_{n \to \infty} \frac{f(n)}{g(n)} &lt; \infty$$
대충, 의미를 받아들일 때는 '충분히 큰 $n$에 대해, $f$를 $g$의 상수 배로
바운드를 잡을 수 있다' 라고 생각하시면 됩니다. 예를 들어 $3n^2 + 4n + 6$
은 $O(n^2)$ 다 라고 말할 수 있는 것입니다.

Big-Omega notation이라는 것이 있습니다. Big-Omega는 반대로, 알고리즘의
하한에 대한 논의입니다. [^1][^2]
$$f \in \Omega(g) \iff ^\exists N, C \in \N \st ^\forall n \geq N, f(n) \geq Cg(n) \iff \liminf_{n \to \infty} \frac{f(n)}{g(n)} &gt; 0$$

간혹 Little-o 와 Little-omeaga도 씁니다.
$$f \in o(g) \iff \lim_{n \to \infty} \frac{f(n)}{g(n)} &lt; \infty$$

중요한 notation으로, $f \in O(g)$ 이고 $f \in \Omega(g)$ (또는
$g \in O(f)$) 이면, $f \in \Theta(g)$ 이고 $g \in \Theta(f)$ 라고
씁니다. 대략 시간 복잡도의 관점에서 두 함수는 사실상 같게 취급된다는
의미입니다.

예를 들어, 위에서 본 $3n^2 + 4n + 6$는 $O(n^2)$ 이고 $O(n^3)$ 이지만,
$\Theta(n^3)$은 아닙니다. 하지만 $\Theta(n^2)$ 가 됨은 알 수 있습니다.

우리는 편의상 알고리즘을 공부하면서, 1억 = 1초라는 Rule of Thumb을
이용합니다. 즉, $O(n^2)$ 알고리즘이면, 대략 $n = 10000$ 까지는 1초에
작동할 것이라고 믿겠다는 말입니다. 이 규칙이 몇년째 바뀌지 않았다고
들은것 같은데, 그동안 컴퓨터는 빠르게 발전했기 때문에 사실 지금은
1억보다는 더 많이 돌아가기는 합니다만, 우리가 $O(n^2)$이라고 말하더라도
실제로는 $3n^2 + 6n$ 같은 것일 경우가 많으므로 Rule of Thumb으로는
유효하다고 생각합니다.

시간 복잡도는 매우 중요하고 항상 생각해야 하지만, 시간 복잡도가 모든
것을 좌우하지 않음도 꼭 기억해야 합니다. 컴퓨터 구조 같은 수업에서 왜
그런지, 어떻게 그런지 공부하게 됩니다. 어떤 연산은 복잡도에 비해
빠르거나 느립니다.

**주의** 왜인지 정확히는 모르겠지만 Big-Theta를
의미하면서도 말은 Big-O로 말하는 이상한 Tradition이 있습니다. 저도 그
영향을 받았기 때문에 그렇게 쓰는 일이 많이 있습니다. (추측이 좀
섞여있지만) 사실 중요한 것은 알고리즘이 얼마나 빠른지이므로, Big-O가
가장 중요해서일 것입니다. 예로 $O(n^2)$ 알고리즘을 개발했다면, 이게
실제로 $\Theta(n \log^4 n)$ 인지 tight하게 prove하는 것보다는 '아무튼
$n^2$ 이상의 퍼포먼스를 보여준다' 는 점이 중요하기 때문이 아닌가 합니다.
$\Theta$ 를 최대한 사용하려고 노력하겠지만, 일부 그렇지 못한 경우가 있을
수 있습니다.

**주의2** $f \in O(g)$ 를 의미하면서 무려 $f = O(g)$ 같은 Abuse of notation도 꽤 흔한 듯 합니다.

## Additional topics and Problems

1.  Amortized Analysis가 무엇인지 찾아보고, 어떨때 유용할지 생각해
    보세요.

2.  ($\bigstar$) 알고리즘 수업에서 배우는, Master Theorem이라는 정리가
    있습니다.[^3] 몇가지 알려진 재귀식을 빠르게 해결할 수 있는 방법으로,
    알아두면 알고리즘 분석에 많은 도움이 됩니다. 정리를 기술하면 다음과
    같습니다.

    ------
    어떤 알고리즘이 $n$ 크기의 입력을 받았을 때, $n/b$ 크기의 입력
    $a$개로 문제를 쪼개어 해결한 후, 이를 $f(n)$ 시간에 합친다고 하자.
    즉, 시간 복잡도가 다음과 같다고 하자. $$T(n) = aT(n/b) + f(n)$$
    이때, $c = \log_b a$ 라고 하자. 다음과 같은 경우, 이 점화식의 다음과
    같은 해가 알려져 있다.

    1.  $f(n) \in O(n^d),\ d &lt; c$ : $T(n) \in \Theta(n^c)$

    2.  $f(n) \in \Theta(n^c \log^k n)$ : Depends on value of $k$.

        -   $k &gt; -1$ : $T(n) \in \Theta(n^c \log^{k+1} n)$

        -   $k = -1$ : $T(n) \in \Theta(n^c \log \log n)$

        -   $k &lt; -1$ : $T(n) \in \Theta(n^c)$

    3.  $f(n) \in \Omega(n^d), d &gt; c$ 이고,
        $^\exists k &lt; 1 \st af(n/b)\geq kf(n)$ for large enough $n$ :
        $T(n) \in \Theta(f(n))$

    ------

    증명은 알고리즘 수업에서도 (아마도) 다루지 않겠지만, 다음과 같은
    사실을 이용합니다. 수학에 관심이 많고 이런걸 보고 증명이 없으면 못
    넘어가는 성격이라면 시도해 보세요.[^4]\
    점화식을 Recursion Tree 형태로 그리면 다음과 같은 사실을 쉽게 알 수
    있습니다.
    $$T(n) = \sum_{i = 0}^{\log_b a} a^i f(n/b^i) + O(n^{\log_b a})$$
    이제, 각 Case별로 조건을 나누고 침착하게 Bound를 잡으면 됩니다.
    힌트를 조금 드리자면, 1번과 같은 조건을 해석하는 좋은 방법은
    $f(n) \in O(n^{c - \epsilon})$ 이라고 생각하는 것입니다.

------

[^1]: Knuth의 저서에서는 $f \in \Omega(g) \iff g \in O(f)$ 로
    정의했는데, 이 정의는 원래의 정의와 조금 다르지만 CS에서 사용하는
    함수들에 대해서는 거의 같으며, 훨씬 이해가 편하므로 이쪽을 쓰기도
    합니다. 저희도 대충 이걸로 생각할 계획입니다.

[^2]: 정수론에서 사용하는 Hardy-Litttlewood Omega와 **아주 조금**
    다릅니다!

[^3]: \[CLRS\] 4.3

[^4]: 이 증명을 알아야 한다는 뜻은 아닙니다

[^5]: David Harvey, Joris van der Hoeven, 2019, Integer Multiplication
    in time $O(n \log n)$

[^6]: 다만\... 링크드 리스트는 실제 퍼포먼스가 매우 느립니다. 링크드
    리스트의 $O(1)$ 과 배열의 $O(1)$은 좀 다르긴 합니다.</content><author><name>Wonseok Shin</name><email>gratus907@snu.ac.kr</email></author><category term="ds-alg-note" /><summary type="html">Contents</summary></entry><entry><title type="html">삼성 SCPC 예선 2라운드 풀이 / 후기</title><link href="http://localhost:4000/cp-rounds/SCPC-2021-Round2/" rel="alternate" type="text/html" title="삼성 SCPC 예선 2라운드 풀이 / 후기" /><published>2021-08-07T00:00:00+09:00</published><updated>2021-08-07T00:00:00+09:00</updated><id>http://localhost:4000/cp-rounds/SCPC-2021-Round2</id><content type="html" xml:base="http://localhost:4000/cp-rounds/SCPC-2021-Round2/">&lt;div id=&quot;toc&quot;&gt;
Contents
&lt;/div&gt;
* TOC
{:toc}
----------

Round 1 후기는 [여기에](/cp_rounds/SCPC-2021-Round1/), 작년 Round 2 후기는 [티스토리](https://gratus-blog.tistory.com/121) 에 있습니다.

작년에 이어, 정말 말도 안되게 고통받았습니다. 제가 참가해본 CP 대회를 통틀어 가장 힘들고 지치는 대회가 아닌가 싶습니다. 

구현 코드는 항상 그렇듯 [Github Repo](https://github.com/gratus907/Gratus_PS/tree/master/Contests/Others/%5BSamsung%5D%20SCPC/2021/) 에 있습니다. 

## Intro / Preperation
작년 (2020년 9월 초) 대회와 비교하여 제 PS 실력에서는 별로 다를게 없습니다. 이제는 CP보다는 이론적인 공부에 집중하려는 생각에서 그렇기도 하고, 학교 공부가 빡세서 그렇기도 했습니다.

그렇지만 체감상, Codejam 때도 그렇고 그냥 최근 코포를 몇번 쳐보면서 느낀건 작년 이맘때는 코포기준 거품 2100이었다면 지금은 그냥 2100은 돌아갈 수 있을 것 같습니다. 

듀얼 모니터와 우분투 데스크탑 환경, 12시간 대회임을 고려해서 무제한 커피와 박카스 등을 세팅했습니다. 

결과 면에서는, 본선 진출은 딱 컷 사이에 있는 듯 합니다. 결과 사진은 여기에 붙이고, 마지막에 조금 적어보겠습니다.
![picture 1](../../images/88d3b82d0de7aafc9410beeb7ec9d8d2de4af5f0725f30757cd5e89827ed3e47.png)  

문제 설명은 대회가 끝나고 아직 문제가 공개되지 않았으므로 간단하게는 적겠습니다. 본선이 끝나면 Practice에 문제가 공개되는 것으로 알고 있습니다. 

## Problem 1. 원 안의 점 (09:09 AC)
- 반지름이 $R$이고 중심이 원점인 원 안에 들어있는 정수 격자점의 개수를 세는 문제.
- 정말 이 문제가 맞다고? 라는 생각이 들었습니다. 
- $x$좌표를 1부터 $R$까지 돌면서, 격자점의 개수를 세 주면 됩니다. $\sqrt{R^2 - x^2}$ 을 이용하되, 딱 경계에 있는 점들을 빼 주어야 함을 유의하면 어렵지 않습니다.

## Problem 2. 직8각형 (09:37 AC)
- 직8각형이란, 한 변의 길이가 $K$인 정사각형을 십자로 5개 붙였을 때, 그 Convex Hull을 이루는 8각형으로 정의합니다.
- 주어진 점 $(x_i, y_i)$ 8개를 조금씩 움직여서, 정수 좌표의 직8각형을 만들고자 합니다. 
- 최소 이동 거리를 구하는 문제입니다. (이동 거리는 $l_0$ 거리로 잽니다)
- 먼저, $l_0$ 거리를 쓰면 $x$ 방향과 $y$ 방향이 독립적임을 관찰합니다.
- $x$ 좌표를 Minimize 하기 위해, 먼저 점 8개가 각각 직8각형의 어느 점으로 움직일지를 정합니다. 직8각형의 왼쪽 아래 점을 $(u, v)$로 하고, 이를 움직이면서 이동거리를 최소화한다고 생각하겠습니다.
- 이제, 어떤 점 $u$를 찍으면, 주어진 8개의 점이 어디로 가야 하는지 정확히 알고 있습니다.
- 구체적으로 $\abs{u - x_1} + \abs{u - (x_1 + K)} + \dots$ 와 같은 형태의, 절댓값 8개로 구성된 식이 나옵니다. 이미 8개의 점의 permutation을 고정했으므로, $u$를 잘 정해서 절댓값 함수 $\abs{u - t_i}$ 의 합을 최소화하는 문제입니다.
- 여러 절댓값들의 합의 최소는 중간값임이 잘 알려져 있습니다.
- 이제, 각 Permutation에 대해 $8 \log 8$ (8개의 수 정렬) 에 옵티멀한 $x$좌표를 구할 수 있습니다. $y$도 마찬가지이므로, $8! * (2 * 8 \log 8)$ 시간에 안정적으로 한 케이스씩 해결해낼 수 있습니다. 
- 여담 : 처음에는 8각형 기하인줄 알고 기겁했습니다. 8각형을 잘 그리는 기하였다면 포기하고 자러 갔을지도 모르겠습니다.

------ 

3번 문제에 대해 쓸 말이 많아서 4번을 먼저 적습니다. 

------
## Problem 4. 패턴 매칭 (10:56 209/400점)
- 두 문자열이 '매칭' 된다는 것을, 다음과 같이 정의하겠습니다.
  - 임의의 인덱스 $i, j$에 대해, $a_i = a_j$ if and only if $b_i = b_j$
- 이제, $N = 2e6$ 크기의 문자열 $S$에서, 수백 정도 크기의 문자열 패턴 수십 개가 주어집니다. 이 패턴이 각각 몇 번 등장하는지를 세는 문제입니다. 
- 각 문자마다 '나와 같은 문자를 최근에 내 직전에 본게 언제인지' 를 기억합니다. 
- 패턴의 길이가 $L$일 때, 위 '최근에 본 위치' 를 매칭하면 됩니다. 주의할 점은 $L$보다 멀리 떨어져 있다면 본 적 없는 것과 같게 보아야 한다는 점 정도입니다. 
- 이 풀이는 복잡도상 서브태스크 1을 풀기 위해 코딩했지만, 서브태스크 2가 왜인지 모르게 뚫려버려서 이 문제를 그대로 버렸습니다.
- 이후에는 생각을 조금 했습니다. 
  - 결국 이 문제는 유사-KMP 아닌가? (맞습니다) 그걸로 마지막 서브태스크는 못 뚫을텐데? (복잡도상 못 뚫습니다)
  - KMP를 여러번 하는데 빨리 해야 한다 -&gt; Aho-Corasick? 근데 그걸 어떻게 쓰지? (아호-코라식은 맞습니다. 잠깐 스쳐간 생각이고 유의미한 접근은 하지 못했습니다)
  - (마지막 생각) 와! 결국은 Union-Find인데 
    - 패턴 길이만큼을 윈도우로 훑으면서 지울 수 있어야 하니까 링크컷 트리고, 
    - 포레스트 간의 isomorphism을 빠르게 판정해야 하니까, 포레스트가 실시간으로 변하면서 isomorphic한 집합관계를 해싱으로..?
  - 이런 해괴한 생각을 하기 시작했으면 그 문제는 풀 수 없다는 것은 만고불변의 진리이므로, 접었습니다. 


## Problem 3. 산탄총 (16:54 154/300점)
작년에도 3번 문제가 지옥이었는데, 이번 SCPC 2차도 3번 문제가 제 멘탈을 갈아버렸습니다. 이 글을 보기 전 작년 제 후기를 읽으면 제 마음이 조금 더 이해 가실 텐데, 작년에는 오후 2-4시 사이에 분노와 절망으로 5번 정도 제출 기회를 날려먹었기 때문에, 이번에는 정말 침착하고자 노력했습니다.
- 문제 설명을 먼저 하자면, $N = 600$ 인 $N * N$ 보드에 $-1e9 \leq A_{ij} \leq 1e9$ 점수판이 주어집니다. 
- 어떤 점에 산탄총을 쏘면, 그 점이 $(r, c)$ 일 때, $A_{rc} * K$ 점을 받고, 산탄총이기 때문에 그 주변 보드에 따라서도 점수가 주어집니다.
- 구체적으로, $(r, c)$ 탄착점과 택시거리로 $L$만큼 떨어진 점 $(i, j)$ 에 대해서는 $A_{ij}$에 $K-L$배를 곱한 점수가 주어집니다. $L &gt; K$면 탄알이 스치지 않기 때문에, 0점으로 봅니다.
- 만약 전체가 다 음수 점수로 가득차 있다면 허공에 쏠 수도 있고, 아무튼 보드 바깥의 점도 조준할 수 있습니다. 만약 $A_{11}$ 만 양수고 다 절댓값이 큰 음수라면, $(2-K, 1)$ 같은 점을 조준해서 탄착군 끝에 살짝 걸리게 쏠 수도 있다는 말이 됩니다. 

- 이 문제를 해결하는 브루트포스 전략의 복잡도는 $O(N^4)$ 이고, 47점을 받을 수 있습니다.
- 한 열/행을 prefix sum을 응용해서 빠르게 계산하면, $O(N^3)$ 에 풀 수 있고, 154점을 받을 수 있다고 합니다.

이게 풀이의 전부라면 3번과 4번의 순서를 바꿔 적지 않았을 것입니다. 저는 먼저, $O(N^4)$를 10시쯤에 구현한 후, $O(N^3)$ 으로는 도저히 마지막 테스트케이스를 뚫을 자신이 없었기 때문에 한참 생각해서 이런 풀이를 구상했습니다.
- 점수판을 확장해서, 가능한 모든 탄착지까지 ($K$ 거리 까지) 확장합니다. $K \leq N$이므로, $3N \times 3N$ 크기 보드가 됩니다.
- 맨해튼 거리의 특징상 기울어진 마름모꼴의 탄착군이 형성되는데, 이는 연산이 불편하므로 꼭짓점들에 0을 채워넣어 $2K - 1 \times 2K - 1$ 의 정사각형 탄착군을 만듭니다. 
- 이제, 특정 $(r, c)$ 를 조준하는 행동은, Convolution 연산이 됩니다!
- Convolution을 빠르게 계산하는 방법으로, 행렬 전체를 한 줄로 펴고, Filter (여기서는 탄착군) 을 뒤집은 다음 FFT를 적용하는 방법이 있습니다. 그림을 보면 이해가 쉽습니다. 
![picture 2](../../images/e9adc7628fb1b04ad36a052eff4022640de1a23bc75854f9d097a819e3dc327b.png)  
(그림 크기 마크다운에서 조절하는 방법 있으면 부탁드립니다 ㅜㅜ )

아무튼, 이 그림에서, 보라색이 필터입니다 (검은색 0, 1, 2가 탄착에 따른 점수). 원래는 3 * 3 이어야 하지만, Convolution을 위해서는 오른쪽과 행 길이가 같게 맞춰야 하고, 뒤집어야 합니다.

이제, 자주색과 보라색을 컨볼루션한 결과 다항식에서, `A[i + 6]` 은, A의 $i$번 위치에 6번을 갖다 대고 점수를 산출했을 때의 점수가 됩니다.

- 정확하게 몇차 다항식을 곱셈하는지 생각해 봅시다. 
  - 먼저 보드를 확장할 때 $R = (N + 2 (K - 1))$에 대해 $R * R$ 칸으로 확장해야 하는데, $K = N$ 이 최대이므로 최대 $9N^2$ 차 다항식이 됩니다.
  - 필터는 $(2K-1)$ 크기의 정사각형이지만, 보라색 인덱스에서 보듯 열개수를 맞춰야 하므로 $R * (2K - 1)$, 즉 $6N^2$ 차 다항식이 됩니다.
  - $N = 600$ 임을 고려하면, 300만차 다항식과 200만차 다항식으로 생각할 수 있습니다. 빠르게 곱할 수 있을까요?
- 저는 여기서, FFT를 믿기로 했습니다. FFT를 쓰기로 한 이후로 6번정도 코드를 고쳤는데, 다음과 같은 과정을 거쳤습니다. 
- 아래는 FFT로 고통받은 과정입니다. 이 문제의 정해는 FFT가 아닙니다만, FFT 문제에서 생각할 수 있는 거의 모든 것을 해본듯 합니다. 

1. 가장 먼저 깨달은 문제점은 속도가 아니라 정밀도입니다. 
    - 이 문제는 각 항이 10억까지고, 거기에 수백정도 되는 수를 수만개 곱해서 두들겨 더해야 하기 때문에, 거의 long long int 끝자락 스케일의 답을 갖습니다. 일반적인 FFT 구현체는 이 정밀도를 감당할 수 없습니다. 
    - FFT의 정밀도를 높이는 방법으로, $A(x)$ 를 $C = \sqrt{M}$ ($M$은 계수의 최댓값) 으로 잘라 $A(x) = P(x) + C Q(x)$ 로 만들어 준 다음, 각 $P$와 $Q$의 계수를 $C$ 이하로 강제하고 다항식곱셈을 4번 하는 방법이 있습니다. 이경우는 데이터와 필터로 나누어 볼 때 데이터에는 10억까지의 수가 쓰여 있지만 필터에는 최대 600이므로 데이터만 자르면 됩니다. 이를 편의상 '2조각 트릭' 이라고 부르겠습니다.
    - 참고로, 왜인지는 잘 모르겠지만 이 문제는 2조각 트릭으로도 해결하지 못했습니다. 제가 최댓값 케이스를 로컬에서 테스트해본 결과 아주 약간 답이 틀리던데, 오차를 잘 줄이면 2조각으로 가능해야 맞습니다.
    - 대표적인 예시는, FFT에서 $w^n$ 을 계산할 때, $w$를 계속 곱하면 수치오차가 계속 누적되지만, 매번 $w^n$을 삼각함수로 직접 계산하면 오차가 적습니다. 다만 이방법이 훨씬 느립니다.
    - 저는 3조각을 썼습니다. 계수가 10억이므로 $C = 1000$을 기준으로 3조각으로 나누고 합칠때 1000, 100만을 곱해서 합치면 됩니다.
    - 3조각을 쓴 두가지 이유는, 첫째로 $w^n$을 직접 구하면 그 자체가 끔찍하게 느리고, 두번째로 어차피 아래 헬조선 FFT를 쓰게 되면 이런것을 하지 못해서입니다.
2. 이렇게 했더니, 최대 케이스 하나에 10초 정도가 걸렸습니다. 여러가지 방법을 시도했습니다. 
    - 가장 먼저 시도한 것은, Koosaga님의 그 유명한 `HELL-JOSEON-FFT` 입니다. 이 구현체는 [Koosaga팀노트](https://github.com/koosaga/DeobureoMinkyuParty) 에서 볼 수 있으며, AVX 명령어를 이용해서 놀라운 성능을 자랑합니다. 다만 저는 AVX 내부 구현을 잘 모르기 때문에, AVX를 쓰면 2조각으로 줄일 수 있을지 어떨지 자신이 없습니다. 
    - 오후 4시까지 수많은 케이스들을 돌려보면서 고전하다가 Hell-Joseon FFT with 3-pieces로 154점을 받았습니다. 
3. 사실 느린것은 실수연산이 개많아서가 아닐까? 라고 생각하고, NTT를 사용했습니다. NTT는 체로 $\C$ 대신 $\Z_p$를 쓰는 FFT인데, 이론은 수많은 좋은 책과 자료가 있으므로 넘어가고, 정수만으로 FFT를 할 수 있습니다. 다만 이 문제는 NTT를 바로 쓸 수 없는데, 크게 두 가지 이유가 있습니다. 
    - NTT는 본질적으로 양수에만 적용할 수 있습니다. 이를 해결하기 위해, 보드를 양수-부분과 음수-부분으로 나누어, 2배로 늘렸습니다. 양수 convolution과 음수 convolution을 따로 하고, 양수부분 - 음수부분으로 답을 계산하면 됩니다.
    - 이 문제는 long long 끝자락의 수까지 커버해야 하기 때문에, 숫자가 너무 큽니다. 큰 숫자를 처리하는 방법은 위 1번에서 설명한 대로 2조각으로 쪼개는 방법이 있고, 양수 음수를 각각 쪼개야 해서 총 4조각으로 쪼개야 합니다. 
    - 또다른 방법은, 소수 2개를 써서 $\Z_{p_1}, \Z_{p_2}$에서 각각 NTT한 다음, 중국인의 나머지 정리를 이용하여 복원하는 방법입니다. 이 방법을 쓰기 위해서는 큰 소수 2개가 필요하며, 두 소수의 원시근을 모두 알아야 합니다. 몇가지 소수가 알려져 있으므로 가져다 쓰면 됩니다. 그 유명한 `998,244,353` 과 구글링하다보면 알수있는 `2,281,701,377` 가 있습니다. 두 소수의 곱이 long long 범위를 넘어가지만 뭐... 그부분만 잠깐 `__int128`을 쓰면 적어도 그 부분은 그렇게 느리지 않습니다.
    - 두 가지 방법을 모두 구현해 보고, 벤치마크해 본 다음, 깨닫게 된 것은
      - 정수만 쓰다보니 필연적으로 다항식 곱셈을 무려 4번 해야 하며
      - 실수 연산보다는 덜하지만 모듈러도 개느리기 때문에 (특히 모듈러 인버스는 어쩔수없이 실수의 나눗셈보다 느리니까요) 이 방법도 느리다는 것입니다. 
      - 구체적으로, 일반 3조각 FFT &lt; 일반 2조각 FFT ~ NTT with 2 prime and CRT &lt; 2조각 NTT &lt;&lt; 3조각 Hell-Joseon FFT였습니다. 헬조선 FFT를 이길 수는 없었습니다.
      - 다만 벤치마크는 완벽히 똑같은 환경이 아니라서 (4번에서 왜인지 나옵니다) 믿으면 안 됩니다. 그러나 일반인이 짠 NTT로 PS 최강자가 한계까지 최적화한 헬조선 FFT에 비빌수 없음은 앞으로도 가져갈 교훈인듯 합니다.
4. 남은 최적화는, 어떻게든 AVX 내부구현을 이해해서 2조각 헬조선 FFT를 해내는 방법이 있습니다. 꽤 오랜 시간을 부었지만 처참하게 패배했습니다. 거의 똑같이생긴 `_mm_.....` 함수가 너무 많았습니다. 우웩.
5. 헬조선 3조각 FFT에 -O2를 붙이면 최대 케이스 하나를 3초 이내에 쳐낼 수 있었으므로, 자잘한 최적화를 해보기로 했습니다.
    - 먼저, `vector&lt;int64_t&gt;` 로 구현된 부분들을 최대한 `vector&lt;int&gt;` 만으로 처리할 수 있는지 해봤습니다. 메모리를 덜 쓰는 것 자체는 메모리 제한이 걸리는 상황이 아니어서 걱정하지 않았지만, 메모리를 절반으로 줄이는 과정에서 캐시히트가 조금  나아지기 때문에 일반적으로 조금 나은 성능을 가져옵니다. 최대케이스 기준으로 상당한 차이가 있었던것 같은데 정확한 기억은 없습니다.
    - 다음으로, `vector&lt;int&gt;` 를 생성하고 복사하는 시간을 아끼기 위해, 최대한 모든걸 `int a[4444444]` 같은 큰 배열로 만들어 놓고 in-place로 수정했습니다.
    - 이제, resize하는 시간 등을 아낄 수 있습니다. `fill`, `memset` 등을 사방에 후려쳐서 초기화하면 됩니다. 

이 모든 방법을 적용하면서, 추가로 이런 고민도 했었습니다. 실현에 옮기지는 못했습니다.
- 생각해 보면, 전체 칸들 중 데이터는 $8/9$ 정도가, 필터는 최대 $1/3$ 정도가 0입니다. 이를 미리 알고 있다면, 값의 sparse함을 이용하여 뭔가 해볼 수 있지 않을까 하는 생각에 꽤 오래 빠져 있었습니다. 특히 하시설에서 CNN을 Zero-skipping 최적화하던 기억이 오버랩됐었는데, 그때와는 달리 지금은 FFT를 쓰고 있기 때문에 0을 압축할 방법이 마땅치 않습니다. 차라리 1/1000 같이 엄청 sparse 하다면 배열 대신 `unordered_map` 으로 다항식을 관리하는 생각을 해봤을텐데, 저정도 0을 줄이기 위해 맵같은걸 쓸수는 없습니다. 

아무튼... 이렇게 많은 생각을 해봤지만 마지막에 최종적으로는 로컬에서 최대케이스 (300만차 * 200만차 다항식 곱셈) 에 2초 근처로 소요되었고, 이를 8시 30분경에 다시 제출했지만 점수가 오르지 않았습니다. 결과적으로 작년 3번과 똑같은 참사가 벌어져 굉장히 힘들었습니다.

참고로, 정해는 마름모꼴을 한칸 살짝 밀었을 때 바뀌는 델타값이 마름모꼴 반쪽만큼이 빠지고 다른 반쪽이 들어가는 것과 같음을 관찰하여, 이를 모두 전처리하고 조준점을 한칸 옮길때마다 $O(1)$에 업데이트하는 풀이라고 합니다. 대회 끝나고 `dhdroid`가 알려줬습니다. 제 FFT 풀이를 오픈채팅방에서 끝나고 말하면서 이게 안 뚫리냐고 분노를 표출했는데, 검수에 참여했던 한 분이 &quot;아니 FFT라니 뭔 소리냐&quot;, &quot;검토하면서 한번도 나온적 없는 풀이다&quot; 라는 말을 들었습니다. 

대략 8시간 정도 FFT를 최적화했습니다.

------
## Problem 5. Hanoi Tower (20:55 2/500점)
- 하노이 탑 문제인데, 맨 위 대신 타워의 중간값을 뽑아서 다른 타워의 중간에 집어넣는 연산을 할 수 있습니다. 
- 굉장히 특이한 형태로 채점하는데, 최대 98만번 정도의 연산을 할 수 있고, $N$ 몇을 풀 것인지 내가 정한 다음 그 답을 제출하면 $N$에 따라 점수를 줍니다.
- $N = 26$ 을 풀면 500점인데, $N = 25$ 가 겨우 250점이고, 그 아래는 1씩 줄어들때마다 0.75배씩 줄어드는 가혹한 점수 체계를 가지고 있습니다.
- FFT 가지고 이상한짓 하다가 중간에 잠깐 읽고 몇개 손으로 해봤는데, 저는 이런 Construction형 문제에 굉장히 약해서... $N = 6$ 도 잘 해결하지 못했습니다.


- 이 문제는 풀이와는 별개로, 제 굉장한 추함이 드러나는 문제입니다. 대략 8시 15분? 20분? 쯤, 3번 문제의 만점자가 120명 정도였던 것으로 기억합니다. 이 대회는 본선 진출자가 128명 근처로 알려져 있고, 저는 이때 3번 만점을 받지 못할 것으로 판단했기 때문에, 전략을 고민할 때가 왔습니다. 4번 문제는 만점자가 30명 정도였으므로 고려할 필요가 없고, 4번을 만점받은 모든 사람이 3번을 만점받았다고 가정하면 제 위로 3번 만점자들 대부분이 위치할 것입니다. 
- 낮에 잠깐 30분정도 생각을 하면서, 손으로 하노이탑을 움직여보기 힘들었으므로 파이썬으로 시뮬레이션 코드를 짰습니다. 
- 무지성 백트래킹은 굉장히 느려서 $N = 7$ (1점) 정도를 해결하는 데 그칠 것 같았습니다 (결과적으로 맞는 판단인지는 모르겠습니다). $N = 8$ 도 똑같이 1점이고, 백트래킹을 짜는 것도 쉬운 일이 아닙니다.
- 다행히 동점자 산출은 배점이 가장 높은 문제의 제출횟수가 우선하고, 저는 4번 제출을 많이 하지 않았기 때문에 동점자 산출까지 오면 나쁘지 않다고 생각했습니다.  
- 저는 이때 100+150+154+209 였는데, 이 점수에 걸린 사람은 굉장히 많을 것 같았습니다.
- 어떻게든 $N = 9$ 를 해결할 수 있다면 2점을 받으니까, 저 점수셋 + 1점 백트래킹까지 제칠 수 있지 않을까? 라는 생각이 들었습니다. 

- 아까 시뮬레이터를 짰으므로, 우선 답의 스케일을 좀 보고자 랜덤한 움직임을 생성해서 문제를 풀 수 있는지 확인했습니다. $n = 8$ 까지는 1만번 정도 움직임이 있었고, $n = 9$ 는 4만~20만 번 사이에서 답을 찾아 주는 것 같았습니다. 
- 일단 길이가 4만 좀 안 되는 답을 제출해서 2점을 먹겠다는 생각을 했는데, 안타깝게도 백준과는 달리 SCPC 플랫폼은 20KB까지만 제출할 수 있었습니다.
- 대략 반 정도 줄이면 되는거 아닌가?
- 0-9와 a-z는 합쳐서 36개이고, 가능한 움직임은 6가지입니다. 따라서, 0-9와 a-z로 무식하게 움직임 두개씩을 한글자에 인코딩하면, {0-9a-z} 2만글자 좀 안되게 써서 저 4만개짜리 답을 넣을 수 있습니다. 
- 제 감동적인 2점 답안입니다. 누가 볼까 무섭지만 여기까지 읽어주셨다면 한번쯤 웃게 해줘야 하지 않을까요?

```cpp
#include &lt;bits/stdc++.h&gt;
#define eps 1e-7
#define all(x) ((x).begin()),((x).end())
#define usecppio ios::sync_with_stdio(0);cin.tie(0);
using namespace std;
using pii = pair&lt;int, int&gt;;
bool dbg = 0;

string str[36] = {&quot;AA&quot;, &quot;AB&quot;, &quot;AC&quot;, &quot;AD&quot;, &quot;AE&quot;, &quot;AF&quot;, &quot;BA&quot;, &quot;BB&quot;, &quot;BC&quot;, &quot;BD&quot;, &quot;BE&quot;, &quot;BF&quot;, &quot;CA&quot;, &quot;CB&quot;, &quot;CC&quot;, &quot;CD&quot;, &quot;CE&quot;, &quot;CF&quot;, &quot;DA&quot;, &quot;DB&quot;, &quot;DC&quot;, &quot;DD&quot;, &quot;DE&quot;, &quot;DF&quot;, &quot;EA&quot;, &quot;EB&quot;, &quot;EC&quot;, &quot;ED&quot;, &quot;EE&quot;, &quot;EF&quot;, &quot;FA&quot;, &quot;FB&quot;, &quot;FC&quot;, &quot;FD&quot;, &quot;FE&quot;, &quot;FF&quot;};
string S = &quot;6r57fom1yd43h8ykvivwumikze68uwdw7g5fr6vmd64.... (이하생략, 1.8만글자 정도)&quot;;
void solve() {
    int M; cin &gt;&gt; M;
    cout &lt;&lt; 9 &lt;&lt; '\n';
    for (char c : S) {
        if ('0' &lt;= c and c &lt;= '9') {
            cout &lt;&lt; str[c - '0'];
        }
        else {
            cout &lt;&lt; str[c - 'a' + 10];
        }
    }
    cout &lt;&lt; &quot;D&quot;;
    // (어쩌다보니 찾은 답이 홀수개라서...끝자리가 D더군요)
}


int32_t main() {
    usecppio
    int T, test_case;
    cin &gt;&gt; T;
    for(test_case = 0; test_case &lt; T; test_case++) {
        cout &lt;&lt; &quot;Case #&quot; &lt;&lt; test_case+1 &lt;&lt; endl;
        solve();
        cout &lt;&lt; endl;
    }
    return 0;
}
```

------

## 대회를 마치며 

FFT Madness와 함께 대회가 끝났습니다. 3번 만점자는 130명이고, 대회 컷이 128명이라고 가정하면 이들 중 최소한 3명 이상이 4번을 손도 못 댔어야 하는데 (4-1 + 3번의 점수가 4-2 + 3-2보다 높으므로), 이럴 확률은 굉장히 낮다고 생각합니다. 다만 뭐... 3명 정도의 적은 수는 언제나 이상한 일들이 일어날 수도 있고, 128명으로 딱 정해진 것인지도 알수 없으므로 본선 진출 확률은 '높지 않지만 0은 아닌' 상황인듯 합니다. 흡사 월드컵 축구에서 조별리그 '경우의 수' 를 따지는 듯한...

작년보다는 덜 힘들었지만, 12시간은 언제나 사람을 피폐하게 합니다. 지금도 정신과 육체가 모두 매우 피폐하기 때문에, 여기까지 짧게 줄이겠습니다. 만약 본선에 진출하게 되면 그 후기도 당연히 올라올 예정입니다 :) 

친구들 중에서는 `coffeetea`는 작년의 저처럼 버퍼 이슈에 당해버렸고, `dhdroid`는 본인의 엄청난 강점인 Construction을 살려서 굉장히 높은 성적을 달성했습니다. 같이 팀할때도 느꼈지만 장기전에 강하고, 한번 말려도 저와는 달리 깊은 수렁에 빠져들지 않으며, Construction에 특히 강한데 이 모든 조건을 만족하는 대회가 아니었나 싶습니다. 

나중에 SCPC를 준비하는 분들을 위해 조언을 남기자면, 이 대회는 제가 본 어떤 대회보다도 상수커팅이 빡셉니다. `dhdroid`의 정해 $O(N^2)$ 코드가 시간제한 3초를 2.7초인가로 간신히 통과했다고 하고, 그외에도 별다르게 줄일 방법이 없는 코드임에도 불구하고 vector 같은 것을 많이 사용했다는 이유로 제한시간의 50~80%까지 나와버립니다. JAVA는 보통 시간을 조금 (1.5배 정도) 더 주던데, JAVA로 이 문제들이 풀리기는 하는지도 의문입니다. 구글 코드잼 같은 대회가 약간 시간이 좀 넉넉해서 &quot;이게 통과된다고?&quot; 라는 생각이 드는데 비해, 이 대회는 &quot;이게 짤린다고???&quot; 라는 생각이 들게 합니다.

작년 2차도 승패를 가르는 결전지는 3번의 상수최적화였습니다. 알고리즘 문제풀이를 겨루는 대회에서 같은 복잡도인데도 (정말 말도안되게 느린게 아니라면) 2배 정도의 차이로 시간을 커트해버리는것이 정말 개인적으로 이해가 안 갔었는데, 매년 이렇게 제한을 맞추는 것을 보면 주최측이 원하는 능력 리스트에서 적절한 상수최적화 (시간, 메모리 둘 다) 가 꽤 상위에 있는듯 합니다. 

저는 이제 출전기회가 3번 정도 (내년 + 대학원 석사과정까지는 되는것으로 알고있으므로 2년 더) 남았는데, 저는 정올출신이 아니다보니 사실 대회 성적에는 크게 미련 없습니다만 본선은 한번쯤 가보고 싶긴 하네요. 지금처럼 취미 + 머리쓰는 연습으로 조금씩 하다보면 되지 않을까 생각하고 있습니다.</content><author><name>Wonseok Shin</name><email>gratus907@snu.ac.kr</email></author><category term="cp-rounds" /><category term="algorithms" /><summary type="html">Contents</summary></entry><entry><title type="html">Random Walk / Pagerank</title><link href="http://localhost:4000/advanced-algorithms/random-walk-on-graphs/" rel="alternate" type="text/html" title="Random Walk / Pagerank" /><published>2021-08-06T00:00:00+09:00</published><updated>2021-08-06T00:00:00+09:00</updated><id>http://localhost:4000/advanced-algorithms/random-walk-on-graphs</id><content type="html" xml:base="http://localhost:4000/advanced-algorithms/random-walk-on-graphs/">&lt;div id=&quot;toc&quot;&gt;
Contents
&lt;/div&gt;
* TOC
{:toc}
----------

그래프 데이터 $G = (V, E)$ 가 주어졌을 때, 우리는 다음과 같은 두 질문에 답하고 싶습니다. 
1. 그래프에서 중요한 노드가 어디인가? 
2. 그래프의 특정한 정점 $u$ 의 입장에서 볼 때, 가장 연관이 깊은 노드는 어디인가?

전자의 질문에 답하는 가장 보편적인 방법이 Pagerank, 후자의 질문에 답하는 보편적인 방법이 RWR입니다. 이번 포스팅에서는 이 두개를 같이 간단하게 알아보려고 합니다.  
우리는 Directed graph를 기본 모델로 생각하겠습니다. 

## PageRank 
Pagerank는 Google이 검색 결과를 정리하기 위해 개발한 알고리즘으로, 웹페이지의 순위 (rank) 를 정하기 위해 고안되었습니다.

### Motivation
대략적인 motivation은 아래 두 가지입니다.

- 많은 페이지로부터 인용된 (링크가 걸린) 페이지는 **중요하다**
- **중요한** 페이지로부터 링크가 걸린 페이지는 **중요하다**

굉장히 직관적으로 말이 되는 원칙입니다. 

### Algorithm
기본적으로 Pagerank는 stochastic하게 매겨집니다. 즉, 어떤 노드 $i$의 pagerank값 $r_i$는 $i$뿐 아니라 스텝수 $j$ (시간이라고 받아들이면 됩니다) 의 영향을 받으며, $r_{i, j}$ 는 $r_{-, j-1}$ 들에 의해 계산된다는 뜻입니다. 

- 각 노드의 최초 중요도 $r_{i, 0}$ 은 편의상 $1/N$ 으로 정합니다. ($N$은 당연히 노드 개수)
- 이제, 업데이트 과정은 다음과 같습니다. $N(i)$ 는 원래 neighbor를 의미하지만, 잠시 'incoming neighbors' 만 생각하기로 하겠습니다. 즉 $N(i)$는 $i$로 들어오는 edge를 갖는 노드의 집합을 의미합니다. 대신, 반대로 $i$가 인용하는 노드의 집합을 $L(i)$ 라고 쓰겠습니다. 
$$r_{i, t} = \frac{1-d}{N} + d \sum_{c \in N(i)} \frac{r_{c, t-1}}{\abs{L(c)}}$$
- 나머지 값들은 대충 자명합니다. $d$는 Damping factor라 해서, 얼마나 빠르게 수렴할지를 정하는 상수값입니다. 통상 0.85 정도를 사용합니다. 
- 가장 자연스러운 언어로 설명하자면, 완전 랜덤하게 하이퍼링크를 클릭하는 가상의 surfer가 있을 때, $t$시간이 지난 후 이 surfer가 어디에 위치할지의 확률 분포를 계산하는 방식입니다. Damping은 여기서, 클릭을 멈추고 현재 노드에 정착할 확률을 제공합니다. 

### Linear Algebra PoV
지금까지의 논의를 선형대수학의 언어로 다시 써 보겠습니다. 
- 먼저, 인접행렬 $A$에 대해, $A$를 Normalize해서 각 열의 합을 1로 고정합니다. 이를 Markov matrix라고 부릅니다. 
- Markov matrix는 마르코프 체인을 나타내는 행렬이라는 뜻인데, 이러기 위해서는 Dangling node가 있어서는 안 됩니다. 위 construction은 이를 보장하지 않기 때문에, 전체를 connected component로 이어주기 위해 
  $$S = A + \frac{1}{N}\mathbb{1}e^T$$
  이런 행렬을 새로이 계산합니다. 여기서 $\mathbb{1}$ 은 모든 항이 1인 열벡터이고, $e$는 열의 값이 0인 dangling $j$에 대해서만 1인 벡터입니다. 
- Damping factor를 고려해서 최종적인 Google Matrix (실제로 이런 이름이 붙었습니다) 를 아래와 같이 만듭니다. 
  $$G = \frac{1-d}{N} \mathbb{11^T} + d S$$
- 선형대수학의 Perron-Frobenius 정리에 의하면 irreducible markov matrix는 시작점 $x_0$와 상관없이, $x_i = G x_{i-1}$ 연산을 반복하면 어딘가로 수렴함이 알려져 있습니다. 
- 선형대수학적으로, 이 행렬은 $0 &lt; d &lt; 1$ 에 대해 Unique Maximal Eigenvalue $\lambda = 1$ 을 가집니다. 이 Eigenvalue에 대응하는 eigenvector가 바로 pagerank vector입니다.  
- Iteration $x_{i} = G x_{i-1}$ 을 빠르게 수렴시키는 방법은 numerical linear algebra의 영역이며, $G$행렬은 일반적으로 엄청나게 크지만 대신 sparse하기 때문에 그냥 그대로 놓고 iteration을 반복해도 생각보다 빠르게 계산할 수 있습니다. 

------

## Random Walk with Restart
Pagerank가 global한 노드의 중요도를 계산해 주는데 반해, Random Walk (with Restart) 는 Local한 관점에서의 중요도를 제공합니다. 어떤 노드 $u$에 대해, 각 노드 $i$ 의 중요도 벡터 $r_i = C_{u, i}$를 계산한다고 생각하면 되겠습니다. 기본적인 관점 (random-surfer) 이 Pagerank와 똑같기 때문에, Personalized Pagerank 라는 이름으로 불리기도 합니다. [^1]

### Motivation
노드 간의 어떤 연관성을 찾는 방법은 보통 두가지를 생각해 볼 수 있을 것입니다.
- Path의 길이 기반 : Shortest path 등의 metric에 기반하는 방법들
- Flow 기반 : Flow network를 만들어서 Flow가 얼마나 흐를 수 있는지에 기반하는 방법들 

이 두 방법 모두, 명환한 한계가 있습니다. 예를 들어 SNS 그래프에서 나를 기준으로, A를 통해 B로, C를 통해 D로 갈 수 있다고 하겠습니다.  
이때, A가 수많은 사람을 알고 있는 유명인이고, C가 일반적인 친구라면, 나는 B보다는 D와 더 가까운 사이라고 판단해야 합니다. 그러나 위 두 방법들은 이러한 차이를 잡아내지 못합니다. 이런 점에서 Random walk는 A에서 갈수있는 노드가 많다는 점을 Penalize하기 때문에 보다 적절하다고 할 수 있습니다.


### Algorithm 
위 Pagerank와 똑같지만, 한 노드 $u$에서만 시작하기로 합니다. Notation의 편의를 위해 Adjacent matrix를 $A$로, 이를 normalize해서 얻은 Weight matrix를 $W$ 로 쓰겠습니다.  
RWR-vector는 다음 식을 통해 계산됩니다. 
$$r_{i} = dWr_{i-1} + (1-d) e_u$$
여기서 $e_u$는 시작노드 $u$만 1인 standard basis vector입니다.  
이 식이 벡터 $r$로 수렴한다고 하면, $r = dWr + (1-d) e_u$이므로, 이를 조금 정리하면 $(I - dW)r = (1-d) e_u$에서,
$$r = (1 - d) (I - dW)^{-1} e_u$$ 
이렇게 계산할 수 있습니다. 

### Fast Computation
이 알고리즘은 실제로 쓰기에는 상당히 느리기 때문에 (행렬곱셈 연산이 느리므로...) 다양한 방법들이 개발되어 왔습니다. 특히, Pagerank는 한번 돌리면 모든 노드에 대한 정보를 얻으므로 그 cost가 amortize되지만, RWR은 쿼리노드가 바뀌면 처음부터 다시 해야한다는 점에서, 쿼리당 복잡도가 매우 높습니다. 이를 개선하기 위한 방법들에 대해서는 별도 포스팅으로 다룰 예정입니다.

------

[^1]: 공식적으로 발표된 논문에서는 아주 약간의 차이가 있으나, 식 정리의 문제이고 실제로는 identical합니다.</content><author><name>Wonseok Shin</name><email>gratus907@snu.ac.kr</email></author><category term="advanced-algorithms" /><category term="graphs" /><category term="data-science" /><summary type="html">Contents</summary></entry></feed>